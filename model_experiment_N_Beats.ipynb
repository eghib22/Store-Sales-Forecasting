{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eghib22/Store-Sales-Forecasting/blob/main/model_experiment_N_Beats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1HPsAmGXy2E",
        "outputId": "865ef598-e426-4b58-eb86-8762a6502b98",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TkCVhuaYIXT",
        "outputId": "df5492f8-1527-4315-c99f-79025e8f5dad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cJoCWUAqYMf2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "qybR_XwZbbiu",
        "outputId": "0076a213-44d9-43c0-f04c-cdb8d185bc3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d4a12f4d-2d21-48f9-b46b-a5218ef2f4d4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d4a12f4d-2d21-48f9-b46b-a5218ef2f4d4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ekaterineghibradze\",\"key\":\"b1414052fbae86987efff2083c8dcbd1\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "D1kbLG2Pb9lE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ~/.kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtNbrtCycvO7",
        "outputId": "d62f6f4a-82a5-4849-c0b1-01384b35d5f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 74 Jul  6 15:15 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN4023PLZ-1I",
        "outputId": "ae33ef52-8917-427f-f990-0d3509d11b9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 840MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNg-36PMbu7W",
        "outputId": "b05a686e-c4c4-4d8d-855e-186408b7e59a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '*.csv.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGHVXffUeA_V",
        "outputId": "4fd73359-b26e-4c5c-d179-be160aabd7b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  features.csv.zip\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  sampleSubmission.csv.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '*.csv.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpflLFtWeQpt",
        "outputId": "00d5779e-ae37-46d6-93f8-604d4adadebc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv                \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "features = pd.read_csv('features.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "sample_submission = pd.read_csv('sampleSubmission.csv')\n",
        "\n",
        "# Explore the data\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(train.head())\n",
        "print(train.columns)\n",
        "\n",
        "print(\"\\nTest shape:\", test.shape)\n",
        "print(test.head())\n",
        "print(test.columns)\n",
        "\n",
        "print(\"\\nFeatures shape:\", features.shape)\n",
        "print(features.head())\n",
        "print(features.columns)\n",
        "\n",
        "print(\"\\nStores shape:\", stores.shape)\n",
        "print(stores.head())\n",
        "print(stores.columns)\n",
        "\n",
        "print(\"\\nSample Submission shape:\", sample_submission.shape)\n",
        "print(sample_submission.head())\n",
        "print(sample_submission.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpgC7NMjecK",
        "outputId": "ade3f56b-7986-4c0d-9483-c5ef0f71bed2",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (421570, 5)\n",
            "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
            "0      1     1  2010-02-05      24924.50      False\n",
            "1      1     1  2010-02-12      46039.49       True\n",
            "2      1     1  2010-02-19      41595.55      False\n",
            "3      1     1  2010-02-26      19403.54      False\n",
            "4      1     1  2010-03-05      21827.90      False\n",
            "Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday'], dtype='object')\n",
            "\n",
            "Test shape: (115064, 4)\n",
            "   Store  Dept        Date  IsHoliday\n",
            "0      1     1  2012-11-02      False\n",
            "1      1     1  2012-11-09      False\n",
            "2      1     1  2012-11-16      False\n",
            "3      1     1  2012-11-23       True\n",
            "4      1     1  2012-11-30      False\n",
            "Index(['Store', 'Dept', 'Date', 'IsHoliday'], dtype='object')\n",
            "\n",
            "Features shape: (8190, 12)\n",
            "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
            "0      1  2010-02-05        42.31       2.572        NaN        NaN   \n",
            "1      1  2010-02-12        38.51       2.548        NaN        NaN   \n",
            "2      1  2010-02-19        39.93       2.514        NaN        NaN   \n",
            "3      1  2010-02-26        46.63       2.561        NaN        NaN   \n",
            "4      1  2010-03-05        46.50       2.625        NaN        NaN   \n",
            "\n",
            "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
            "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
            "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
            "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
            "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
            "4        NaN        NaN        NaN  211.350143         8.106      False  \n",
            "Index(['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
            "       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment',\n",
            "       'IsHoliday'],\n",
            "      dtype='object')\n",
            "\n",
            "Stores shape: (45, 3)\n",
            "   Store Type    Size\n",
            "0      1    A  151315\n",
            "1      2    A  202307\n",
            "2      3    B   37392\n",
            "3      4    A  205863\n",
            "4      5    B   34875\n",
            "Index(['Store', 'Type', 'Size'], dtype='object')\n",
            "\n",
            "Sample Submission shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02             0\n",
            "1  1_1_2012-11-09             0\n",
            "2  1_1_2012-11-16             0\n",
            "3  1_1_2012-11-23             0\n",
            "4  1_1_2012-11-30             0\n",
            "Index(['Id', 'Weekly_Sales'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge train and test with features and stores\n",
        "train_merged = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "train_merged = pd.merge(train_merged, stores, on='Store', how='left')\n",
        "\n",
        "test_merged = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_merged = pd.merge(test_merged, stores, on='Store', how='left')\n",
        "\n",
        "# Convert Date to datetime\n",
        "train_merged['Date'] = pd.to_datetime(train_merged['Date'])\n"
      ],
      "metadata": {
        "id": "GznCjGNPkKZ7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_data = train_merged[train_merged['Date'] < '2012-01-01']\n",
        "val_data = train_merged[(train_merged['Date'] >= '2012-01-01') & (train_merged['Date'] < '2012-07-01')]\n",
        "test_data = train_merged[train_merged['Date'] >= '2012-07-01']\n",
        "\n",
        "print(\"Train:\", train_data.shape)\n",
        "print(\"Validation:\", val_data.shape)\n",
        "print(\"Test (local):\", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQnvIhe4lnEZ",
        "outputId": "cd3dbecd-6e62-4fdf-f487-f46c0162f920"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (294132, 17)\n",
            "Validation: (77110, 17)\n",
            "Test (local): (50328, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "8h6sCnEsnEbg",
        "outputId": "2d7f91cc-47a2-4703-feb7-d07d5f95f9d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meghib22\u001b[0m (\u001b[33meghib22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_151543-wuwe7byz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/wuwe7byz' target=\"_blank\">n-beats-training-run</a></strong> to <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/wuwe7byz' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/wuwe7byz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/wuwe7byz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e425e122910>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Map Type A/B/C to 0/1/2\n",
        "    type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "    df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "    # Ensure IsHoliday is int (True/False → 1/0)\n",
        "    if 'IsHoliday_x' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "        df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "    elif 'IsHoliday' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "    # Fill NaNs in MarkDown columns with 0\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    # Extract date features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "vb3wF8lEL135"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# WandB configuration\n",
        "wandb_config = {\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"lookback\": 12,\n",
        "    \"horizon\": 1,\n",
        "    \"num_blocks\": 3,\n",
        "    \"units\": 256,\n",
        "    \"model_type\": \"N-BEATS\",\n",
        "    \"dataset\": \"Walmart Sales\",\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"loss\": \"MSE\"\n",
        "}\n",
        "\n",
        "class NBeatsBlock(layers.Layer):\n",
        "    \"\"\"N-BEATS block implementation\"\"\"\n",
        "\n",
        "    def __init__(self, units, thetas_dim, horizon, backcast_length, share_weights=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.thetas_dim = thetas_dim\n",
        "        self.horizon = horizon\n",
        "        self.backcast_length = backcast_length\n",
        "        self.share_weights = share_weights\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = layers.Dense(units, activation='relu')\n",
        "        self.fc2 = layers.Dense(units, activation='relu')\n",
        "        self.fc3 = layers.Dense(units, activation='relu')\n",
        "        self.fc4 = layers.Dense(units, activation='relu')\n",
        "\n",
        "        # Theta layers\n",
        "        self.theta_f = layers.Dense(thetas_dim, activation='linear')\n",
        "        self.theta_b = layers.Dense(thetas_dim, activation='linear')\n",
        "\n",
        "        # Basis functions (simplified - using linear basis)\n",
        "        self.basis_f = layers.Dense(horizon, activation='linear')\n",
        "        self.basis_b = layers.Dense(backcast_length, activation='linear')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Forward pass through FC layers\n",
        "        x = self.fc1(inputs)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        # Generate thetas\n",
        "        theta_f = self.theta_f(x)\n",
        "        theta_b = self.theta_b(x)\n",
        "\n",
        "        # Generate forecast and backcast\n",
        "        forecast = self.basis_f(theta_f)\n",
        "        backcast = self.basis_b(theta_b)\n",
        "\n",
        "        return forecast, backcast\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'thetas_dim': self.thetas_dim,\n",
        "            'horizon': self.horizon,\n",
        "            'backcast_length': self.backcast_length,\n",
        "            'share_weights': self.share_weights\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def preprocess(df):\n",
        "    \"\"\"Preprocess the data\"\"\"\n",
        "    # Encode categorical type\n",
        "    type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
        "\n",
        "    # Handle Type column\n",
        "    if 'Type' in df.columns:\n",
        "        df['Type'] = df['Type'].map(type_map)\n",
        "        df['Type'] = df['Type'].fillna(0)  # Fill any NaN with 0\n",
        "\n",
        "    # Handle IsHoliday columns\n",
        "    if 'IsHoliday_x' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "        df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "    elif 'IsHoliday' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "    # Date features (before dropping Date)\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df['Year'] = df['Date'].dt.year.astype(int)\n",
        "        df['Month'] = df['Date'].dt.month.astype(int)\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "        df['Day'] = df['Date'].dt.day.astype(int)\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek.astype(int)\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear.astype(int)\n",
        "\n",
        "        # Drop Date column after extracting features\n",
        "        df = df.drop(columns=['Date'])\n",
        "\n",
        "    # Fill MarkDown NaNs\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0.0)\n",
        "\n",
        "    # Ensure all numeric columns are proper numeric types\n",
        "    numeric_cols = ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_sequences(data, lookback, horizon, target_col='Weekly_Sales'):\n",
        "    \"\"\"Create sequences for time series forecasting\"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    # Ensure we have the right columns\n",
        "    if target_col not in data.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found in data\")\n",
        "\n",
        "    # Get feature columns (excluding target)\n",
        "    feature_cols = [col for col in data.columns if col != target_col]\n",
        "\n",
        "    print(f\"Creating sequences with {len(feature_cols)} features: {feature_cols}\")\n",
        "\n",
        "    # Group by Store and Dept to maintain temporal order\n",
        "    for (store, dept), group in data.groupby(['Store', 'Dept']):\n",
        "        # Reset index to ensure continuous indexing\n",
        "        group = group.reset_index(drop=True)\n",
        "\n",
        "        if len(group) < lookback + horizon:\n",
        "            continue\n",
        "\n",
        "        # Create sequences for this store-dept combination\n",
        "        for i in range(len(group) - lookback - horizon + 1):\n",
        "            # Input sequence (features only)\n",
        "            seq_features = group.iloc[i:i+lookback][feature_cols].values\n",
        "\n",
        "            # Target sequence\n",
        "            target_seq = group.iloc[i+lookback:i+lookback+horizon][target_col].values\n",
        "\n",
        "            # Ensure correct shape\n",
        "            if seq_features.shape[0] == lookback and len(target_seq) == horizon:\n",
        "                sequences.append(seq_features)\n",
        "                targets.append(target_seq)\n",
        "\n",
        "    sequences = np.array(sequences, dtype=np.float32)\n",
        "    targets = np.array(targets, dtype=np.float32)\n",
        "\n",
        "    print(f\"Created {len(sequences)} sequences with shape: {sequences.shape}\")\n",
        "    print(f\"Target shape: {targets.shape}\")\n",
        "\n",
        "    return sequences, targets\n",
        "\n",
        "def create_walmart_nbeats_model(input_shape, horizon=1, num_blocks=3, units=256):\n",
        "    \"\"\"Create simplified N-BEATS inspired model for Walmart sales forecasting\"\"\"\n",
        "\n",
        "    # Input layer\n",
        "    inputs = layers.Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "    # Flatten the input for N-BEATS\n",
        "    flattened = layers.Flatten(name='flatten')(inputs)\n",
        "\n",
        "    # Add some preprocessing layers\n",
        "    x = layers.Dense(512, activation='relu', name='dense_preprocess_1')(flattened)\n",
        "    x = layers.Dropout(0.2, name='dropout_1')(x)\n",
        "    x = layers.Dense(256, activation='relu', name='dense_preprocess_2')(x)\n",
        "    x = layers.Dropout(0.2, name='dropout_2')(x)\n",
        "\n",
        "    # Simplified N-BEATS inspired blocks\n",
        "    forecasts = []\n",
        "    residual = x\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "        # FC layers for this block\n",
        "        block_out = layers.Dense(units, activation='relu', name=f'block_{i}_dense_1')(residual)\n",
        "        block_out = layers.Dense(units, activation='relu', name=f'block_{i}_dense_2')(block_out)\n",
        "        block_out = layers.Dense(units, activation='relu', name=f'block_{i}_dense_3')(block_out)\n",
        "\n",
        "        # Forecast and backcast\n",
        "        forecast = layers.Dense(horizon, activation='linear', name=f'forecast_{i}')(block_out)\n",
        "        backcast = layers.Dense(units, activation='linear', name=f'backcast_{i}')(block_out)\n",
        "\n",
        "        forecasts.append(forecast)\n",
        "\n",
        "        # Residual connection (make sure dimensions match)\n",
        "        if residual.shape[-1] != backcast.shape[-1]:\n",
        "            residual = layers.Dense(units, name=f'residual_proj_{i}')(residual)\n",
        "\n",
        "        residual = layers.Subtract(name=f'residual_subtract_{i}')([residual, backcast])\n",
        "\n",
        "    # Sum all forecasts\n",
        "    if len(forecasts) > 1:\n",
        "        output = layers.Add(name='sum_forecasts')(forecasts)\n",
        "    else:\n",
        "        output = forecasts[0]\n",
        "\n",
        "    # Final output layer\n",
        "    output = layers.Dense(horizon, activation='linear', name='final_output')(output)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=output, name='walmart_nbeats')\n",
        "    return model\n",
        "\n",
        "def calculate_wmae(y_true, y_pred, weights=None):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "    if weights is None:\n",
        "        weights = np.ones_like(y_true)\n",
        "\n",
        "    # Ensure arrays are flattened\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    weights = weights.flatten()\n",
        "\n",
        "    # Calculate weighted MAE\n",
        "    mae = np.abs(y_true - y_pred)\n",
        "    wmae = np.sum(weights * mae) / np.sum(weights)\n",
        "\n",
        "    return wmae\n",
        "\n",
        "def prepare_walmart_data(train_data, val_data, test_data, lookback=12, horizon=1):\n",
        "    \"\"\"Prepare data for N-BEATS model\"\"\"\n",
        "\n",
        "    # Preprocess all datasets\n",
        "    train_processed = preprocess(train_data.copy())\n",
        "    val_processed = preprocess(val_data.copy())\n",
        "    test_processed = preprocess(test_data.copy())\n",
        "\n",
        "    # Get feature columns (excluding target and any remaining non-numeric columns)\n",
        "    feature_cols = [col for col in train_processed.columns\n",
        "                   if col not in ['Weekly_Sales', 'Date'] and\n",
        "                   train_processed[col].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
        "\n",
        "    print(f\"Feature columns: {feature_cols}\")\n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "\n",
        "    # Debug: Check data types\n",
        "    print(\"Data types in train_processed:\")\n",
        "    print(train_processed[feature_cols].dtypes)\n",
        "\n",
        "    # Scale features (excluding target)\n",
        "    scaler_features = StandardScaler()\n",
        "    scaler_target = StandardScaler()\n",
        "\n",
        "    # Fit scalers on training data\n",
        "    train_features = train_processed[feature_cols].astype(np.float32)\n",
        "    train_target = train_processed['Weekly_Sales'].values.reshape(-1, 1).astype(np.float32)\n",
        "\n",
        "    scaler_features.fit(train_features)\n",
        "    scaler_target.fit(train_target)\n",
        "\n",
        "    # Transform all datasets\n",
        "    train_processed[feature_cols] = scaler_features.transform(train_features)\n",
        "    train_processed['Weekly_Sales'] = scaler_target.transform(train_target).flatten()\n",
        "\n",
        "    val_features = val_processed[feature_cols].astype(np.float32)\n",
        "    val_target = val_processed['Weekly_Sales'].values.reshape(-1, 1).astype(np.float32)\n",
        "    val_processed[feature_cols] = scaler_features.transform(val_features)\n",
        "    val_processed['Weekly_Sales'] = scaler_target.transform(val_target).flatten()\n",
        "\n",
        "    test_features = test_processed[feature_cols].astype(np.float32)\n",
        "    test_target = test_processed['Weekly_Sales'].values.reshape(-1, 1).astype(np.float32)\n",
        "    test_processed[feature_cols] = scaler_features.transform(test_features)\n",
        "    test_processed['Weekly_Sales'] = scaler_target.transform(test_target).flatten()\n",
        "\n",
        "    # Create sequences\n",
        "    X_train, y_train = create_sequences(train_processed, lookback, horizon)\n",
        "    X_val, y_val = create_sequences(val_processed, lookback, horizon)\n",
        "    X_test, y_test = create_sequences(test_processed, lookback, horizon)\n",
        "\n",
        "    # Ensure correct data types\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    y_train = y_train.astype(np.float32)\n",
        "    X_val = X_val.astype(np.float32)\n",
        "    y_val = y_val.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "    y_test = y_test.astype(np.float32)\n",
        "\n",
        "    print(f\"Training sequences: {X_train.shape}, {y_train.shape}\")\n",
        "    print(f\"Validation sequences: {X_val.shape}, {y_val.shape}\")\n",
        "    print(f\"Test sequences: {X_test.shape}, {y_test.shape}\")\n",
        "    print(f\"Data types - X_train: {X_train.dtype}, y_train: {y_train.dtype}\")\n",
        "\n",
        "    return (X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "            scaler_features, scaler_target)\n",
        "\n",
        "def train_nbeats_model(X_train, y_train, X_val, y_val,\n",
        "                      epochs=100, batch_size=64, learning_rate=0.001):\n",
        "    \"\"\"Train N-BEATS model with WandB logging\"\"\"\n",
        "\n",
        "    # Create model\n",
        "    model = create_walmart_nbeats_model(\n",
        "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "        horizon=y_train.shape[1],\n",
        "        num_blocks=wandb_config[\"num_blocks\"],\n",
        "        units=wandb_config[\"units\"]\n",
        "    )\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"Model Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Log model architecture to WandB\n",
        "    wandb.log({\"model_parameters\": model.count_params()})\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    # Callbacks with WandB integration\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "        WandbMetricsLogger(log_freq=\"epoch\"),  # New WandB metrics logger\n",
        "        WandbModelCheckpoint(\n",
        "            filepath=\"best_model.h5\",\n",
        "            monitor=\"val_loss\",\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, scaler_target):\n",
        "    \"\"\"Evaluate model and calculate WMAE with WandB logging\"\"\"\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform predictions and targets\n",
        "    y_pred = scaler_target.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    y_true = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    wmae = calculate_wmae(y_true, y_pred)\n",
        "\n",
        "    # Calculate WMAE with holiday weights (higher weight for holiday periods)\n",
        "    holiday_weights = np.where(y_true > np.median(y_true), 5, 1)  # Higher weight for higher sales\n",
        "    wmae_weighted = calculate_wmae(y_true, y_pred, holiday_weights)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # Log all metrics to WandB\n",
        "    wandb.log({\n",
        "        \"test_mae\": mae,\n",
        "        \"test_wmae\": wmae,\n",
        "        \"test_wmae_weighted\": wmae_weighted,\n",
        "        \"test_mse\": mse,\n",
        "        \"test_rmse\": rmse,\n",
        "        \"test_mape\": mape\n",
        "    })\n",
        "\n",
        "    # Create and log prediction plots\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Scatter plot\n",
        "    sample_idx = np.random.choice(len(y_true), min(1000, len(y_true)), replace=False)\n",
        "    axes[0].scatter(y_true[sample_idx], y_pred[sample_idx], alpha=0.6)\n",
        "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    axes[0].set_xlabel('Actual Sales')\n",
        "    axes[0].set_ylabel('Predicted Sales')\n",
        "    axes[0].set_title('Predictions vs Actual')\n",
        "\n",
        "    # Time series plot\n",
        "    sample_range = slice(0, min(100, len(y_true)))\n",
        "    axes[1].plot(y_true[sample_range], label='Actual', marker='o', markersize=3)\n",
        "    axes[1].plot(y_pred[sample_range], label='Predicted', marker='s', markersize=3)\n",
        "    axes[1].set_xlabel('Time Steps')\n",
        "    axes[1].set_ylabel('Sales')\n",
        "    axes[1].set_title('Time Series Sample')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Log plots to WandB\n",
        "    wandb.log({\"predictions_plot\": wandb.Image(fig)})\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Log prediction distribution\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    ax.hist(y_true, bins=50, alpha=0.7, label='Actual', density=True)\n",
        "    ax.hist(y_pred, bins=50, alpha=0.7, label='Predicted', density=True)\n",
        "    ax.set_xlabel('Sales Values')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title('Distribution of Actual vs Predicted Sales')\n",
        "    ax.legend()\n",
        "\n",
        "    wandb.log({\"distribution_plot\": wandb.Image(fig)})\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Test MAE: {mae:.2f}\")\n",
        "    print(f\"Test WMAE: {wmae:.2f}\")\n",
        "    print(f\"Test WMAE (Holiday weighted): {wmae_weighted:.2f}\")\n",
        "    print(f\"Test RMSE: {rmse:.2f}\")\n",
        "    print(f\"Test MAPE: {mape:.2f}%\")\n",
        "\n",
        "    return y_pred, y_true, mae, wmae, wmae_weighted\n",
        "\n",
        "def plot_predictions(y_true, y_pred, n_samples=100):\n",
        "    \"\"\"Plot predictions vs actual values\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot sample predictions\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sample_idx = np.random.choice(len(y_true), min(n_samples, len(y_true)), replace=False)\n",
        "    plt.scatter(y_true[sample_idx], y_pred[sample_idx], alpha=0.6)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual Sales')\n",
        "    plt.ylabel('Predicted Sales')\n",
        "    plt.title('Predictions vs Actual (Sample)')\n",
        "\n",
        "    # Plot time series sample\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sample_range = slice(0, min(50, len(y_true)))\n",
        "    plt.plot(y_true[sample_range], label='Actual', marker='o')\n",
        "    plt.plot(y_pred[sample_range], label='Predicted', marker='s')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Sales')\n",
        "    plt.title('Time Series Sample')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize WandB\n",
        "    wandb.init(\n",
        "        project=\"Store-Sales-Forecasting\",\n",
        "        entity=\"agasi22-free-university-of-tbilisi-\" ,\n",
        "        name=\"n-beats-training-run\",\n",
        "        config=wandb_config,\n",
        "        tags=[\"nbeats\", \"time-series\", \"walmart\", \"sales-forecasting\"]\n",
        "    )\n",
        "\n",
        "    # Update config with wandb config\n",
        "    config = wandb.config\n",
        "\n",
        "    print(\"🚀 Starting Walmart Sales Forecasting with N-BEATS\")\n",
        "    print(f\"📊 WandB Project: {wandb.run.project}\")\n",
        "    print(f\"🔧 Run Name: {wandb.run.name}\")\n",
        "\n",
        "    # Assuming you have already loaded and preprocessed your data\n",
        "    # train_data, val_data, test_data should be available\n",
        "\n",
        "    print(\"\\n📈 Preparing data for N-BEATS model...\")\n",
        "    (X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "     scaler_features, scaler_target) = prepare_walmart_data(\n",
        "        train_data, val_data, test_data,\n",
        "        lookback=config.lookback, horizon=config.horizon\n",
        "    )\n",
        "\n",
        "    # Log data statistics\n",
        "    wandb.log({\n",
        "        \"train_sequences\": len(X_train),\n",
        "        \"val_sequences\": len(X_val),\n",
        "        \"test_sequences\": len(X_test),\n",
        "        \"feature_dimensions\": X_train.shape[2],\n",
        "        \"lookback_window\": config.lookback,\n",
        "        \"forecast_horizon\": config.horizon\n",
        "    })\n",
        "\n",
        "    print(\"\\n🤖 Training N-BEATS model...\")\n",
        "    model, history = train_nbeats_model(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        epochs=config.epochs,\n",
        "        batch_size=config.batch_size,\n",
        "        learning_rate=config.learning_rate\n",
        "    )\n",
        "\n",
        "    print(\"\\n📊 Evaluating model...\")\n",
        "    y_pred, y_true, mae, wmae, wmae_weighted = evaluate_model(\n",
        "        model, X_test, y_test, scaler_target\n",
        "    )\n",
        "\n",
        "    print(\"\\n📈 Creating training history plots...\")\n",
        "    # Plot training history and log to WandB\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0].set_title('Model Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(history.history['mae'], label='Training MAE')\n",
        "    axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
        "    axes[1].set_title('Model MAE')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"training_history\": wandb.Image(fig)})\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Log final summary\n",
        "    wandb.log({\n",
        "        \"final_train_loss\": history.history['loss'][-1],\n",
        "        \"final_val_loss\": history.history['val_loss'][-1],\n",
        "        \"final_train_mae\": history.history['mae'][-1],\n",
        "        \"final_val_mae\": history.history['val_mae'][-1],\n",
        "        \"best_val_loss\": min(history.history['val_loss']),\n",
        "        \"training_epochs\": len(history.history['loss'])\n",
        "    })\n",
        "\n",
        "    print(f\"\\n✅ Training completed successfully!\")\n",
        "    print(f\"📊 Final Results:\")\n",
        "    print(f\"   • Test MAE: {mae:.2f}\")\n",
        "    print(f\"   • Test WMAE: {wmae:.2f}\")\n",
        "    print(f\"   • Test WMAE (Holiday weighted): {wmae_weighted:.2f}\")\n",
        "    print(f\"🔗 View results at: {wandb.run.url}\")\n",
        "\n",
        "    # Save model artifact to WandB\n",
        "    model_artifact = wandb.Artifact(\"walmart-nbeats-model\", type=\"model\")\n",
        "    model.save(\"walmart_nbeats_model.h5\")\n",
        "    model_artifact.add_file(\"walmart_nbeats_model.h5\")\n",
        "    wandb.log_artifact(model_artifact)\n",
        "\n",
        "    # Finish WandB run\n",
        "    wandb.finish()\n",
        "    print(\"🎉 WandB logging completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vNAz7VuitFFv",
        "outputId": "8dbdba5c-7629-44d7-890b-44a6e30b497e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">n-beats-training-run</strong> at: <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/wuwe7byz' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/wuwe7byz</a><br> View project at: <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_151543-wuwe7byz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_152644-tijqym0b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/tijqym0b' target=\"_blank\">n-beats-training-run</a></strong> to <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/tijqym0b' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/tijqym0b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Walmart Sales Forecasting with N-BEATS\n",
            "📊 WandB Project: Store-Sales-Forecasting\n",
            "🔧 Run Name: n-beats-training-run\n",
            "\n",
            "📈 Preparing data for N-BEATS model...\n",
            "Feature columns: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear']\n",
            "Number of features: 20\n",
            "Data types in train_processed:\n",
            "Store             int64\n",
            "Dept              int64\n",
            "Temperature     float64\n",
            "Fuel_Price      float64\n",
            "MarkDown1       float64\n",
            "MarkDown2       float64\n",
            "MarkDown3       float64\n",
            "MarkDown4       float64\n",
            "MarkDown5       float64\n",
            "CPI             float64\n",
            "Unemployment    float64\n",
            "Type              int64\n",
            "Size              int64\n",
            "IsHoliday         int64\n",
            "Year              int64\n",
            "Month             int64\n",
            "Week              int64\n",
            "Day               int64\n",
            "DayOfWeek         int64\n",
            "DayOfYear         int64\n",
            "dtype: object\n",
            "Creating sequences with 20 features: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear']\n",
            "Created 256034 sequences with shape: (256034, 12, 20)\n",
            "Target shape: (256034, 1)\n",
            "Creating sequences with 20 features: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear']\n",
            "Created 40496 sequences with shape: (40496, 12, 20)\n",
            "Target shape: (40496, 1)\n",
            "Creating sequences with 20 features: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear']\n",
            "Created 14245 sequences with shape: (14245, 12, 20)\n",
            "Target shape: (14245, 1)\n",
            "Training sequences: (256034, 12, 20), (256034, 1)\n",
            "Validation sequences: (40496, 12, 20), (40496, 1)\n",
            "Test sequences: (14245, 12, 20), (14245, 1)\n",
            "Data types - X_train: float32, y_train: float32\n",
            "\n",
            "🤖 Training N-BEATS model...\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"walmart_nbeats\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"walmart_nbeats\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m20\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_preprocess_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m123,392\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_preprocess… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_preprocess_2  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_preprocess… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_0_dense_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_0_dense_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_0_dense_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_0_dense_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_0_dense_2[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ backcast_0 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_0_dense_3[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_subtract_0 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mSubtract\u001b[0m)          │                   │            │ backcast_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_1_dense_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ residual_subtrac… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_1_dense_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_1_dense_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_1_dense_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_1_dense_2[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ backcast_1 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_1_dense_3[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_subtract_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ residual_subtrac… │\n",
              "│ (\u001b[38;5;33mSubtract\u001b[0m)          │                   │            │ backcast_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_2_dense_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ residual_subtrac… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_2_dense_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_2_dense_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_2_dense_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ block_2_dense_2[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ forecast_0 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ block_0_dense_3[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ forecast_1 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ block_1_dense_3[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ forecast_2 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ block_2_dense_3[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sum_forecasts (\u001b[38;5;33mAdd\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ forecast_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │                   │            │ forecast_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │                   │            │ forecast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ sum_forecasts[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_preprocess_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">123,392</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_preprocess… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_preprocess_2  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_preprocess… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_0_dense_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_0_dense_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_0_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_0_dense_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_0_dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ backcast_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_0_dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_subtract_0 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)          │                   │            │ backcast_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_1_dense_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ residual_subtrac… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_1_dense_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_1_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_1_dense_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_1_dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ backcast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_1_dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_subtract_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ residual_subtrac… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)          │                   │            │ backcast_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_2_dense_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ residual_subtrac… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_2_dense_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_2_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ block_2_dense_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ block_2_dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ forecast_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ block_0_dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ forecast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ block_1_dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ forecast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ block_2_dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sum_forecasts (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ forecast_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │                   │            │ forecast_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │                   │            │ forecast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ sum_forecasts[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m979,205\u001b[0m (3.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">979,205</span> (3.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m979,205\u001b[0m (3.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">979,205</span> (3.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m7999/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8189 - mae: 0.5758"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 18ms/step - loss: 0.8188 - mae: 0.5758 - val_loss: 6.7910 - val_mae: 2.1483 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 18ms/step - loss: 0.7402 - mae: 0.5358 - val_loss: 12.8059 - val_mae: 2.7380 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 18ms/step - loss: 0.7177 - mae: 0.5248 - val_loss: 14.5226 - val_mae: 2.9636 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6814 - mae: 0.5102"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 18ms/step - loss: 0.6814 - mae: 0.5102 - val_loss: 4.1646 - val_mae: 1.7502 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m8000/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6302 - mae: 0.4859"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 17ms/step - loss: 0.6302 - mae: 0.4859 - val_loss: 1.5593 - val_mae: 1.0527 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 17ms/step - loss: 0.5872 - mae: 0.4659 - val_loss: 1.6296 - val_mae: 1.0398 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 17ms/step - loss: 0.5554 - mae: 0.4514 - val_loss: 3.1079 - val_mae: 1.3600 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 17ms/step - loss: 0.5327 - mae: 0.4427 - val_loss: 2.4510 - val_mae: 1.3468 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 17ms/step - loss: 0.5194 - mae: 0.4367 - val_loss: 4.3670 - val_mae: 1.7334 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m7999/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5045 - mae: 0.4282\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 17ms/step - loss: 0.5045 - mae: 0.4282 - val_loss: 4.2547 - val_mae: 1.7700 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 17ms/step - loss: 0.4712 - mae: 0.4119 - val_loss: 3.2298 - val_mae: 1.4741 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m8001/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4547 - mae: 0.4038"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 18ms/step - loss: 0.4547 - mae: 0.4038 - val_loss: 1.4207 - val_mae: 0.9934 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 21ms/step - loss: 0.4443 - mae: 0.3987 - val_loss: 1.4243 - val_mae: 0.9718 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 20ms/step - loss: 0.4390 - mae: 0.3957 - val_loss: 3.1493 - val_mae: 1.5375 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 20ms/step - loss: 0.4322 - mae: 0.3933 - val_loss: 8.1655 - val_mae: 2.3457 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 19ms/step - loss: 0.4304 - mae: 0.3921 - val_loss: 5.5301 - val_mae: 1.9862 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m8001/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4254 - mae: 0.3895\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 20ms/step - loss: 0.4254 - mae: 0.3895 - val_loss: 11.6650 - val_mae: 2.8511 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 20ms/step - loss: 0.4075 - mae: 0.3807 - val_loss: 6.8265 - val_mae: 2.2263 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 19ms/step - loss: 0.3999 - mae: 0.3770 - val_loss: 1.5836 - val_mae: 1.0247 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 20ms/step - loss: 0.3956 - mae: 0.3745 - val_loss: 22.3370 - val_mae: 3.8124 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 20ms/step - loss: 0.3919 - mae: 0.3725 - val_loss: 26.3899 - val_mae: 4.1326 - learning_rate: 2.5000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m8000/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3911 - mae: 0.3715\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m8002/8002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 20ms/step - loss: 0.3911 - mae: 0.3715 - val_loss: 16.5978 - val_mae: 3.4209 - learning_rate: 2.5000e-04\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "\n",
            "📊 Evaluating model...\n",
            "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
            "Test MAE: 22023.43\n",
            "Test WMAE: 22023.43\n",
            "Test WMAE (Holiday weighted): 21135.60\n",
            "Test RMSE: 26540.53\n",
            "Test MAPE: 26992.43%\n",
            "\n",
            "📈 Creating training history plots...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Training completed successfully!\n",
            "📊 Final Results:\n",
            "   • Test MAE: 22023.43\n",
            "   • Test WMAE: 22023.43\n",
            "   • Test WMAE (Holiday weighted): 21135.60\n",
            "🔗 View results at: https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/tijqym0b\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>█████████▄▄▄▄▄▄▄▂▂▂▂▂▁</td></tr><tr><td>epoch/loss</td><td>█▇▇▆▅▄▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch/mae</td><td>█▇▇▆▅▄▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch/val_loss</td><td>▃▄▅▂▁▁▁▁▂▂▂▁▁▁▃▂▄▃▁▇█▅</td></tr><tr><td>epoch/val_mae</td><td>▄▅▅▃▁▁▂▂▃▃▂▁▁▂▄▃▅▄▁▇█▆</td></tr><tr><td>feature_dimensions</td><td>▁</td></tr><tr><td>final_train_loss</td><td>▁</td></tr><tr><td>final_train_mae</td><td>▁</td></tr><tr><td>final_val_loss</td><td>▁</td></tr><tr><td>final_val_mae</td><td>▁</td></tr><tr><td>forecast_horizon</td><td>▁</td></tr><tr><td>lookback_window</td><td>▁</td></tr><tr><td>model_parameters</td><td>▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_mape</td><td>▁</td></tr><tr><td>test_mse</td><td>▁</td></tr><tr><td>test_rmse</td><td>▁</td></tr><tr><td>test_sequences</td><td>▁</td></tr><tr><td>test_wmae</td><td>▁</td></tr><tr><td>test_wmae_weighted</td><td>▁</td></tr><tr><td>train_sequences</td><td>▁</td></tr><tr><td>training_epochs</td><td>▁</td></tr><tr><td>val_sequences</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>1.42072</td></tr><tr><td>epoch/epoch</td><td>21</td></tr><tr><td>epoch/learning_rate</td><td>0.00013</td></tr><tr><td>epoch/loss</td><td>0.39614</td></tr><tr><td>epoch/mae</td><td>0.37172</td></tr><tr><td>epoch/val_loss</td><td>16.59784</td></tr><tr><td>epoch/val_mae</td><td>3.42093</td></tr><tr><td>feature_dimensions</td><td>20</td></tr><tr><td>final_train_loss</td><td>0.39614</td></tr><tr><td>final_train_mae</td><td>0.37172</td></tr><tr><td>final_val_loss</td><td>16.59784</td></tr><tr><td>final_val_mae</td><td>3.42093</td></tr><tr><td>forecast_horizon</td><td>1</td></tr><tr><td>lookback_window</td><td>12</td></tr><tr><td>model_parameters</td><td>979205</td></tr><tr><td>test_mae</td><td>22023.42578</td></tr><tr><td>test_mape</td><td>26992.42773</td></tr><tr><td>test_mse</td><td>704399488.0</td></tr><tr><td>test_rmse</td><td>26540.52539</td></tr><tr><td>test_sequences</td><td>14245</td></tr><tr><td>test_wmae</td><td>22023.42578</td></tr><tr><td>test_wmae_weighted</td><td>21135.59971</td></tr><tr><td>train_sequences</td><td>256034</td></tr><tr><td>training_epochs</td><td>22</td></tr><tr><td>val_sequences</td><td>40496</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">n-beats-training-run</strong> at: <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/tijqym0b' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/tijqym0b</a><br> View project at: <a href='https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">https://wandb.ai/agasi22-free-university-of-tbilisi-/Store-Sales-Forecasting</a><br>Synced 5 W&B file(s), 3 media file(s), 10 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_152644-tijqym0b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 WandB logging completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9V7x8-mUu-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}