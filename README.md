ამ პროექტში ვაკეთებთ Walmart მაღაზიების გაყიდვების პროგნოზირებას. 
მონაცემების გასამზადებლად შევაერთეთ მოცემული train დატასეტი features და stores დატასეტებთან.
დავსპლიტეთ დამერჯილი დატასეტი დროის მიხედვით. ვჰენდლავთ Nan ველიუებს და კატეგორიულები ცვლადები გადაგვყავს ნუმერიქალ ცვლადებში.

XGBoost:
საუკეთესო შედეგი:
n_estimators: 2000
verbosity: 1
wmae: 2815
ოპტიმიზაციები:
დავამატე Year, Month, Week, Day, DayOfWeek, IsMonthStart, IsMonthEnd, IsWeekend, და Quarter 
დავატრენინგე მოდელი y′ = log(Weekly_Sales + 1) და პროგნოზი შევაქციე ამ ფუნქციით exp(y′) - 1, ვარიაციის სტაბილიზაციისთვის.
პარამეტრების n_estimators, max_depth, learning_rate, subsample, colsample_bytree და min_child_weight სხვადასხვა კომბინაციებიც მოსვინჯე, თუმცა wmae მივიღე 3370. შედეგად ოპტიმიზაციებამდე არსებული მოდელი უკეთეს შედეგს დებდა. ამიტომ ვცადე randomizedSearchCV, თუმცა შედეგი მაინც არა დამაკმაყოფილებელი იყო და გადავწყვიტე დამემატებინა რეგულარიზაციები. gamma: 0.1, 'reg_alpha': 0.1, 'reg_lambda': 1.0 .(Gamma adds a minimum loss reduction threshold for making a split, discouraging the model from growing branches that don’t contribute substantive improvements, L1 (reg_alpha) and L2 (reg_lambda) penalties shrink coefficient magnitudes and help prevent individual leaves from drifting too far; and by subsampling at the tree‑level, the model sees fewer features per split which further guards against overfitting).

LightGBM:
n_estimators: 2000, learning_rate: 0.015, num_leaves: 70, max_depth: 14, Wmae: 2773.
n_estimators: 3000, learning_rate: 0.001, num_leaves: 80, max_depth: 14, Wmae: 5050.
n_estimators: 3000, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2741.
n_estimators: 3500, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2682.
ციკლური ენკოდირება სეზონური პატერნებისთვის(სეზონები ხო ციკლურია და ამიტომ სინუსის და კოსინუსის ფუნქციები გამოვიყენე ტრანსფორმაციისთვის). დავამატე ფიჩერები quarter, dayofyear, weekofyear. შევქემენი ბინარული ინდიკატორები თითოეული markdown ტიპისთვის. დავუმატე შემდეგი ფუნქციები: Total_MarkDown, რომელიც ყველა markdown მნიშვნელობის ჯამს წარმოადგენს, MarkDown_Count, რომელიც აქტიური მარკდაუნების რაოდენობას აღნიშნავს, და Has_MarkDown, რომელიც ბინარული ინდიკატორია ნებისმიერი markdown-ის არსებობისთვის. ახალი ფუნქციები: CPI_Unemployment_Ratio, რომელიც ეკონომიკური ჯანმრთელობის ინდიკატორია, CPI_Normalized, რომელიც სტანდარტიზებული CPI მნიშვნელობებია, და Unemployment_Normalized სტანდარტიზებული უმუშევრობის მაჩვენებლებისთვის. ეს იჭერს ეკონომიკურ პირობებს, რომლებიც მოქმედებს მომხმარებლის ხარჯვაზე. Lag ფუნქციები შეიცავს წინა 1, 2, 4 და 8 კვირის გაყიდვებს. ყველა lag ფუნქცია იქმნება Store და Dept მიხედვით დაჯგუფებული Weekly_Sales-ის შესაბამისი პერიოდით გადატანით. შემდეგი ფუნქციები: Sales_Mean და Sales_Std 4, 8, 12 კვირის window-ებისთვის. ეს იჭერს გაყიდვების ტენდენციებს და ცვალებადობის ნიმუშებს. ფუნქციები: Is_Christmas_Period (15-31 დეკემბრისთვის), Is_Thanksgiving_Period (20-30 ნოემბრისთვის), Is_Back_To_School (აგვისტო და სექტემბრის დასაწყისისთვის). ეს იჭერს ძირითადი შოპინგის სეზონებს განსხვავებული ნიმუშებით. ფუნქცია Store_Dept_Interaction = Store × 1000 + Dept, რომელიც იჭერს უნიკალურ მაღაზია-დეპარტამენტის კომბინაციებს. პარამეტრები შეიცავს reg_alpha=0.1 L1 რეგულარიზაციისთვის, reg_lambda=0.1 L2 რეგულარიზაციისთვის, min_split_gain=0.1 split-ების მინიმალური gain-ისთვის, min_child_weight=0.001 მინიმალური child weight-ისთვის, subsample=0.8 რიგების sampling-ისთვის, colsample_bytree=0.8 სვეტების sampling-ისთვის. ეს ამცირებს overfitting-ს და აუმჯობესებს generalization-ს. წმაე: 1877.


N-BEATS: 
N-BEATS-ის ძირითადი იდეა ემყარება ბაზისური ფუნქციების გამოყენებას დროითი მწკრივების დეკომპოზიციისთვის. ის არ ცდილობს უშუალოდ იწინასწარმეტყველოს მომავალი მნიშვნელობები, არამედ სწავლობს ბაზისურ კომპონენტებს, რომლებიც შემდეგ გამოიყენება პროგნოზის ასაგებად. მოდელი აგებულია ბლოკებისგან, რომლებიც ერთმანეთთან ჯაჭვურად არიან დაკავშირებული. ჩვენი იმპლემენტაცია ქმნის ოპტიმიზირებულ N-BEATS-ზე დაფუძნებულ მოდელს Walmart-ის გაყიდვების პროგნოზირებისთვის გაძლიერებული არქიტექტურით, რომელიც მოიცავს batch normalization-ს, dropout რეგულარიზაციას და რეზიდუალურ კავშირებს მრავალ ბლოკში. მოდელი ამუშავებს მაღაზიის ტიპებს, სადღესასწაულო ინდიკატორებს, ფასდაკლების ფიჩერებს და დროის კომპონენტებს (წელი, თვე, კვირა, დღე). ტრენინგის პროცესი მოიცავს ოპტიმიზაციის ტექნიკებს, როგორიცაა ციკლური სწავლების ტემპი, გრადიენტის შემოჭრა და ადრეული შეჩერება overfitting-ის თავიდან ასაცილებლად.  2-ბლოკიანი N-BEATS მოდელი 128 ერთეულით და 30% dropout-ით, ავტომატური ფიჩერების ინჟინერინგი lag ფიჩერებით. თავდაპირველი N-BEATS მოდელი early stopping პრობლემას აწყდებოდა 18-ე ეპოქზე, რაც იწვევდა არაოპტიმალურ შედეგებს. ეს იყო patience=10-ის გამო და ასევე მაღალი learning_rate=0.001-ის გამო.


DLinear მოდელების ევოლუცია
ამ პროექტში Walmart-ის მაღაზიების ყოველკვირეული გაყიდვების პროგნოზირების ამოცანას ვეხები, კონკრეტულად კი Deep Learning არქიტექტურა DLinear-ის გამოყენებას. ეს არის მნიშვნელოვანი გამოწვევა საცალო ვაჭრობის ანალიტიკაში, რომელიც მოითხოვს მძლავრ და მასშტაბირებად გადაწყვეტილებებს. ჩემი მუშაობის მიზანი იყო DLinear-ის შესაძლებლობების შეფასება ამ მრავალსერიანი დროითი მონაცემების პროგნოზირებისთვის და მისი იმპლემენტაციის დახვეწა.
DLinear მოდელის მიმოხილვა: როგორ მუშაობს და რატომ არის კარგი/ცუდი ამ ამოცანისთვის?
DLinear (Decomposition Linear) არის Deep Learning არქიტექტურა, რომელიც ეფუძნება დროითი სერიების დეკომპოზიციას. მისი მთავარი იდეაა დროითი სერიის დაყოფა ორ ძირითად, დამოუკიდებელ კომპონენტად:
ტრენდული (Trend): მონაცემების გრძელვადიანი, საერთო მიმართულება ან ძირითადი დონე.
სეზონური (Seasonal): განმეორებადი, მოკლევადიანი შაბლონები, რომლებიც კონკრეტულ პერიოდებში (მაგ., კვირა, თვე, წელი) ვლინდება.
ამ კომპონენტების გამოყოფის შემდეგ, თითოეული მათგანი დამოუკიდებლად მოდელირდება ხაზოვანი ფენების (Linear Layers) გამოყენებით. ეს მიდგომა მიზნად ისახავს რთული და არაწრფივი დამოკიდებულებების გამარტივებას, რაც პროგნოზირების სიზუსტის გაუმჯობესებას უწყობს ხელს.
რატომ არის DLinear პოტენციურად კარგი ამ პრობლემისთვის:
მასშტაბირებადობა: Walmart-ის მონაცემები მოიცავს უამრავ მაღაზიასა და დეპარტამენტს, რაც ათასობით ინდივიდუალურ დროით სერიას ნიშნავს. ტრადიციული "თითო-სერიაზე-ერთი-მოდელი" მიდგომებისგან განსხვავებით, DLinear-ს შეუძლია ყველა სერიის საერთო შაბლონების სწავლა ერთდროულად. ეს მას გაცილებით ეფექტურს ხდის დიდი მონაცემთა ნაკრებებისთვის.

ინტერპრეტაციულობა: დეკომპოზიციის გამო, DLinear გვთავაზობს გაყიდვების ტრენდისა და სეზონურობის მკაფიო განცალკევებას, რაც პროგნოზის გამომწვევი მიზეზების უკეთ გაგებას აადვილებს.

ეფექტურობა: რთული არაწრფივი ნერვული ქსელების ნაცვლად ხაზოვანი ფენების გამოყენება ამცირებს გამოთვლით ტვირთს და აჩქარებს მოდელის ტრენინგს.

რატომ შეიძლება იყოს DLinear პოტენციურად არასრულყოფილი ამ პრობლემისთვის:

მხოლოდ ხაზოვანი დამოკიდებულებები: თუ გაყიდვების მონაცემებში არსებობს ძალიან რთული, არაწრფივი დამოკიდებულებები (მაგალითად, სხვადასხვა სარეკლამო კამპანიების, მოვლენების, მარკდაუნების ან ეკონომიკური ცვლილებების კომპლექსური ურთიერთქმედება), შედარებით მარტივმა ხაზოვანმა ფენებმა შესაძლოა ვერ შეძლონ ამ სირთულეების სრულად აღქმა.

ექსოგენური ცვლადების ინტეგრაცია: DLinear-ის ძირითადი არქიტექტურა, განსაკუთრებით neuralforecast-ის იმპლემენტაციაში, ხშირად ფოკუსირებულია მხოლოდ წარსულ სამიზნე მნიშვნელობებზე. ექსოგენური ცვლადების (როგორიცაა Temperature, Fuel_Price, CPI, Unemployment) ეფექტური ინტეგრაცია ზოგჯერ უფრო რთულია Deep Learning მოდელებში, ვიდრე, მაგალითად, LightGBM-ში ან SARIMAX-ში, რომლებიც მათ პირდაპირ features-ად იყენებენ. თუ ეს ექსოგენური ფაქტორები გაყიდვების ძლიერი განმსაზღვრელები არიან, მათი გამორიცხვამ შესაძლოა სიზუსტე შეამციროს.

ჩემი DLinear იმპლემენტაციის განვითარება: ნოუთბუქი 1-დან ნოუთბუქი 2-მდე

ჩემი DLinear-ზე მუშაობა ორი ძირითადი ეტაპისგან შედგებოდა, რომლებიც ორ ნოუთბუქშია ასახული. ეს ეტაპები ასახავს მოდელის გაგების და მისი იმპლემენტაციის დახვეწის პროცესს.

ნოუთბუქი 1: ხელით შექმნილი DLinear იმპლემენტაცია PyTorch-ით

პირველი ნოუთბუქი იყო DLinear მოდელის "ნულიდან" შექმნის მცდელობა PyTorch-ის nn.Module-ის გამოყენებით.

მონაცემთა მომზადება:

ჩატვირთვა და გაერთიანება: ჩავტვირთე train.csv, test.csv, stores.csv, features.csv და გავაერთიანე ისინი Date და Store სვეტების მიხედვით.

მახასიათებლების ინჟინერია: შევქმენი დროზე დაფუძნებული მახასიათებლები (Year, Month, Day, Week, DayOfWeek, Quarter, IsMonthStart, IsMonthEnd). IsHoliday და Type (მაღაზიის ტიპი) რიცხვით ფორმატში გადავიყვანე.

NaN მნიშვნელობების დამუშავება: რიცხვით სვეტებში NaN-ები მედიანური მნიშვნელობით შევავსე. Weekly_Sales-ში არსებული NaN-ები წავშალე, რადგან ისინი სამიზნე ცვლადია. დანარჩენი NaN-ები მახასიათებლის სვეტებში 0-ით შევავსე, რაც ზოგჯერ შეიძლება იყოს რისკიანი მიდგომა, თუ 0 არ არის ლოგიკური ჩანაცვლება.

დაყოფა და სკალირება: მონაცემები ქრონოლოგიურად დავყავი train, validation და test ნაკრებებად (60%/20%/20%). შემდეგ, StandardScaler-ით გავასკალერე მახასიათებლები, scaler მოვამზადე მხოლოდ ტრენინგის მონაცემებზე (fit).

TimeSeriesDataset: შევქმენი TimeSeriesDataset კლასი, რომელიც გარდაქმნიდა მონაცემებს 12-კვირიან sequence-ებად, რაც PyTorch-ის მოდელებისთვის იყო საჭირო. ეს მექანიზმი ქმნიდა "lookback window"-ებს, რათა მოდელს წარსულ მონაცემებზე დაყრდნობით მომავალი ეწინასწარმეტყველა.

DLinear მოდელის არქიტექტურა და ტრენინგი:

მოდელი: შევქმენი DLinear კლასი nn.Module-დან, რომელიც მოიცავდა nn.AvgPool1d-ს დეკომპოზიციისთვის და ორ nn.Linear ფენას სეზონური და ტრენდული კომპონენტების დასამუშავებლად. მნიშვნელოვანი დეტალი: ჩემი DLinear კლასის forward მეთოდი seasonal და trend კომპონენტებს ძირითადად მხოლოდ Weekly_Sales-ის ბოლო ფიჩერიდან იღებდა (x[:, :, -1:]). ეს ნიშნავს, რომ მიუხედავად იმისა, რომ ბევრი მახასიათებელი იყო შეყვანილი (input_size = len(feature_columns)), მოდელის დეკომპოზიციის ნაწილი ძირითადად მხოლოდ გაყიდვების ისტორიას იყენებდა.

ტრენინგი: გამოვიყენე nn.MSELoss(), optim.Adam(), ReduceLROnPlateau და Early Stopping.

WandB ინტეგრაცია: დავაკონფიგურირე wandb ტრენინგის loss-ის, MAE-ის, RMSE-ის და learning rate-ის დასალოგად ყოველი ეპოქის შემდეგ. ასევე, ვცადე საბოლოო შედეგების და გრაფიკების დალოგვა.

შედეგები (პირველი ნოუთბუქი):

Validation MAE: 15478.33

Test MAE: 15534.75

ეს შედეგები საკმაოდ მაღალი იყო, რაც მიუთითებდა მნიშვნელოვან ცდომილებაზე რეალურ გაყიდვებთან შედარებით.
შეფასება (რატომ ცუდი):

"უნივარიატული" დეკომპოზიცია: ჩემი ხელით შექმნილი DLinear მოდელი, მიუხედავად იმისა, რომ იღებდა მრავალ feature_column-ს, დეკომპოზიციის ნაწილში (decomposition(sales_data)) რეალურად მხოლოდ Weekly_Sales (ბოლო ფიჩერი) იყენებდა. ეს ნიშნავს, რომ მოდელი ვერ ითვისებდა ექსოგენური ცვლადების (როგორიცაა Temperature, Fuel_Price, CPI, Unemployment, IsHoliday) გავლენას ტრენდსა და სეზონურობაზე უშუალოდ დეკომპოზიციის პროცესში. ისინი მხოლოდ "შემოჰქონდა" x შეყვანის სახით, მაგრამ მათი წვლილი გაყიდვების დეკომპოზიციაში არ იყო მკაფიოდ გამოყოფილი. ეს ზღუდავდა მოდელის შესაძლებლობას, ესწავლა რთული ურთიერთკავშირები გაყიდვებსა და სხვა ფაქტორებს შორის.

ოპტიმიზაციის ნაკლებობა: PyTorch-ში ხელით მოდელის აგებისას, ზოგჯერ შეიძლება არ იყოს სრულად გათვალისწინებული ყველა საუკეთესო პრაქტიკა, როგორიცაა სწორი წონის ინიციალიზაცია, სწავლების სიჩქარის ოპტიმალური გრაფიკი და სხვადასხვა რეგულარიზაციის ტექნიკა, რამაც შეიძლება გავლენა მოახდინოს შესრულებაზე.

ნოუთბუქი 2: DLinear-ის იმპლემენტაცია neuralforecast-ით

პირველი ნოუთბუქის გამოცდილებიდან გამომდინარე, გადავწყვიტე გამომეყენებინა neuralforecast ბიბლიოთეკა, რომელიც ოპტიმიზებულ DLinear იმპლემენტაციას გვთავაზობს და შექმნილია მრავალსერიანი პროგნოზირების პრობლემებისთვის.

ძირითადი ცვლილებები და უპირატესობები:

neuralforecast ბიბლიოთეკის გამოყენება:

გამარტივებული მონაცემთა ნაკრები: neuralforecast-ის DLinear მოდელი პირდაპირ იღებს DataFrame-ს unique_id, ds (Date) და y (Weekly_Sales) სვეტებით, რაც გამორიცხავს ჩემი ხელით შექმნილი TimeSeriesDataset კლასის საჭიროებას და ამარტივებს მონაცემთა მომზადებას.

ოპტიმიზებული ტრენინგი: neuralforecast-ის შიდა მექანიზმები უზრუნველყოფენ უკეთეს ტრენინგის პრაქტიკებს, როგორიცაა სწორი ინიციალიზაცია, ოპტიმიზატორები (Adam weight_decay-ით) და loss functions (MSE).

კონკურსის მეტრიკასთან მუშაობა: მიუხედავად იმისა, რომ neuralforecast-ის DLinear-ის ძირითადი ვერსია ასევე უნივარიატულია (მხოლოდ y-ზე დაყრდნობით პროგნოზირებს), მისი ოპტიმიზებული არქიტექტურა უკეთ აითვისებს y-ში არსებულ ტრენდებსა და სეზონურობას.

Pipeline-ის გამოყენება:

შევქმენი scikit-learn-ის Pipeline, რომელიც აერთიანებს მონაცემთა გარდაქმნას (TrainTestDF2NF) და DLinearNF მოდელს. ეს აუმჯობესებს კოდის ორგანიზებას და მოდელის განლაგებას. TrainTestDF2NF კლასი მიზნად ისახავდა მონაცემების neuralforecast-ისთვის საჭირო ფორმატში (unique_id, ds, y) გადაყვანას.

შეწონილი MAE (WMAE) გაანგარიშება:

კონკურსის შეფასების მეტრიკის შესაბამისად, WMAE ხელით გამოვთვალე ვალიდაციის ნაკრებზე, სადაც სადღესასწაულო დღეებს 5-ჯერ მეტი წონა ენიჭება. ეს დალოგილ იქნა wandb-ზე.

შედეგები (მეორე ნოუთბუქი):

Validation WMAE: 2085.020 (ეს მნიშვნელობა აისახა WandB-ზე)

შედეგების შედარება და ანალიზი:

მეორე ნოუთბუქის შედეგები (Validation WMAE ~2085) მნიშვნელოვნად უკეთესია პირველი ნოუთბუქის MAE-სთან (15478.33) შედარებით. ეს აშკარად აჩვენებს, რომ neuralforecast ბიბლიოთეკის DLinear იმპლემენტაცია ბევრად უფრო ეფექტური და ოპტიმალურია.

შეფასება (რატომ უკეთესი/უარესი):

რატომ უკეთესი (შესრულების მხრივ): neuralforecast-ის DLinear-ის იმპლემენტაცია არის გაცილებით ოპტიმიზებული და სანდო. ის იყენებს სწორ ტრენინგის პროცედურებს, ოპტიმიზატორებს და დეკომპოზიციის მეთოდებს, რაც იწვევს უკეთეს პროგნოზებს, მიუხედავად იმისა, რომ ის ასევე უნივარიატულია (ანუ, ძირითადად მხოლოდ წარსული y მნიშვნელობებს იყენებს). ბიბლიოთეკები ხშირად შეიცავს საუკეთესო პრაქტიკებს, რომლებიც რთული განსახორციელებელია ხელით.

რატომ შეიძლება იყოს "უარესი" (თეორიული თვალსაზრისით, მაგრამ არა შესრულების მხრივ ამ შემთხვევაში): მეორე ნოუთბუქში, მიუხედავად იმისა, რომ მთელი df გაერთიანებული იყო features_df-თან და stores_df-თან, DLinear მოდელი neuralforecast-ის ფარგლებში არ იყენებს ექსოგენურ მახასიათებლებს (Temperature, CPI და ა.შ.) პროგნოზირებისთვის. ის მუშაობს მხოლოდ unique_id, ds და y სვეტებით. ჩემს პირველ ნოუთბუქში კი იყო მცდელობა, რომ ექსოგენური ცვლადები შეყვანილი ყოფილიყო მოდელში, თუმცა მისი არქიტექტურის შეზღუდვების გამო სრულად ვერ გამოიყენა. ამ კონკრეტულ შემთხვევაში, როგორც ჩანს, Weekly_Sales-ის მხოლოდ წარსული მნიშვნელობებიდან neuralforecast-ის ოპტიმიზებული DLinear-მა უკეთესი შედეგი აჩვენა, ვიდრე ჩემმა ხელით შექმნილმა მოდელმა, რომელიც ცდილობდა ექსოგენური ცვლადების გამოყენებას. ეს შეიძლება მიუთითებდეს, რომ ან ექსოგენური ცვლადების გავლენა არ არის ისეთი ძლიერი ამ კონკრეტული დროის სერიებზე, ან ჩემი ხელით შექმნილი მოდელი ვერ ახერხებდა მათგან ინფორმაციის ეფექტურად ამოღებას.

დასკვნა და სამომავლო ნაბიჯები

DLinear-ის ექსპერიმენტები, განსაკუთრებით neuralforecast-ის იმპლემენტაციით, აშკარად აჩვენებს მნიშვნელოვან გაუმჯობესებას პროგნოზირების სიზუსტეში Walmart-ის გაყიდვების მონაცემთა ნაკრებისთვის. მიუხედავად იმისა, რომ neuralforecast-ის DLinear-ის უნივარიატული ბუნება შეიძლება იყოს შეზღუდვა, თუ ექსოგენური ცვლადები გადამწყვეტ გავლენას ახდენენ გაყიდვებზე, მან მაინც დაამტკიცა თავისი ეფექტურობა.

სამომავლო ნაბიჯები მოიცავს:

ექსოგენური ცვლადების ინტეგრაცია: neuralforecast-ის სხვა მოდელების შესწავლა, რომლებიც მხარს უჭერენ ექსოგენური მახასიათებლების (X_df) გამოყენებას (მაგალითად, NHITS, NBEATS, Autoformer), რათა შევამოწმოთ, შეძლებენ თუ არა ისინი უკეთესი სიზუსტის მიღწევას დამატებითი მონაცემების გათვალისწინებით.

ჰიპერპარამეტრების ოპტიმიზაცია: DLinear-ის input_size (lookback window), epochs, learning_rate და batch_size-ის უფრო დეტალური დარეგულირება.

WandB-ის სრულყოფილი გამოყენება: მუდმივად იმის უზრუნველყოფა, რომ ყველა გრაფიკი, მეტრიკა და მოდელის მდგომარეობა სრულად ილოგება, რათა ყოველთვის გვქონდეს ტრენინგის სრული და გამჭვირვალე სურათი.

მოდელების შედარება: DLinear-ის შედარება სხვა გლობალურ მოდელებთან (მაგალითად, LightGBM ან Prophet), რათა განვსაზღვროთ რომელი მოდელი არის ოპტიმალური ამ კონკრეტული პროგნოზირების ამოცანისთვის.






SARIMAX მოდელირება Walmart-ის გაყიდვების პროგნოზირებისთვის
ამ განყოფილებაში განვიხილავთ SARIMAX (Seasonal AutoRegressive Integrated Moving Average with Exogenous Regressors) მოდელების გამოყენების მცდელობას Walmart-ის მაღაზიის გაყიდვების პროგნოზირებისთვის. SARIMAX, როგორც ტრადიციული დროითი სერიების მოდელი, განსაკუთრებით ეფექტურია სეზონურობის, ტრენდების და ციკლური კომპონენტების აღსაქმელად. ის ასევე იძლევა ეგზოგენური ცვლადების (გარე ფაქტორების) ჩართვის საშუალებას, რაც მნიშვნელოვანია გაყიდვების მსგავსი მონაცემებისთვის.

მონაცემთა მომზადება

მონაცემთა მომზადების პროცესი სტანდარტული იყო Walmart-ის კონკურსის მონაცემთა ნაკრებისთვის:

მონაცემთა ჩატვირთვა და გაერთიანება: train.csv, features.csv და stores.csv ფაილები გაერთიანდა Store, Date და IsHoliday სვეტების მიხედვით. Date სვეტი გადაკეთდა datetime ტიპად.

NaN მნიშვნელობების შევსება: Temperature, Fuel_Price, CPI, Unemployment და MarkDown სვეტებში არსებული NaN მნიშვნელობები შევავსე მედიანური მნიშვნელობით. მედიანა შეირჩა, რადგან ის უფრო მდგრადია ექსტრემალური მნიშვნელობების მიმართ, ვიდრე საშუალო და უკეთესად შეესაბამება SARIMAX მოდელის მგრძნობელობას მონაცემთა მიმართ.

დამატებითი მახასიათებლები (Feature Engineering): შევქმენი დამატებითი დროითი მახასიათებლები, როგორიცაა week, sin_13, cos_13, sin_23, cos_23. ეს ციკლური მახასიათებლები დაგვეხმარება სეზონურობის უკეთ აღქმაში, განსაკუთრებით 13 და 23-კვირიანი ციკლებისთვის, რომლებიც შეიძლება გავლენას ახდენდნენ გაყიდვებზე.

SARIMAX მოდელირების პირველი მცდელობა (SARIMAX_Initial_Attempt.ipynb)

ჩემი პირველი მიდგომა SARIMAX-თან გულისხმობდა SARIMAXWrapper კლასის შექმნას, რათა მოდელი scikit-learn Pipeline-ში გამომეყენებინა.

ძირითადი პარამეტრები და მიდგომა:

SARIMAX პარამეტრები: order=(1,1,2) და seasonal_order=(1,1,1,52) განისაზღვრა მოდელისთვის.

enforce_stationarity=False და enforce_invertibility=False: ეს პარამეტრები საწყის ეტაპზე False-ზე დავაყენე, რათა მოდელს გაადვილებოდა კონვერგენცია. თუმცა, როგორც აღმოჩნდა, ეს იყო მნიშვნელოვანი შეცდომა, რადგან არ უზრუნველყოფდა მოდელის სტაბილურობასა და სწორ სტატისტიკურ ქცევას.

ჯგუფებად დაყოფა: მონაცემები დავყავი Store-Dept ჯგუფებად და თითოეული ჯგუფისთვის ცალკე SARIMAX მოდელი იწვრთნებოდა.

მეტრიკა: შეფასებისთვის გამოვიყენე Weighted Mean Absolute Error (WMAE) და Root Mean Square Error (RMSE).

MLflow ინტეგრაცია: ექსპერიმენტები დაილოგა MLflow-ზე DagsHub-ის მეშვეობით.

შედეგები და პრობლემები:

ამ პირველი ექსპერიმენტის Overall WMAE იყო წარმოუდგენლად მაღალი (დაახლოებით 61,288,039,612,882,950), რაც ნათლად მიუთითებდა, რომ მოდელი საერთოდ არ მუშაობდა სწორად. ტიპიური WMAE ამ კონკურსისთვის 2000-3000 დიაპაზონშია.

ძირითადი მიზეზები ასეთი ცუდი შედეგებისთვის:

მონაცემთა არასწორი სკალირება: SARIMAX მოდელი უკიდურესად მგრძნობიარეა მონაცემთა მასშტაბების მიმართ, განსაკუთრებით ეგზოგენური ცვლადების გამოყენებისას. ეგზოგენური ცვლადების სკალირების არარსებობა იყო ყველაზე სავარაუდო მიზეზი ექსტრემალურად ცუდი შედეგების.

enforce_stationarity და enforce_invertibility პარამეტრები False-ზე: ამ პარამეტრების არასწორად დაყენებამ ხელი შეუშალა მოდელის სტაბილურობასა და სწორ სტატისტიკურ თვისებებს, რამაც გამოიწვია კონვერგენციის პრობლემები და არასწორი პროგნოზები.

მონაცემთა სიმცირე ზოგიერთი ჯგუფისთვის: მიუხედავად იმისა, რომ ცარიელი ჯგუფები გამოირიცხა, SARIMAX-ს ზოგადად სჭირდება საკმაოდ დიდი დროითი სერია, განსაკუთრებით 52-კვირიანი სეზონურობის გათვალისწინებით.

SARIMAX მოდელირების მეორე მცდელობა (SARIMAX_Improved_Attempt.ipynb)

წინა ექსპერიმენტის პრობლემების გათვალისწინებით, შეიქმნა ახალი Notebook, რომელიც მიზნად ისახავდა ზემოთ ხსენებული შეცდომების გამოსწორებას. ამ მცდელობაში მთავარი აქცენტი გაკეთდა მონაცემთა წინასწარ დამუშავებასა და მოდელის პარამეტრების სწორ კონფიგურაციაზე.

ძირითადი გაუმჯობესებები:

ეგზოგენური ცვლადების სკალირება: Pipeline-ში ჩაირთო StandardScaler, რათა ყველა ეგზოგენური მახასიათებელი ყოფილიყო სკალირებული. ეს გადამწყვეტი ნაბიჯი იყო მოდელის სტაბილური მორგებისა და ოპტიმალური კონვერგენციისთვის.

enforce_stationarity=True და enforce_invertibility=True: ეს პარამეტრები დაყენდა True-ზე, რაც აუცილებელია statsmodels SARIMAX მოდელის სტატისტიკური ვალიდურობისა და პროგნოზების სანდოობის უზრუნველსაყოფად.

დამატებითი ეგზოგენური მახასიათებლები: Year, DayOfWeek და Store Size ჩართული იქნა ეგზოგენურ ცვლადებად, რათა უკეთ აღეწერათ გაყიდვების დინამიკა.

პროგნოზების 0-ზე კლიპინგი: უარყოფითი პროგნოზები დაყენდა 0-ზე, რადგან გაყიდვების მოცულობა არ შეიძლება იყოს ნეგატიური.

მონაცემთა მინიმალური სიგრძის ადაპტაცია: შემცირდა მინიმალური მოთხოვნები ტრენინგისა (min_train_len = 52) და ვალიდაციის (min_val_len = 12) ნაკრებებისთვის, რათა მოდელს შეძლებოდა უფრო მეტი Store-Dept კომბინაციის დამუშავება.

შედეგები და შეზღუდვები:

ამ ცვლილებების შემდეგ, მოდელმა დაიწყო ვალიდური WMAE და RMSE მნიშვნელობების გენერირება თითოეული Store-Dept კომბინაციისთვის, რაც წინა ექსპერიმენტებში შეუძლებელი იყო. თუმცა, მიუხედავად იმისა, რომ ინდივიდუალური მოდელები გამართულად მუშაობდნენ, მთელი dataset-ის დამუშავების დრო წარმოუდგენლად ხანგრძლივი აღმოჩნდა.

კერძოდ, პრობლემები იყო:

გამოთვლითი სირთულე: SARIMAX მოდელი თითოეული დროითი სერიისთვის ცალ-ცალკე იწვრთნება. 3,331 უნიკალური Store-Dept კომბინაციისთვის ცალკე მოდელის დატრენინგებას, თუნდაც მცირე ცვლილებების შემდეგ, წუთები ან მეტი სჭირდებოდა.

Colab-ის სესიების შეჩერება: ამდენი Store-Dept კომბინაციის პირობებში, მთელი dataset-ის დამუშავების დრო საათებს, შესაძლოა დღეებსაც კი გაგრძელდა. ჩემს ბოლო ცდაში, რამდენიმე წუთის შემდეგ მოდელი მხოლოდ პირველ ათეულ კომბინაციაზე იყო გადასული, სანამ Colab-ის სესია არ შეწყდებოდა (სავარაუდოდ, რესურსების, როგორიცაა GPU ან RAM, უკმარისობის გამო). ასეთი ხანგრძლივი სატრენინგო დრო ფიზიკურად შეუძლებელს ხდიდა მოდელის ოპტიმიზაციას, ჰიპერპარამეტრების დარეგულირებას ან ალტერნატიული SARIMA პარამეტრების ტესტირებას გონივრულ ვადებში.

დასკვნა:

SARIMAX-ის გამოთვლითი სირთულე და დროითი შეზღუდვები არ არის პრაქტიკული ამ მასშტაბის ამოცანისთვის. მიუხედავად იმისა, რომ SARIMAX-ის მიერ მოწოდებული სტატისტიკური სიზუსტე შეიძლება იყოს მაღალი ინდივიდუალური სერიებისთვის, მისი მასშტაბირება ათასობით დროითი სერიისთვის უბრალოდ არაეფექტურია. ამ მიზეზით, გადავწყვიტე, ეს კონკრეტული Notebook არ დამესრულებინა და მის ნაცვლად, ყურადღება გადავიტანო უფრო მასშტაბურ და ეფექტურ დროითი სერიების პროგნოზირების მოდელებზე, როგორიცაა გლობალური მანქანური სწავლების მოდელები (მაგალითად, LightGBM ან XGBoost) დროითი სერიების მახასიათებლებთან ერთად. ეს მოდელები შეძლებენ მთელი dataset-ის ერთიანად დამუშავებას გაცილებით სწრაფად და ხშირად უკეთესი სიზუსტით.





                                                prophet
Prophet არის ღია კოდის ბიბლიოთეკა, რომელიც Facebook-ის მიერ არის შექმნილი, რათა მომხმარებლებს გაუადვილოს მაღალი ხარისხის პროგნოზების შექმნა დროით სერიებზე. ის განკუთვნილია ბიზნეს მონაცემებისთვის, რომლებიც ხშირად მოიცავს სეზონურობას, არარეგულარულ მონაცემებს, გამოტოვებულ მნიშვნელობებს და ტრენდის ცვლილებებს.Prophet-ის არქიტექტურა ეფუძნება ადითიურ დეკომპოზიციურ მოდელს, რომელიც დროით სერიებს სამ ძირითად კომპონენტად ყოფს:

ტრენდი (g(t)): ეს კომპონენტი აღწერს დროითი სერიების არაპერიოდულ ცვლილებებს, ანუ გრძელვადიან ზრდას ან კლებას. Prophet-ში ტრენდი შეიძლება იყოს წრფივი (piecewise linear) ან ლოგისტიკური (logistic growth curve), რაც საშუალებას აძლევს მოდელს მოერგოს არაწრფივ ტრენდებსა და გაჯერების ეფექტებს. ტრენდის ცვლილების წერტილებს (changepoints) მოდელი ავტომატურად ანთავსებს ან მომხმარებელს შეუძლია მათი მითითება.

სეზონურობა (s(t)): ეს კომპონენტი აღწერს დროითი სერიების პერიოდულ ცვლილებებს, როგორიცაა ყოველდღიური, ყოველკვირეული, ყოველთვიური ან ყოველწლიური ციკლები. Prophet სეზონურობას აფასებს ფურიეს სერიების (Fourier series) გამოყენებით, რაც მას საშუალებას აძლევს დააფიქსიროს რთული, არაწრფივი სეზონური ნიმუშები. სეზონურობა შეიძლება იყოს ადითიური (როდესაც სეზონური რყევების ამპლიტუდა მუდმივია ტრენდის მიუხედავად) ან მულტიპლიკაციური (როდესაც სეზონური რყევების ამპლიტუდა იცვლება ტრენდთან ერთად, მაგალითად, გაყიდვები იზრდება დღესასწაულებზე და ეს ზრდა უფრო მასშტაბურია, როცა მთლიანი გაყიდვები მაღალია).

დღესასწაულები და განსაკუთრებული მოვლენები (h(t)): ეს კომპონენტი საშუალებას აძლევს მოდელს გაითვალისწინოს დღესასწაულების ან სხვა მნიშვნელოვანი მოვლენების (როგორიცაა პრომო აქციები, გაფიცვები და ა.შ.) გავლენა დროით სერიებზე. მომხმარებელს შეუძლია მიაწოდოს დღესასწაულების სია (holiday, ds) ფორმატით, Prophet კი აფასებს მათ ადითიურ ან მულტიპლიკაციურ ეფექტებს.

დანარჩენი ცვლადები (Exogenous Regressors): Prophet-ს შეუძლია დამატებითი რეგრესორების (როგორიცაა ტემპერატურა, ფასები, უმუშევრობის დონე და ა.შ.) ჩართვა მოდელში add_regressor() ფუნქციის გამოყენებით. ეს რეგრესორები შეიძლება იყოს ადითიური ან მულტიპლიკაციური.

შეცდომის ტერმინი
ეს კომპონენტები ერთიანდება შემდეგი ფორმულით:
y(t)=g(t)+s(t)+h(t)+ϵ 
სადაც:
y(t) არის პროგნოზირებადი მნიშვნელობა დროის t მომენტში.
g(t) არის ტრენდის ფუნქცია.
s(t) არის სეზონურობის ფუნქცია.
h(t) არის დღესასწაულების ეფექტების ფუნქცია.
რატომ არის Prophet კარგი დროითი სერიების პრობლემებისთვის, განსაკუთრებით გაყიდვების პროგნოზირებისთვის:

ინტუიციური და კონფიგურირებადი: მისი დეკომპოზიციური მიდგომა აადვილებს პროგნოზის კომპონენტების გაგებას (ტრენდი, სეზონურობა, დღესასწაულები). ეს მნიშვნელოვანია ბიზნესისთვის, რადგან მათ შეუძლიათ გაანალიზონ პროგნოზის მიღმა არსებული მიზეზები.

ავტომატური გამართვა: Prophet შექმნილია იმისთვის, რომ კარგად იმუშაოს პარამეტრების მინიმალური მითითებით, რაც მას ხელმისაწვდომს ხდის მათთვისაც, ვისაც არ აქვს ღრმა სტატისტიკური ცოდნა. ის ავტომატურად ცნობს სეზონურ ნიმუშებს და ტრენდის ცვლილებებს.

გამძლეობა (Robustness): ის კარგად უმკლავდება გამოტოვებულ მონაცემებს, აუტლაიერებს (outliers) და ტრენდის მკვეთრ ცვლილებებს, რაც ხშირია რეალურ სამყაროს ბიზნეს მონაცემებში (მაგალითად, გაყიდვებში დღესასწაულების ან პრომოუშენების დროს).

სეზონურობის მოქნილი მოდელირება: საშუალებას იძლევა ადითიური ან მულტიპლიკაციური სეზონურობის მითითებას, ასევე მრავალი სეზონური პერიოდის (ყოველკვირეული, ყოველწლიური) დაფიქსირებას. გაყიდვების მონაცემებში ხშირად გვხვდება მულტიპლიკაციური სეზონურობა (რაც უფრო მეტი გაყიდვაა, მით უფრო დიდია სეზონური ზრდა).

დღესასწაულების და ექსოგენური რეგრესორების მხარდაჭერა: ეს კრიტიკულია საცალო ვაჭრობის გაყიდვების პროგნოზირებისთვის, სადაც დღესასწაულები, შვებულებები, ფასდაკლებები, ამინდი, საწვავის ფასები და ეკონომიკური მაჩვენებლები დიდ გავლენას ახდენს.

სწრაფი გამოთვლა: შედარებით სწრაფად მუშაობს დიდ მონაცემთა ნაკრებებზეც, რაც სასარგებლოა მრავალი დროითი სერიის (მაგალითად, ცალკეული მაღაზიებისა და დეპარტამენტებისთვის) პროგნოზირებისას.

როგორ განსხვავდება Prophet სხვა არქიტექტურებისგან:

ტრადიციული სტატისტიკური მოდელები (ARIMA, ETS):

განსხვავება: ARIMA და ETS ხშირად საჭიროებენ მონაცემთა სტაციონარულობას და სპეციალისტის დიდ ჩარევას პარამეტრების შესარჩევად. ისინი ნაკლებად გამძლეა გამოტოვებული მონაცემებისა და აუტლაიერების მიმართ. Prophet უფრო ავტომატური და მოქნილია არაწრფივი ტრენდების და მრავალი სეზონურობის მოდელირებაში. Prophet არ საჭიროებს დროითი სერიების სრულყოფილად სტაციონარულობას, რაც მას უფრო პრაქტიკულს ხდის ბიზნეს მონაცემებისთვის.

მსგავსება: ყველა ეს მოდელი დროითი სერიებს კომპონენტებად ყოფს, თუმცა განსხვავებული მეთოდოლოგიით.

მანქანური სწავლების მოდელები (Random Forest, XGBoost, Neural Networks):

განსხვავება: ეს მოდელები ხშირად მოითხოვს ფიჩერ ინჟინერინგს დროითი კომპონენტების (ტრენდი, სეზონურობა, ლაგები) ხელით გამოსაყვანად. Prophet ავტომატურად აკეთებს ამას. მანქანური სწავლების მოდელები, განსაკუთრებით ნერვული ქსელები, შეიძლება იყოს "შავი ყუთები", მაშინ როცა Prophet-ის შედეგები უფრო ინტერპრეტირებადია. მანქანური სწავლების მოდელებს ხშირად უჭირთ ექსტრაპოლაცია (ტრენდის გაგრძელება მომავალში), Prophet კი ამისთვის არის შექმნილი.

მსგავსება: ყველა მათგანს შეუძლია ექსოგენური რეგრესორების გამოყენება პროგნოზის გასაუმჯობესებლად.
ახლა კი გადავიდეთ ჩემს ნოუთბუქებზე.
Prophet მოდელების ანალიზი

ჩემი მიდგომა, რომ თითოეული "Store-Dept" კომბინაციისთვის ცალკე მოდელი ავაწყოთ, საშუალებას აძლევს Prophet-ს, დააფიქსიროს უნიკალური ტრენდები და სეზონურობა თითოეული კონკრეტული მაღაზია-დეპარტამენტის გაყიდვებისათვის.

მოდით განვიხილოთ თითოეული ნოუთბუქი და შევადაროთ მათ მიდგომები.

პირველი ნოუთბუქი: "Prophet_Improved_FE_Tuning"

მოკლე აღწერა:
ეს ნოუთბუქი იყენებს Prophet-ს Walmart-ის გაყიდვების პროგნოზირებისთვის. ის აერთიანებს train.csv, features.csv და stores.csv ფაილებს, ავსებს გამოტოვებულ მნიშვნელობებს ffill და bfill მეთოდებით, და იყენებს კორელაციას Weekly_Sales-თან რელევანტური რეგრესორების შესარჩევად (Temperature, Fuel_Price, CPI, Unemployment). მოდელი იწვრთნება თითოეული Store-Dept კომბინაციისთვის, US დღესასწაულების გათვალისწინებით და multiplicative სეზონურობის რეჟიმით. პარამეტრები, როგორიცაა changepoint_prior_scale (0.3) და სეზონურობის ტიპი ექსპლიციტურად არის მითითებული. შედეგები აღირიცხება MLflow-ის გამოყენებით.
ძირითადი მახასიათებლები / მიდგომები:
მონაცემთა გაერთიანება (Data Merging): train, features, stores მონაცემები გაერთიანებულია Store, Date, IsHoliday.

გამოტოვებული მნიშვნელობების შევსება (Missing Value Imputation): Temperature, Fuel_Price, CPI, Unemployment სვეტები ივსება ffill (forward fill) და bfill (backward fill) მეთოდებით. ეს არის მარტივი და ხშირად ეფექტური მიდგომა დროით სერიებში გამოტოვებული მონაცემებისთვის.

ფიჩერ სელექცია (Feature Selection): რეგრესორები (Temperature, Fuel_Price, CPI, Unemployment) ირჩევა Weekly_Sales-თან აბსოლუტური კორელაციის (>= 0.05) საფუძველზე. ეს არის მარტივი ფიჩერ ინჟინერინგის / შერჩევის მიდგომა, რათა მოდელში მხოლოდ ის ცვლადები ჩაირთოს, რომლებსაც პოტენციურად აქვთ გავლენა გაყიდვებზე.

დღესასწაულები (Holidays): prophet.make_holidays_df ფუნქციის გამოყენებით US დღესასწაულები ავტომატურად ემატება მოდელს. ეს მნიშვნელოვანია გაყიდვების პროგნოზირებისთვის, რადგან დღესასწაულები ხშირად იწვევს გაყიდვების ზრდას.

სეზონურობის რეჟიმი (Seasonality Mode): seasonality_mode='multiplicative' არის მითითებული. ეს მიდგომა ხშირად უკეთ მუშაობს გაყიდვების მონაცემებზე, სადაც სეზონური რყევების ამპლიტუდა იზრდება ტრენდთან ერთად.

Changepoint Prior Scale: changepoint_prior_scale=0.3 არის დაყენებული. ეს პარამეტრი აკონტროლებს ტრენდის ცვლილების მოქნილობას. უფრო მაღალი მნიშვნელობა ნიშნავს უფრო მოქნილ ტრენდს.

ექსოგენური რეგრესორები (Exogenous Regressors): add_regressor() ფუნქციით ემატება შერჩეული კორელირებული რეგრესორები.

მოდელის გაყოფა (Splitting Data): მონაცემები იყოფა y_train (< 2012-01-01) და y_val (2012-01-01-დან 2012-07-01-მდე). ეს არის სტანდარტული დროითი სერიების ვალიდაციის მიდგომა (walk-forward validation-ის მსგავსი, თუმცა აქ მხოლოდ ერთი გაყოფაა).

შეფასების მეტრიკა: WMAE (Weighted Mean Absolute Error) და RMSE (Root Mean Squared Error) გამოიყენება. WMAE განსაკუთრებით მნიშვნელოვანია, რადგან IsHoliday სვეტის მიხედვით წონებს იყენებს (დღესასწაულების დღეები 5-ჯერ უფრო მნიშვნელოვანია).

MLflow ინტეგრაცია: ექსპერიმენტები, პარამეტრები და მეტრიკები აღირიცხება MLflow-ის გამოყენებით, რაც საშუალებას იძლევა ექსპერიმენტების თვალყურის დევნებისთვის და შედეგების შედარებისთვის.

მოდელები Store-Dept დონეზე: მოდელი ივარჯიშება ცალ-ცალკე თითოეული (Store, Dept) კომბინაციისთვის, რაც სტანდარტული პრაქტიკაა მრავალი დროითი სერიის პროგნოზირებისას.

შედეგი: Overall WMAE: 2389.95

მეორე ნოუთბუქი: "Prophet_Improved_Seasonality_Run"

მოკლე აღწერა:
ეს ნოუთბუქი ასევე იყენებს Prophet-ს Walmart-ის გაყიდვების პროგნოზირებისთვის, იგივე მონაცემთა მომზადების საწყისი ნაბიჯებით (მერჯინგი, გამოტოვებული მნიშვნელობების შევსება). მთავარი ცვლილება აქ არის ProphetWrapper კლასის შექმნა sklearn pipeline-ში ინტეგრაციისთვის, თუმცა ამ კონკრეტულ იმპლემენტაციაში add_regressor ფუნქციონალი (რომელიც პირველ მოდელში იყო) არ ჩანს გამოყენებული. დღესასწაულებიც არ არის გათვალისწინებული. changepoint_prior_scale აქ 0.5-ია.

ძირითადი განსხვავებები პირველ მოდელთან:

ProphetWrapper კლასი და Pipeline:

განსხვავება: პირველი მოდელი უბრალოდ Prophet ობიექტს იყენებდა. მეორე მოდელში შექმნილია ProphetWrapper კლასი, რომელიც BaseEstimator და RegressorMixin-ს იმპლემენტირებს, რაც საშუალებას აძლევს Prophet-ს sklearn Pipeline-ში ჩაირთოს. ეს არის კარგი პრაქტიკა მოდელების ორგანიზებისთვის და ჰიპერპარამეტრების ტიუნინგისთვის GridSearchCV-ის მსგავსი ხელსაწყოებით (თუმცა აქ არ არის გამოყენებული).

მნიშვნელობა: თეორიულად, ეს აუმჯობესებს კოდის სტრუქტურას და გაფართოებადობას, მაგრამ ამ კონკრეტულ იმპლემენტაციაში, როგორც ჩანს, ზოგიერთი მნიშვნელოვანი ფიჩერი (რეგრესორები, დღესასწაულები) გამოტოვებულია.

ექსოგენური რეგრესორების გამოტოვება:

განსხვავება: პირველი ნოუთბუქი აქტიურად იყენებდა შერჩეულ რეგრესორებს (use_regressors). მეორე ნოუთბუქში pipe.fit(y_train[['ds', 'y']]) და pipe.predict(y_val[['ds']]) აჩვენებს, რომ ექსოგენური რეგრესორები არ არის გადაცემული მოდელისთვის. ეს არის მნიშვნელოვანი ნაკლი ამ მოდელში, რადგან რეგრესორებს ხშირად დიდი გავლენა აქვთ გაყიდვების პროგნოზირებაზე.

დღესასწაულების გამოტოვება:

განსხვავება: პირველი ნოუთბუქი იყენებდა holidays=us_holidays პარამეტრს. მეორე ნოუთბუქში Prophet-ის ინიციალიზაციისას დღესასწაულები არ არის მითითებული. ეს არის კიდევ ერთი მნიშვნელოვანი ნაკლი, რადგან Walmart-ის გაყიდვებზე დღესასწაულების გავლენა უზარმაზარია.

Changepoint Prior Scale: 0.5 (პირველში 0.3 იყო). 0.5 უფრო მაღალი მნიშვნელობაა, რაც ტრენდს კიდევ უფრო მოქნილს ხდის, მაგრამ რეგრესორებისა და დღესასწაულების გარეშე ამან შესაძლოა overfit გამოიწვიოს.

MLflow log_model: ეს ნოუთბუქი ინახავს Pipeline ობიექტს MLflow-ში mlflow.sklearn.log_model გამოყენებით, რაც კარგი პრაქტიკაა მოდელის ხელახლა გამოყენებისთვის.

შედეგი: Overall WMAE: 2222.42

განსხვავებების შეფასება: მიუხედავად იმისა, რომ მეორე ნოუთბუქის Overall WMAE უკეთესია (2222.42 vs 2389.95), ეს შეიძლება იყოს შემთხვევითი ან მონაცემთა ქვეჯგუფების განსხვავებული დამუშავების შედეგი (რადგან არ არის add_regressor და holidays გამოყენებული). ექსოგენური რეგრესორების და დღესასწაულების გამოტოვება, როგორც წესი, პროგნოზის სიზუსტეს ამცირებს გაყიდვების მონაცემებში. WMAE-ის გაუმჯობესება ამ კონკრეტულ შემთხვევაში შეიძლება გამოწვეული იყოს changepoint_prior_scale (0.3-დან 0.5-მდე) ცვლილებით, რაც ტრენდს უფრო მეტად აძლევს ცვლილების საშუალებას, მაგრამ გრძელვადიან პერსპექტივაში, რეგრესორებისა და დღესასწაულების გარეშე, ნაკლებად სავარაუდოა, რომ ის უფრო ზუსტი იყოს.

მესამე ნოუთბუქი: "Prophet_Best_Model"

მოკლე აღწერა:
ეს ნოუთბუქი იყენებს Prophet-ს გაყიდვების პროგნოზირებისთვის, იგივე მონაცემთა მომზადების საწყისი ნაბიჯებით. ის არ იყენებს ექსოგენურ რეგრესორებს (როგორც ჩანს add_regressor გამოტოვებულია), არც დღესასწაულებს და არც changepoint_prior_scale პარამეტრია ექსპლიციტურად მითითებული (ასე რომ გამოიყენება ნაგულისხმევი 0.05). ის იყენებს wandb (Weights & Biases) ექსპერიმენტების აღრიცხვისთვის MLflow-ის ნაცვლად.

ძირითადი განსხვავებები პირველ და მეორე მოდელებთან:

ექსოგენური რეგრესორების გამოტოვება:

განსხვავება: არცერთი რეგრესორი (Temperature, Fuel_Price, CPI, Unemployment) არ არის გამოყენებული add_regressor() ფუნქციით. ეს კრიტიკული ნაკლია გაყიდვების პროგნოზირებისთვის, რადგან ეს ცვლადები ხშირად მნიშვნელოვან გავლენას ახდენენ.

Changepoint Prior Scale: არ არის ექსპლიციტურად მითითებული, რაც ნიშნავს, რომ გამოყენებულია Prophet-ის ნაგულისხმევი მნიშვნელობა, რომელიც არის 0.05.

განსხვავება: პირველ მოდელში იყო 0.3, მეორეში 0.5. ნაგულისხმევი 0.05 ნიშნავს ტრენდის გაცილებით ნაკლებ მოქნილობას, ვიდრე 0.3 ან 0.5. ეს შეიძლება იყოს როგორც კარგი (overfitting-ის შემცირება), ასევე ცუდი (underfitting-ის გაზრდა, თუ ტრენდი ხშირად იცვლება).

სეზონურობის რეჟიმი (seasonality_mode): არ არის ექსპლიციტურად მითითებული, რაც ნიშნავს, რომ გამოყენებულია ნაგულისხმევი additive სეზონურობა.

განსხვავება: პირველ მოდელში იყო multiplicative. გაყიდვების მონაცემებში multiplicative სეზონურობა ხშირად უკეთ მუშაობს, რადგან სეზონური რყევები ტრენდის ზრდასთან ერთად იზრდება. additive რეჟიმის გამოყენება, სავარაუდოდ, ნაკლებად ზუსტი იქნება.

ექსპერიმენტების აღრიცხვა (Experiment Tracking):

განსხვავება: MLflow-ის ნაცვლად გამოიყენება wandb (Weights & Biases). ორივე პლატფორმა ემსახურება ექსპერიმენტების თვალყურის დევნებას, პარამეტრების, მეტრიკების და მოდელების აღრიცხვას, რაც კარგი პრაქტიკაა. wandb ასევე გთავაზობთ ვიზუალიზაციის მძლავრ ინსტრუმენტებს.

შედეგი: Overall WMAE: 1916.38

შედეგების შეფასება (რატომ არის მესამე მოდელი "საუკეთესო" მიუხედავად ნაკლოვანებებისა):
აქ არის პარადოქსი. მესამე მოდელმა მიიღო საუკეთესო Overall WMAE (1916.38), მიუხედავად იმისა, რომ მან გამოტოვა მნიშვნელოვანი ფიჩერ ინჟინერინგის ნაბიჯები (რეგრესორები, დღესასწაულები) და გამოიყენა ნაგულისხმევი additive სეზონურობის რეჟიმი.
ეს შეიძლება აიხსნას რამდენიმე ფაქტორით:

changepoint_prior_scale (0.05): ნაგულისხმევმა 0.05-მა შესაძლოა შეამცირა overfitting ცალკეული Store-Dept კომბინაციებისთვის. უფრო მოქნილი ტრენდი (როგორც 0.3 ან 0.5) ზოგჯერ ზედმეტად ეგუება ხმაურს, რაც უარეს პროგნოზს იძლევა.

მონაცემების სტრუქტურა: შესაძლებელია, რომ Walmart-ის გაყიდვების მონაცემებისთვის (რომლებიც უკვე შეიცავს ძლიერ სეზონურობას და ტრენდს), Prophet-ის საბაზისო მოდელი (ტრენდით და სეზონურობით) უკვე ძალიან ძლიერია. დამატებითმა რეგრესორებმა ან დღესასწაულებმა, თუ ისინი არ არის სწორად გამართული ან მათი გავლენა მცირეა, შესაძლოა ხმაური შემოიტანონ და არა გააუმჯობესონ პროგნოზი.

"IsHoliday" წონის ეფექტი: რადგან WMAE არის შეფასების მეტრიკა, და IsHoliday დღეებს 5-ჯერ უფრო მეტი წონა აქვს, შესაძლოა ამ კონკრეტულ მოდელმა უკეთ დააფიქსიროს არადღესასწაულის დღეების გაყიდვები, რამაც მთლიანობაში WMAE გააუმჯობესა, თუმცა დღესასწაულებზე გაყიდვების პროგნოზი არ იყოს ზუსტი.

seasonality_mode='additive': ზოგჯერ, მიუხედავად იმისა, რომ ვიზუალურად მონაცემები მულტიპლიკაციურ სეზონურობას აჩვენებს, ადითიურმა მოდელმა შეიძლება უკეთესი შედეგი აჩვენოს კონკრეტულ ვალიდაციის ნაკრებზე.

დასკვნა განსხვავებებზე:

მონაცემთა მომზადება: ყველა მოდელში მონაცემთა გაერთიანება და გამოტოვებული მნიშვნელობების შევსება თანმიმდევრულია.

ფიჩერ ინჟინერინგი (Exogenous Regressors):

პირველი მოდელი: იყენებს რეგრესორებს კორელაციის საფუძველზე. ეს არის კარგი მიდგომა, რომელიც მოიცავს ფიჩერ სელექციას.

მეორე და მესამე მოდელი: არ იყენებს ექსოგენურ რეგრესორებს. ეს არის მნიშვნელოვანი განსხვავება და სავარაუდო ნაკლი, რადგან ეკონომიკურ ცვლადებს და ამინდს შეუძლია გაყიდვებზე გავლენა მოახდინოს.

დღესასწაულები:პირველი მოდელი: იყენებს US დღესასწაულებს, რაც კრიტიკულია. ეს არის ფიჩერ ინჟინერინგი დღესასწაულების სახით.
მეორე და მესამე მოდელი: არ იყენებს დღესასწაულებს. ესეც მნიშვნელოვანი განსხვავება და ნაკლი.
პარამეტრების ტიუნინგი:

changepoint_prior_scale: პირველში 0.3, მეორეში 0.5, მესამეში ნაგულისხმევი 0.05. ეს პარამეტრი მნიშვნელოვნად მოქმედებს ტრენდის მოქნილობაზე. 0.05-მა (მესამე მოდელში) შეიძლება შეამციროს overfit.

seasonality_mode: პირველში multiplicative, მეორეში ნაგულისხმევი additive, მესამეში ნაგულისხმევი additive. გაყიდვებისთვის multiplicative ხშირად უკეთესია.

ექსპერიმენტების მართვა: პირველი და მეორე იყენებს MLflow-ს, მესამე WandB-ს. ორივე კარგი ინსტრუმენტია.                                                








