ამ პროექტში ვაკეთებთ Walmart მაღაზიების გაყიდვების პროგნოზირებას. 
მონაცემების გასამზადებლად შევაერთეთ მოცემული train დატასეტი features და stores დატასეტებთან.
დავსპლიტეთ დამერჯილი დატასეტი. ვჰენდლავთ Nan ველიუებს და კატეგორიულები ცვლადები გადაგვყავს ნუმერიქალ ცვლადებში.

XGBoost:
საუკეთესო შედეგი:
n_estimators: 2000
verbosity: 1
wmae: 2815
ოპტიმიზაციები:
დავამატე Year, Month, Week, Day, DayOfWeek, IsMonthStart, IsMonthEnd, IsWeekend, და Quarter 
დავატრენინგე მოდელი y′ = log(Weekly_Sales + 1) და პროგნოზი შევაქციე ამ ფუნქციით exp(y′) - 1, ვარიაციის სტაბილიზაციისთვის.
პარამეტრების n_estimators, max_depth, learning_rate, subsample, colsample_bytree და min_child_weight სხვადასხვა კომბინაციებიც მოსვინჯე, თუმცა wmae მივიღე 3370. შედეგად ოპტიმიზაციებამდე არსებული მოდელი უკეთეს შედეგს დებდა. ამიტომ ვცადე randomizedSearchCV, თუმცა შედეგი მაინც არა დამაკმაყოფილებელი იყო და გადავწყვიტე დამემატებინა რეგულარიზაციები. gamma: 0.1, 'reg_alpha': 0.1, 'reg_lambda': 1.0 .(Gamma adds a minimum loss reduction threshold for making a split, discouraging the model from growing branches that don’t contribute substantive improvements, L1 (reg_alpha) and L2 (reg_lambda) penalties shrink coefficient magnitudes and help prevent individual leaves from drifting too far; and by subsampling at the tree‑level, the model sees fewer features per split which further guards against overfitting).

LightGBM:
n_estimators: 2000, learning_rate: 0.015, num_leaves: 70, max_depth: 14, Wmae: 2773.
n_estimators: 3000, learning_rate: 0.001, num_leaves: 80, max_depth: 14, Wmae: 5050.
n_estimators: 3000, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2741.
n_estimators: 3500, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2682.
ციკლური ენკოდირება სეზონური პატერნებისთვის(სეზონები ხო ციკლურია და ამიტომ სინუსის და კოსინუსის ფუნქციები გამოვიყენე ტრანსფორმაციისთვის). დავამატე ფიჩერები quarter, dayofyear, weekofyear. შევქემენი ბინარული ინდიკატორები თითოეული markdown ტიპისთვის. დავუმატე შემდეგი ფუნქციები: Total_MarkDown, რომელიც ყველა markdown მნიშვნელობის ჯამს წარმოადგენს, MarkDown_Count, რომელიც აქტიური მარკდაუნების რაოდენობას აღნიშნავს, და Has_MarkDown, რომელიც ბინარული ინდიკატორია ნებისმიერი markdown-ის არსებობისთვის. ახალი ფუნქციები: CPI_Unemployment_Ratio, რომელიც ეკონომიკური ჯანმრთელობის ინდიკატორია, CPI_Normalized, რომელიც სტანდარტიზებული CPI მნიშვნელობებია, და Unemployment_Normalized სტანდარტიზებული უმუშევრობის მაჩვენებლებისთვის. ეს იჭერს ეკონომიკურ პირობებს, რომლებიც მოქმედებს მომხმარებლის ხარჯვაზე. Lag ფუნქციები შეიცავს წინა 1, 2, 4 და 8 კვირის გაყიდვებს. ყველა lag ფუნქცია იქმნება Store და Dept მიხედვით დაჯგუფებული Weekly_Sales-ის შესაბამისი პერიოდით გადატანით. შემდეგი ფუნქციები: Sales_Mean და Sales_Std 4, 8, 12 კვირის window-ებისთვის. ეს იჭერს გაყიდვების ტენდენციებს და ცვალებადობის ნიმუშებს. ფუნქციები: Is_Christmas_Period (15-31 დეკემბრისთვის), Is_Thanksgiving_Period (20-30 ნოემბრისთვის), Is_Back_To_School (აგვისტო და სექტემბრის დასაწყისისთვის). ეს იჭერს ძირითადი შოპინგის სეზონებს განსხვავებული ნიმუშებით. ფუნქცია Store_Dept_Interaction = Store × 1000 + Dept, რომელიც იჭერს უნიკალურ მაღაზია-დეპარტამენტის კომბინაციებს. პარამეტრები შეიცავს reg_alpha=0.1 L1 რეგულარიზაციისთვის, reg_lambda=0.1 L2 რეგულარიზაციისთვის, min_split_gain=0.1 split-ების მინიმალური gain-ისთვის, min_child_weight=0.001 მინიმალური child weight-ისთვის, subsample=0.8 რიგების sampling-ისთვის, colsample_bytree=0.8 სვეტების sampling-ისთვის. ეს ამცირებს overfitting-ს და აუმჯობესებს generalization-ს. წმაე: 1877.


N-BEATS: 
თავდაპირველი N-BEATS მოდელი early stopping პრობლემას აწყდებოდა 18-ე ეპოქზე, რაც იწვევდა არაოპტიმალურ შედეგებს. ეს იყო patience=10-ის გამოდა ასევე მაღალი learning_rate=0.001-ის გამო.








DLinear მოდელების ევოლუცია
ამ პროექტში Walmart-ის მაღაზიების ყოველკვირეული გაყიდვების პროგნოზირების ამოცანას ვეხები, კონკრეტულად კი Deep Learning არქიტექტურა DLinear-ის გამოყენებას. ეს არის მნიშვნელოვანი გამოწვევა საცალო ვაჭრობის ანალიტიკაში, რომელიც მოითხოვს მძლავრ და მასშტაბირებად გადაწყვეტილებებს. ჩემი მუშაობის მიზანი იყო DLinear-ის შესაძლებლობების შეფასება ამ მრავალსერიანი დროითი მონაცემების პროგნოზირებისთვის და მისი იმპლემენტაციის დახვეწა.

DLinear მოდელის მიმოხილვა: როგორ მუშაობს და რატომ არის კარგი/ცუდი ამ ამოცანისთვის?

DLinear (Decomposition Linear) არის Deep Learning არქიტექტურა, რომელიც ეფუძნება დროითი სერიების დეკომპოზიციას. მისი მთავარი იდეაა დროითი სერიის დაყოფა ორ ძირითად, დამოუკიდებელ კომპონენტად:

ტრენდული (Trend): მონაცემების გრძელვადიანი, საერთო მიმართულება ან ძირითადი დონე.

სეზონური (Seasonal): განმეორებადი, მოკლევადიანი შაბლონები, რომლებიც კონკრეტულ პერიოდებში (მაგ., კვირა, თვე, წელი) ვლინდება.

ამ კომპონენტების გამოყოფის შემდეგ, თითოეული მათგანი დამოუკიდებლად მოდელირდება ხაზოვანი ფენების (Linear Layers) გამოყენებით. ეს მიდგომა მიზნად ისახავს რთული და არაწრფივი დამოკიდებულებების გამარტივებას, რაც პროგნოზირების სიზუსტის გაუმჯობესებას უწყობს ხელს.

რატომ არის DLinear პოტენციურად კარგი ამ პრობლემისთვის:

მასშტაბირებადობა: Walmart-ის მონაცემები მოიცავს უამრავ მაღაზიასა და დეპარტამენტს, რაც ათასობით ინდივიდუალურ დროით სერიას ნიშნავს. ტრადიციული "თითო-სერიაზე-ერთი-მოდელი" მიდგომებისგან განსხვავებით, DLinear-ს შეუძლია ყველა სერიის საერთო შაბლონების სწავლა ერთდროულად. ეს მას გაცილებით ეფექტურს ხდის დიდი მონაცემთა ნაკრებებისთვის.

ინტერპრეტაციულობა: დეკომპოზიციის გამო, DLinear გვთავაზობს გაყიდვების ტრენდისა და სეზონურობის მკაფიო განცალკევებას, რაც პროგნოზის გამომწვევი მიზეზების უკეთ გაგებას აადვილებს.

ეფექტურობა: რთული არაწრფივი ნერვული ქსელების ნაცვლად ხაზოვანი ფენების გამოყენება ამცირებს გამოთვლით ტვირთს და აჩქარებს მოდელის ტრენინგს.

რატომ შეიძლება იყოს DLinear პოტენციურად არასრულყოფილი ამ პრობლემისთვის:

მხოლოდ ხაზოვანი დამოკიდებულებები: თუ გაყიდვების მონაცემებში არსებობს ძალიან რთული, არაწრფივი დამოკიდებულებები (მაგალითად, სხვადასხვა სარეკლამო კამპანიების, მოვლენების, მარკდაუნების ან ეკონომიკური ცვლილებების კომპლექსური ურთიერთქმედება), შედარებით მარტივმა ხაზოვანმა ფენებმა შესაძლოა ვერ შეძლონ ამ სირთულეების სრულად აღქმა.

ექსოგენური ცვლადების ინტეგრაცია: DLinear-ის ძირითადი არქიტექტურა, განსაკუთრებით neuralforecast-ის იმპლემენტაციაში, ხშირად ფოკუსირებულია მხოლოდ წარსულ სამიზნე მნიშვნელობებზე. ექსოგენური ცვლადების (როგორიცაა Temperature, Fuel_Price, CPI, Unemployment) ეფექტური ინტეგრაცია ზოგჯერ უფრო რთულია Deep Learning მოდელებში, ვიდრე, მაგალითად, LightGBM-ში ან SARIMAX-ში, რომლებიც მათ პირდაპირ features-ად იყენებენ. თუ ეს ექსოგენური ფაქტორები გაყიდვების ძლიერი განმსაზღვრელები არიან, მათი გამორიცხვამ შესაძლოა სიზუსტე შეამციროს.

ჩემი DLinear იმპლემენტაციის განვითარება: ნოუთბუქი 1-დან ნოუთბუქი 2-მდე

ჩემი DLinear-ზე მუშაობა ორი ძირითადი ეტაპისგან შედგებოდა, რომლებიც ორ ნოუთბუქშია ასახული. ეს ეტაპები ასახავს მოდელის გაგების და მისი იმპლემენტაციის დახვეწის პროცესს.

ნოუთბუქი 1: ხელით შექმნილი DLinear იმპლემენტაცია PyTorch-ით

პირველი ნოუთბუქი იყო DLinear მოდელის "ნულიდან" შექმნის მცდელობა PyTorch-ის nn.Module-ის გამოყენებით.

მონაცემთა მომზადება:

ჩატვირთვა და გაერთიანება: ჩავტვირთე train.csv, test.csv, stores.csv, features.csv და გავაერთიანე ისინი Date და Store სვეტების მიხედვით.

მახასიათებლების ინჟინერია: შევქმენი დროზე დაფუძნებული მახასიათებლები (Year, Month, Day, Week, DayOfWeek, Quarter, IsMonthStart, IsMonthEnd). IsHoliday და Type (მაღაზიის ტიპი) რიცხვით ფორმატში გადავიყვანე.

NaN მნიშვნელობების დამუშავება: რიცხვით სვეტებში NaN-ები მედიანური მნიშვნელობით შევავსე. Weekly_Sales-ში არსებული NaN-ები წავშალე, რადგან ისინი სამიზნე ცვლადია. დანარჩენი NaN-ები მახასიათებლის სვეტებში 0-ით შევავსე, რაც ზოგჯერ შეიძლება იყოს რისკიანი მიდგომა, თუ 0 არ არის ლოგიკური ჩანაცვლება.

დაყოფა და სკალირება: მონაცემები ქრონოლოგიურად დავყავი train, validation და test ნაკრებებად (60%/20%/20%). შემდეგ, StandardScaler-ით გავასკალერე მახასიათებლები, scaler მოვამზადე მხოლოდ ტრენინგის მონაცემებზე (fit).

TimeSeriesDataset: შევქმენი TimeSeriesDataset კლასი, რომელიც გარდაქმნიდა მონაცემებს 12-კვირიან sequence-ებად, რაც PyTorch-ის მოდელებისთვის იყო საჭირო. ეს მექანიზმი ქმნიდა "lookback window"-ებს, რათა მოდელს წარსულ მონაცემებზე დაყრდნობით მომავალი ეწინასწარმეტყველა.

DLinear მოდელის არქიტექტურა და ტრენინგი:

მოდელი: შევქმენი DLinear კლასი nn.Module-დან, რომელიც მოიცავდა nn.AvgPool1d-ს დეკომპოზიციისთვის და ორ nn.Linear ფენას სეზონური და ტრენდული კომპონენტების დასამუშავებლად. მნიშვნელოვანი დეტალი: ჩემი DLinear კლასის forward მეთოდი seasonal და trend კომპონენტებს ძირითადად მხოლოდ Weekly_Sales-ის ბოლო ფიჩერიდან იღებდა (x[:, :, -1:]). ეს ნიშნავს, რომ მიუხედავად იმისა, რომ ბევრი მახასიათებელი იყო შეყვანილი (input_size = len(feature_columns)), მოდელის დეკომპოზიციის ნაწილი ძირითადად მხოლოდ გაყიდვების ისტორიას იყენებდა.

ტრენინგი: გამოვიყენე nn.MSELoss(), optim.Adam(), ReduceLROnPlateau და Early Stopping.

WandB ინტეგრაცია: დავაკონფიგურირე wandb ტრენინგის loss-ის, MAE-ის, RMSE-ის და learning rate-ის დასალოგად ყოველი ეპოქის შემდეგ. ასევე, ვცადე საბოლოო შედეგების და გრაფიკების დალოგვა.

შედეგები (პირველი ნოუთბუქი):

Validation MAE: 15478.33

Test MAE: 15534.75

ეს შედეგები საკმაოდ მაღალი იყო, რაც მიუთითებდა მნიშვნელოვან ცდომილებაზე რეალურ გაყიდვებთან შედარებით.
შეფასება (რატომ ცუდი):

"უნივარიატული" დეკომპოზიცია: ჩემი ხელით შექმნილი DLinear მოდელი, მიუხედავად იმისა, რომ იღებდა მრავალ feature_column-ს, დეკომპოზიციის ნაწილში (decomposition(sales_data)) რეალურად მხოლოდ Weekly_Sales (ბოლო ფიჩერი) იყენებდა. ეს ნიშნავს, რომ მოდელი ვერ ითვისებდა ექსოგენური ცვლადების (როგორიცაა Temperature, Fuel_Price, CPI, Unemployment, IsHoliday) გავლენას ტრენდსა და სეზონურობაზე უშუალოდ დეკომპოზიციის პროცესში. ისინი მხოლოდ "შემოჰქონდა" x შეყვანის სახით, მაგრამ მათი წვლილი გაყიდვების დეკომპოზიციაში არ იყო მკაფიოდ გამოყოფილი. ეს ზღუდავდა მოდელის შესაძლებლობას, ესწავლა რთული ურთიერთკავშირები გაყიდვებსა და სხვა ფაქტორებს შორის.

ოპტიმიზაციის ნაკლებობა: PyTorch-ში ხელით მოდელის აგებისას, ზოგჯერ შეიძლება არ იყოს სრულად გათვალისწინებული ყველა საუკეთესო პრაქტიკა, როგორიცაა სწორი წონის ინიციალიზაცია, სწავლების სიჩქარის ოპტიმალური გრაფიკი და სხვადასხვა რეგულარიზაციის ტექნიკა, რამაც შეიძლება გავლენა მოახდინოს შესრულებაზე.

ნოუთბუქი 2: DLinear-ის იმპლემენტაცია neuralforecast-ით

პირველი ნოუთბუქის გამოცდილებიდან გამომდინარე, გადავწყვიტე გამომეყენებინა neuralforecast ბიბლიოთეკა, რომელიც ოპტიმიზებულ DLinear იმპლემენტაციას გვთავაზობს და შექმნილია მრავალსერიანი პროგნოზირების პრობლემებისთვის.

ძირითადი ცვლილებები და უპირატესობები:

neuralforecast ბიბლიოთეკის გამოყენება:

გამარტივებული მონაცემთა ნაკრები: neuralforecast-ის DLinear მოდელი პირდაპირ იღებს DataFrame-ს unique_id, ds (Date) და y (Weekly_Sales) სვეტებით, რაც გამორიცხავს ჩემი ხელით შექმნილი TimeSeriesDataset კლასის საჭიროებას და ამარტივებს მონაცემთა მომზადებას.

ოპტიმიზებული ტრენინგი: neuralforecast-ის შიდა მექანიზმები უზრუნველყოფენ უკეთეს ტრენინგის პრაქტიკებს, როგორიცაა სწორი ინიციალიზაცია, ოპტიმიზატორები (Adam weight_decay-ით) და loss functions (MSE).

კონკურსის მეტრიკასთან მუშაობა: მიუხედავად იმისა, რომ neuralforecast-ის DLinear-ის ძირითადი ვერსია ასევე უნივარიატულია (მხოლოდ y-ზე დაყრდნობით პროგნოზირებს), მისი ოპტიმიზებული არქიტექტურა უკეთ აითვისებს y-ში არსებულ ტრენდებსა და სეზონურობას.

Pipeline-ის გამოყენება:

შევქმენი scikit-learn-ის Pipeline, რომელიც აერთიანებს მონაცემთა გარდაქმნას (TrainTestDF2NF) და DLinearNF მოდელს. ეს აუმჯობესებს კოდის ორგანიზებას და მოდელის განლაგებას. TrainTestDF2NF კლასი მიზნად ისახავდა მონაცემების neuralforecast-ისთვის საჭირო ფორმატში (unique_id, ds, y) გადაყვანას.

შეწონილი MAE (WMAE) გაანგარიშება:

კონკურსის შეფასების მეტრიკის შესაბამისად, WMAE ხელით გამოვთვალე ვალიდაციის ნაკრებზე, სადაც სადღესასწაულო დღეებს 5-ჯერ მეტი წონა ენიჭება. ეს დალოგილ იქნა wandb-ზე.

შედეგები (მეორე ნოუთბუქი):

Validation WMAE: 2085.020 (ეს მნიშვნელობა აისახა WandB-ზე)

შედეგების შედარება და ანალიზი:

მეორე ნოუთბუქის შედეგები (Validation WMAE ~2085) მნიშვნელოვნად უკეთესია პირველი ნოუთბუქის MAE-სთან (15478.33) შედარებით. ეს აშკარად აჩვენებს, რომ neuralforecast ბიბლიოთეკის DLinear იმპლემენტაცია ბევრად უფრო ეფექტური და ოპტიმალურია.

შეფასება (რატომ უკეთესი/უარესი):

რატომ უკეთესი (შესრულების მხრივ): neuralforecast-ის DLinear-ის იმპლემენტაცია არის გაცილებით ოპტიმიზებული და სანდო. ის იყენებს სწორ ტრენინგის პროცედურებს, ოპტიმიზატორებს და დეკომპოზიციის მეთოდებს, რაც იწვევს უკეთეს პროგნოზებს, მიუხედავად იმისა, რომ ის ასევე უნივარიატულია (ანუ, ძირითადად მხოლოდ წარსული y მნიშვნელობებს იყენებს). ბიბლიოთეკები ხშირად შეიცავს საუკეთესო პრაქტიკებს, რომლებიც რთული განსახორციელებელია ხელით.

რატომ შეიძლება იყოს "უარესი" (თეორიული თვალსაზრისით, მაგრამ არა შესრულების მხრივ ამ შემთხვევაში): მეორე ნოუთბუქში, მიუხედავად იმისა, რომ მთელი df გაერთიანებული იყო features_df-თან და stores_df-თან, DLinear მოდელი neuralforecast-ის ფარგლებში არ იყენებს ექსოგენურ მახასიათებლებს (Temperature, CPI და ა.შ.) პროგნოზირებისთვის. ის მუშაობს მხოლოდ unique_id, ds და y სვეტებით. ჩემს პირველ ნოუთბუქში კი იყო მცდელობა, რომ ექსოგენური ცვლადები შეყვანილი ყოფილიყო მოდელში, თუმცა მისი არქიტექტურის შეზღუდვების გამო სრულად ვერ გამოიყენა. ამ კონკრეტულ შემთხვევაში, როგორც ჩანს, Weekly_Sales-ის მხოლოდ წარსული მნიშვნელობებიდან neuralforecast-ის ოპტიმიზებული DLinear-მა უკეთესი შედეგი აჩვენა, ვიდრე ჩემმა ხელით შექმნილმა მოდელმა, რომელიც ცდილობდა ექსოგენური ცვლადების გამოყენებას. ეს შეიძლება მიუთითებდეს, რომ ან ექსოგენური ცვლადების გავლენა არ არის ისეთი ძლიერი ამ კონკრეტული დროის სერიებზე, ან ჩემი ხელით შექმნილი მოდელი ვერ ახერხებდა მათგან ინფორმაციის ეფექტურად ამოღებას.

დასკვნა და სამომავლო ნაბიჯები

DLinear-ის ექსპერიმენტები, განსაკუთრებით neuralforecast-ის იმპლემენტაციით, აშკარად აჩვენებს მნიშვნელოვან გაუმჯობესებას პროგნოზირების სიზუსტეში Walmart-ის გაყიდვების მონაცემთა ნაკრებისთვის. მიუხედავად იმისა, რომ neuralforecast-ის DLinear-ის უნივარიატული ბუნება შეიძლება იყოს შეზღუდვა, თუ ექსოგენური ცვლადები გადამწყვეტ გავლენას ახდენენ გაყიდვებზე, მან მაინც დაამტკიცა თავისი ეფექტურობა.

სამომავლო ნაბიჯები მოიცავს:

ექსოგენური ცვლადების ინტეგრაცია: neuralforecast-ის სხვა მოდელების შესწავლა, რომლებიც მხარს უჭერენ ექსოგენური მახასიათებლების (X_df) გამოყენებას (მაგალითად, NHITS, NBEATS, Autoformer), რათა შევამოწმოთ, შეძლებენ თუ არა ისინი უკეთესი სიზუსტის მიღწევას დამატებითი მონაცემების გათვალისწინებით.

ჰიპერპარამეტრების ოპტიმიზაცია: DLinear-ის input_size (lookback window), epochs, learning_rate და batch_size-ის უფრო დეტალური დარეგულირება.

WandB-ის სრულყოფილი გამოყენება: მუდმივად იმის უზრუნველყოფა, რომ ყველა გრაფიკი, მეტრიკა და მოდელის მდგომარეობა სრულად ილოგება, რათა ყოველთვის გვქონდეს ტრენინგის სრული და გამჭვირვალე სურათი.

მოდელების შედარება: DLinear-ის შედარება სხვა გლობალურ მოდელებთან (მაგალითად, LightGBM ან Prophet), რათა განვსაზღვროთ რომელი მოდელი არის ოპტიმალური ამ კონკრეტული პროგნოზირების ამოცანისთვის.






SARIMAX მოდელირება Walmart-ის გაყიდვების პროგნოზირებისთვის
ამ განყოფილებაში განვიხილავთ SARIMAX (Seasonal AutoRegressive Integrated Moving Average with Exogenous Regressors) მოდელების გამოყენების მცდელობას Walmart-ის მაღაზიის გაყიდვების პროგნოზირებისთვის. SARIMAX, როგორც ტრადიციული დროითი სერიების მოდელი, განსაკუთრებით ეფექტურია სეზონურობის, ტრენდების და ციკლური კომპონენტების აღსაქმელად. ის ასევე იძლევა ეგზოგენური ცვლადების (გარე ფაქტორების) ჩართვის საშუალებას, რაც მნიშვნელოვანია გაყიდვების მსგავსი მონაცემებისთვის.

მონაცემთა მომზადება

მონაცემთა მომზადების პროცესი სტანდარტული იყო Walmart-ის კონკურსის მონაცემთა ნაკრებისთვის:

მონაცემთა ჩატვირთვა და გაერთიანება: train.csv, features.csv და stores.csv ფაილები გაერთიანდა Store, Date და IsHoliday სვეტების მიხედვით. Date სვეტი გადაკეთდა datetime ტიპად.

NaN მნიშვნელობების შევსება: Temperature, Fuel_Price, CPI, Unemployment და MarkDown სვეტებში არსებული NaN მნიშვნელობები შევავსე მედიანური მნიშვნელობით. მედიანა შეირჩა, რადგან ის უფრო მდგრადია ექსტრემალური მნიშვნელობების მიმართ, ვიდრე საშუალო და უკეთესად შეესაბამება SARIMAX მოდელის მგრძნობელობას მონაცემთა მიმართ.

დამატებითი მახასიათებლები (Feature Engineering): შევქმენი დამატებითი დროითი მახასიათებლები, როგორიცაა week, sin_13, cos_13, sin_23, cos_23. ეს ციკლური მახასიათებლები დაგვეხმარება სეზონურობის უკეთ აღქმაში, განსაკუთრებით 13 და 23-კვირიანი ციკლებისთვის, რომლებიც შეიძლება გავლენას ახდენდნენ გაყიდვებზე.

SARIMAX მოდელირების პირველი მცდელობა (SARIMAX_Initial_Attempt.ipynb)

ჩემი პირველი მიდგომა SARIMAX-თან გულისხმობდა SARIMAXWrapper კლასის შექმნას, რათა მოდელი scikit-learn Pipeline-ში გამომეყენებინა.

ძირითადი პარამეტრები და მიდგომა:

SARIMAX პარამეტრები: order=(1,1,2) და seasonal_order=(1,1,1,52) განისაზღვრა მოდელისთვის.

enforce_stationarity=False და enforce_invertibility=False: ეს პარამეტრები საწყის ეტაპზე False-ზე დავაყენე, რათა მოდელს გაადვილებოდა კონვერგენცია. თუმცა, როგორც აღმოჩნდა, ეს იყო მნიშვნელოვანი შეცდომა, რადგან არ უზრუნველყოფდა მოდელის სტაბილურობასა და სწორ სტატისტიკურ ქცევას.

ჯგუფებად დაყოფა: მონაცემები დავყავი Store-Dept ჯგუფებად და თითოეული ჯგუფისთვის ცალკე SARIMAX მოდელი იწვრთნებოდა.

მეტრიკა: შეფასებისთვის გამოვიყენე Weighted Mean Absolute Error (WMAE) და Root Mean Square Error (RMSE).

MLflow ინტეგრაცია: ექსპერიმენტები დაილოგა MLflow-ზე DagsHub-ის მეშვეობით.

შედეგები და პრობლემები:

ამ პირველი ექსპერიმენტის Overall WMAE იყო წარმოუდგენლად მაღალი (დაახლოებით 61,288,039,612,882,950), რაც ნათლად მიუთითებდა, რომ მოდელი საერთოდ არ მუშაობდა სწორად. ტიპიური WMAE ამ კონკურსისთვის 2000-3000 დიაპაზონშია.

ძირითადი მიზეზები ასეთი ცუდი შედეგებისთვის:

მონაცემთა არასწორი სკალირება: SARIMAX მოდელი უკიდურესად მგრძნობიარეა მონაცემთა მასშტაბების მიმართ, განსაკუთრებით ეგზოგენური ცვლადების გამოყენებისას. ეგზოგენური ცვლადების სკალირების არარსებობა იყო ყველაზე სავარაუდო მიზეზი ექსტრემალურად ცუდი შედეგების.

enforce_stationarity და enforce_invertibility პარამეტრები False-ზე: ამ პარამეტრების არასწორად დაყენებამ ხელი შეუშალა მოდელის სტაბილურობასა და სწორ სტატისტიკურ თვისებებს, რამაც გამოიწვია კონვერგენციის პრობლემები და არასწორი პროგნოზები.

მონაცემთა სიმცირე ზოგიერთი ჯგუფისთვის: მიუხედავად იმისა, რომ ცარიელი ჯგუფები გამოირიცხა, SARIMAX-ს ზოგადად სჭირდება საკმაოდ დიდი დროითი სერია, განსაკუთრებით 52-კვირიანი სეზონურობის გათვალისწინებით.

SARIMAX მოდელირების მეორე მცდელობა (SARIMAX_Improved_Attempt.ipynb)

წინა ექსპერიმენტის პრობლემების გათვალისწინებით, შეიქმნა ახალი Notebook, რომელიც მიზნად ისახავდა ზემოთ ხსენებული შეცდომების გამოსწორებას. ამ მცდელობაში მთავარი აქცენტი გაკეთდა მონაცემთა წინასწარ დამუშავებასა და მოდელის პარამეტრების სწორ კონფიგურაციაზე.

ძირითადი გაუმჯობესებები:

ეგზოგენური ცვლადების სკალირება: Pipeline-ში ჩაირთო StandardScaler, რათა ყველა ეგზოგენური მახასიათებელი ყოფილიყო სკალირებული. ეს გადამწყვეტი ნაბიჯი იყო მოდელის სტაბილური მორგებისა და ოპტიმალური კონვერგენციისთვის.

enforce_stationarity=True და enforce_invertibility=True: ეს პარამეტრები დაყენდა True-ზე, რაც აუცილებელია statsmodels SARIMAX მოდელის სტატისტიკური ვალიდურობისა და პროგნოზების სანდოობის უზრუნველსაყოფად.

დამატებითი ეგზოგენური მახასიათებლები: Year, DayOfWeek და Store Size ჩართული იქნა ეგზოგენურ ცვლადებად, რათა უკეთ აღეწერათ გაყიდვების დინამიკა.

პროგნოზების 0-ზე კლიპინგი: უარყოფითი პროგნოზები დაყენდა 0-ზე, რადგან გაყიდვების მოცულობა არ შეიძლება იყოს ნეგატიური.

მონაცემთა მინიმალური სიგრძის ადაპტაცია: შემცირდა მინიმალური მოთხოვნები ტრენინგისა (min_train_len = 52) და ვალიდაციის (min_val_len = 12) ნაკრებებისთვის, რათა მოდელს შეძლებოდა უფრო მეტი Store-Dept კომბინაციის დამუშავება.

შედეგები და შეზღუდვები:

ამ ცვლილებების შემდეგ, მოდელმა დაიწყო ვალიდური WMAE და RMSE მნიშვნელობების გენერირება თითოეული Store-Dept კომბინაციისთვის, რაც წინა ექსპერიმენტებში შეუძლებელი იყო. თუმცა, მიუხედავად იმისა, რომ ინდივიდუალური მოდელები გამართულად მუშაობდნენ, მთელი dataset-ის დამუშავების დრო წარმოუდგენლად ხანგრძლივი აღმოჩნდა.

კერძოდ, პრობლემები იყო:

გამოთვლითი სირთულე: SARIMAX მოდელი თითოეული დროითი სერიისთვის ცალ-ცალკე იწვრთნება. 3,331 უნიკალური Store-Dept კომბინაციისთვის ცალკე მოდელის დატრენინგებას, თუნდაც მცირე ცვლილებების შემდეგ, წუთები ან მეტი სჭირდებოდა.

Colab-ის სესიების შეჩერება: ამდენი Store-Dept კომბინაციის პირობებში, მთელი dataset-ის დამუშავების დრო საათებს, შესაძლოა დღეებსაც კი გაგრძელდა. ჩემს ბოლო ცდაში, რამდენიმე წუთის შემდეგ მოდელი მხოლოდ პირველ ათეულ კომბინაციაზე იყო გადასული, სანამ Colab-ის სესია არ შეწყდებოდა (სავარაუდოდ, რესურსების, როგორიცაა GPU ან RAM, უკმარისობის გამო). ასეთი ხანგრძლივი სატრენინგო დრო ფიზიკურად შეუძლებელს ხდიდა მოდელის ოპტიმიზაციას, ჰიპერპარამეტრების დარეგულირებას ან ალტერნატიული SARIMA პარამეტრების ტესტირებას გონივრულ ვადებში.

დასკვნა:

SARIMAX-ის გამოთვლითი სირთულე და დროითი შეზღუდვები არ არის პრაქტიკული ამ მასშტაბის ამოცანისთვის. მიუხედავად იმისა, რომ SARIMAX-ის მიერ მოწოდებული სტატისტიკური სიზუსტე შეიძლება იყოს მაღალი ინდივიდუალური სერიებისთვის, მისი მასშტაბირება ათასობით დროითი სერიისთვის უბრალოდ არაეფექტურია. ამ მიზეზით, გადავწყვიტე, ეს კონკრეტული Notebook არ დამესრულებინა და მის ნაცვლად, ყურადღება გადავიტანო უფრო მასშტაბურ და ეფექტურ დროითი სერიების პროგნოზირების მოდელებზე, როგორიცაა გლობალური მანქანური სწავლების მოდელები (მაგალითად, LightGBM ან XGBoost) დროითი სერიების მახასიათებლებთან ერთად. ეს მოდელები შეძლებენ მთელი dataset-ის ერთიანად დამუშავებას გაცილებით სწრაფად და ხშირად უკეთესი სიზუსტით.











