ამ პროექტში ვაკეთებთ Walmart მაღაზიების გაყიდვების პროგნოზირებას. 
მონაცემების გასამზადებლად შევაერთეთ მოცემული train და test დატასეტები features და stores დატასეტებთან.
დავსპლიტეთ დამერჯილი დატასეტი. ვჰენდლავთ Nan ველიუებს და კატეგორიულები ცვლადები გადაგვყავს ნუმერიქალ ცვლადებში.

XGBoost:
საუკეთესო შედეგი:
n_estimators: 2000
verbosity: 1
wmae: 2815
ოპტიმიზაციები:
დავამატე Year, Month, Week, Day, DayOfWeek, IsMonthStart, IsMonthEnd, IsWeekend, და Quarter 
დავატრენინგე მოდელი y′ = log(Weekly_Sales + 1) და პროგნოზი შევაქციე ამ ფუნქციით exp(y′) - 1, ვარიაციის სტაბილიზაციისთვის.
პარამეტრების n_estimators, max_depth, learning_rate, subsample, colsample_bytree და min_child_weight სხვადასხვა კომბინაციებიც მოსვინჯე, თუმცა wmae მივიღე 3370. შედეგად ოპტიმიზაციებამდე არსებული მოდელი უკეთეს შედეგს დებდა.

LightGBM:
n_estimators: 2000, learning_rate: 0.015, num_leaves: 70, max_depth: 14, Wmae: 2773.
n_estimators: 3000, learning_rate: 0.001, num_leaves: 80, max_depth: 14, Wmae: 5050.
n_estimators: 3000, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2741.
n_estimators: 3500, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2682.
ციკლური ენკოდირება სეზონური პატერნებისთვის(სეზონები ხო ციკლურია და ამიტომ სინუსის და კოსინუსის ფუნქციები გამოვიყენე ტრანსფორმაციისთვის). დავამატე ფიჩერები quarter, dayofyear, weekofyear. შევქემენი ბინარული ინდიკატორები თითოეული markdown ტიპისთვის. დავუმატე შემდეგი ფუნქციები: Total_MarkDown, რომელიც ყველა markdown მნიშვნელობის ჯამს წარმოადგენს, MarkDown_Count, რომელიც აქტიური მარკდაუნების რაოდენობას აღნიშნავს, და Has_MarkDown, რომელიც ბინარული ინდიკატორია ნებისმიერი markdown-ის არსებობისთვის. ახალი ფუნქციები: CPI_Unemployment_Ratio, რომელიც ეკონომიკური ჯანმრთელობის ინდიკატორია, CPI_Normalized, რომელიც სტანდარტიზებული CPI მნიშვნელობებია, და Unemployment_Normalized სტანდარტიზებული უმუშევრობის მაჩვენებლებისთვის. ეს იჭერს ეკონომიკურ პირობებს, რომლებიც მოქმედებს მომხმარებლის ხარჯვაზე. Lag ფუნქციები შეიცავს წინა 1, 2, 4 და 8 კვირის გაყიდვებს. ყველა lag ფუნქცია იქმნება Store და Dept მიხედვით დაჯგუფებული Weekly_Sales-ის შესაბამისი პერიოდით გადატანით. შემდეგი ფუნქციები: Sales_Mean და Sales_Std 4, 8, 12 კვირის window-ებისთვის. ეს იჭერს გაყიდვების ტენდენციებს და ცვალებადობის ნიმუშებს. ფუნქციები: Is_Christmas_Period (15-31 დეკემბრისთვის), Is_Thanksgiving_Period (20-30 ნოემბრისთვის), Is_Back_To_School (აგვისტო და სექტემბრის დასაწყისისთვის). ეს იჭერს ძირითადი შოპინგის სეზონებს განსხვავებული ნიმუშებით. ფუნქცია Store_Dept_Interaction = Store × 1000 + Dept, რომელიც იჭერს უნიკალურ მაღაზია-დეპარტამენტის კომბინაციებს. პარამეტრები შეიცავს reg_alpha=0.1 L1 რეგულარიზაციისთვის, reg_lambda=0.1 L2 რეგულარიზაციისთვის, min_split_gain=0.1 split-ების მინიმალური gain-ისთვის, min_child_weight=0.001 მინიმალური child weight-ისთვის, subsample=0.8 რიგების sampling-ისთვის, colsample_bytree=0.8 სვეტების sampling-ისთვის. ეს ამცირებს overfitting-ს და აუმჯობესებს generalization-ს. წმაე: 1877.


N-BEATS: 
თავდაპირველი N-BEATS მოდელი early stopping პრობლემას აწყდებოდა 18-ე ეპოქზე, რაც იწვევდა არაოპტიმალურ შედეგებს. ეს იყო patience=10-ის გამოდა ასევე მაღალი learning_rate=0.001-ის გამო.








DLinear მოდელის მიმოხილვა
DLinear (Decomposition Linear) არის Deep Learning არქიტექტურა, რომელიც დაფუძნებულია დროითი სერიების დეკომპოზიციაზე. მისი მთავარი იდეაა დროითი სერიის დაყოფა ტრენდულ (Trend) და სეზონურ (Seasonal) კომპონენტებად, შემდეგ კი თითოეული კომპონენტის დამოუკიდებლად მოდელირება ხაზოვანი ფენების გამოყენებით. ეს მიდგომა მიზნად ისახავს რთული და არაწრფივი დამოკიდებულებების გამარტივებას და პროგნოზირების სიზუსტის გაუმჯობესებას.
მონაცემთა მომზადება (Data Preprocessing)
1. მონაცემთა ჩატვირთვა და გაერთიანება

თავდაპირველად, Kaggle API-ის გამოყენებით ჩამოვტვირთე და ჩავტვირთე საჭირო მონაცემთა ნაკრებები: train.csv, test.csv, stores.csv და features.csv.
შემდეგ, Date სვეტი გადავაკეთე datetime ტიპად და გავაერთიანე train_df, test_df, stores_df და features_df ერთიან DataFrame-ებში Store და Date სვეტების მიხედვით.

2. Feature Engineering

create_features ფუნქციის გამოყენებით შევქმენი დამატებითი დროზე დაფუძნებული (time-based) და სხვა მახასიათებლები:

Year, Month, Day, Week, DayOfWeek, Quarter - თარიღის კომპონენტები.

IsMonthStart, IsMonthEnd - თვის დასაწყისი/დასასრული.

IsHoliday - დღესასწაულის აღმნიშვნელი (ბულიანისგან რიცხვით ფორმატში გადაყვანილი).

Type (მაღაზიის ტიპი) - LabelEncoder-ის გამოყენებით გადავაკოდირე რიცხვით მნიშვნელობებად.

Numeric column-ებში არსებული NaN მნიშვნელობები შევავსე თითოეული სვეტის მედიანური მნიშვნელობით.

3. მონაცემთა გაწმენდა (Data Cleaning)

Weekly_Sales სვეტში არსებული NaN მნიშვნელობები წავშალე, რადგან ეს ჩვენი სამიზნე ცვლადია.

შერჩეულ feature_columns-ში დარჩენილი NaN მნიშვნელობები შევავსე 0-ით, რათა მოდელმა შეძლოს მათი დამუშავება.

4. მონაცემთა დაყოფა და სკალირება (Data Splitting and Scaling)

მონაცემები ქრონოლოგიურად დავყავი train, validation და test ნაკრებებად (60%/20%/20% თანაფარდობით Date სვეტის quantile-ების მიხედვით). ეს მიდგომა უზრუნველყოფს, რომ მოდელი მომზადდეს წარსულ მონაცემებზე და შეფასდეს მომავალი პერიოდის მონაცემებზე.

მოდელის მომზადებამდე, feature_columns სვეტები StandardScaler-ის გამოყენებით გავასკალერე. Scaler მოვამზადე მხოლოდ train მონაცემებზე, შემდეგ კი მისი გამოყენებით მოხდა validation და test ნაკრებების ტრანსფორმაცია, რათა თავიდან ავიცილოთ Data Leakage.

DLinear მოდელის არქიტექტურა და ტრენინგი
1. TimeSeriesDataset

შევქმენი TimeSeriesDataset კლასი, რომელიც PyTorch-ის Dataset-ს აიმპლემენტირებს. ეს კლასი პასუხისმგებელია დროითი სერიების მონაცემების "sequence"-ებად გადაყვანაზე, რაც Deep Learning მოდელებისთვის არის აუცილებელი. თითოეული "sequence" შედგება sequence_length (ამ შემთხვევაში 12 კვირა) მახასიათებლებისგან და პროგნოზირებადი სამიზნე მნიშვნელობისგან (შემდეგი კვირის Weekly_Sales).

2. DLinear მოდელი

DLinear მოდელის არქიტექტურა შევქმენი nn.Module-ის გამოყენებით. მოდელი მოიცავს decomposition ფენას (AvgPool1d), რომელიც გამოყოფს ტრენდულ კომპონენტს, და ორ ხაზოვან ფენას (Linear_Seasonal, Linear_Trend), რომლებიც ამუშავებენ სეზონურ და ტრენდულ კომპონენტებს.

3. ტრენინგის პარამეტრები და პროცესი

Device: გამოვიყენე cuda თუ GPU ხელმისაწვდომია, წინააღმდეგ შემთხვევაში cpu.

Hyperparameters:

sequence_length: 12 (წინა 12 კვირის მონაცემები პროგნოზისთვის)

batch_size: 64

learning_rate: 0.001

weight_decay: 1e-4

num_epochs: 50

Loss Function: nn.MSELoss() (Mean Squared Error) შეირჩა, როგორც Loss Function.

Optimizer: optim.Adam() გამოყენებულია მოდელის წონების ოპტიმიზაციისთვის.

Scheduler: optim.lr_scheduler.ReduceLROnPlateau გამოყენებულია Learning Rate-ის ავტომატურად შესამცირებლად, თუ Validation Loss არ უმჯობესდება გარკვეული პერიოდის განმავლობაში (patience=5).

Early Stopping: განხორციელდა Early Stopping მექანიზმი patience=10-ით, რათა თავიდან ავიცილოთ Overfitting და შევაჩეროთ ტრენინგი, თუ Validation Loss არ უმჯობესდება 10 ეპოქის განმავლობაში.

WandB Integration: ყველა ექსპერიმენტი დალოგილ იქნა Weights & Biases (WandB) პლატფორმაზე (project="Store-Sales-Forecasting", entity="agasi22-free-university-of-tbilisi-"). WandB-ზე ილოგებოდა Loss, MAE, RMSE და Learning Rate ყოველი ეპოქის შემდეგ, ასევე საბოლოო შედეგები და გრაფიკები.

შედეგები და შეფასება
DLinear მოდელის ტრენინგის შემდეგ, მისი ეფექტურობა შევაფასე Validation და Test ნაკრებებზე. შეფასების მეტრიკებად გამოვიყენე Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) და Mean Squared Error (MSE).

DLinear მოდელის შედეგები:

Final Validation Results:

MAE: 15478.33

RMSE: 27178.58

MSE: 738672181.62

Final Test Results:

MAE: 15534.75

RMSE: 26844.63

MSE: 719990429.43

მიღებული შედეგები, განსაკუთრებით MAE და RMSE, აჩვენებს, რომ DLinear მოდელის პროგნოზები საკმაოდ შორსაა რეალური გაყიდვებისგან. გაყიდვების საშუალო მაჩვენებლებთან შედარებით (რომელიც ხშირად ათიათასებს აღწევს), 15,000-ზე მეტი MAE მიუთითებს მნიშვნელოვან ცდომილებაზე. ეს იმაზე მეტყველებს, რომ მიუხედავად DLinear-ის დეკომპოზიციური მიდგომისა, ამ კონკრეტულ ამოცანაში მან ვერ შეძლო გაყიდვების კომპლექსური დინამიკის ადეკვატურად აღქმა.

დასკვნა და სამომავლო ნაბიჯები
DLinear მოდელის ექსპერიმენტის შედეგები არ იყო დამაკმაყოფილებელი ამ კონკრეტული Walmart-ის გაყიდვების პროგნოზირების ამოცანისთვის. მიუხედავად იმისა, რომ Deep Learning არქიტექტურებს აქვთ პოტენციალი რთული დროითი სერიების მოდელირებისთვის, DLinear-ის ამ იმპლემენტაციამ ვერ მიაღწია სასურველ სიზუსტეს. სავარაუდო მიზეზები შეიძლება იყოს:

მოდელის სირთულე: შესაძლოა DLinear-ის შედარებით მარტივი ხაზოვანი კომპონენტები არ არის საკმარისი Walmart-ის გაყიდვების მონაცემებში არსებული რთული, არაწრფივი დამოკიდებულებების აღსაქმელად.

Feature Engineering-ის გაუმჯობესება: შესაძლოა, დამატებითი, უფრო რთული ან დომენზე სპეციფიკური მახასიათებლების შექმნა საჭიროა Deep Learning მოდელებისთვის.

Hyperparameter Tuning: მიუხედავად იმისა, რომ ჩატარდა ტრენინგი და Scheduler-ი, შესაძლოა DLinear-ისთვის ოპტიმალური ჰიპერპარამეტრების მოძიება მეტ ძალისხმევას მოითხოვდეს.

მონაცემების მრავალფეროვნება: Walmart-ის მონაცემები შეიცავს უამრავ სხვადასხვა მაღაზიასა და დეპარტამენტს, თითოეულს თავისი უნიკალური დინამიკით. DLinear-ის მიდგომამ შესაძლოა ვერ შეძლო ამ მრავალფეროვნების ეფექტურად მოდელირება.

ამ ექსპერიმენტის საფუძველზე, გადავწყვიტეთ, რომ ამ ამოცანისთვის DLinear არ არის საუკეთესო არჩევანი. ჩვენი გუნდი ფოკუსირებას მოახდენს LightGBM, Prophet და ARIMA მოდელებზე, რადგან მათთან ადრეულმა ექსპერიმენტებმა გაცილებით უკეთესი შედეგები აჩვენა. ამ მოდელებზე დეტალურად ვიმუშავებთ, განვახორციელებთ feature selection-ს, cross-validation-ს, ჰიპერპარამეტრების ოპტიმიზაციას და დავაფიქსირებთ ყველა ნაბიჯს MLflow-სა და WandB-ზე, როგორც ინსტრუქციაშია მოთხოვნილი.





სარიმა

მონაცემთა მომზადება (Data Preprocessing)
მონაცემთა ჩატვირთვა და გაერთიანება: ჩვეულებრივად ჩავტვირთე train.csv, features.csv და stores.csv ფაილები. Date სვეტები გადავაკეთე datetime ტიპად. ყველა მონაცემი გაერთიანდა Store, Date და IsHoliday სვეტების მიხედვით.

NaN მნიშვნელობების შევსება: Temperature, Fuel_Price, CPI და Unemployment სვეტებში არსებული NaN მნიშვნელობები შევავსე ffill (forward fill) და bfill (backward fill) მეთოდების გამოყენებით. ეს სტანდარტული მიდგომაა Time Series მონაცემებში.

დამატებითი მახასიათებლები (Feature Engineering): შევქმენი Month (თვე), WeekOfYear (წლის კვირა) და HolidayWeek (განსაზღვრავს, იყო თუ არა მოცემული კვირა Holiday კვირა მაღაზიისა და დეპარტამენტის დონეზე) ეგზოგენური ცვლადებისთვის. ეს ცვლადები შეირჩა, რადგან მათ აქვთ პოტენციალი, გავლენა მოახდინონ გაყიდვების სეზონურობაზე და არდადეგებზე.

SARIMAX მოდელის იმპლემენტაცია და ტრენინგი
SARIMAXWrapper კლასი: შევქმენი SARIMAXWrapper კლასი, რომელიც SARIMAX მოდელს scikit-learn Pipeline-ში გამოსაყენებლად აგებს. ეს საშუალებას მაძლევს, სტანდარტული fit და predict მეთოდები გამოვიყენო. მოდელისთვის განვსაზღვრე order=(1,1,2) და seasonal_order=(1,1,1,52) პარამეტრები. enforce_stationarity=False და enforce_invertibility=False დავაყენე საწყის ეტაპზე, რათა მოდელს გაუადვილდეს კონვერგენცია, თუმცა ეს პარამეტრები, როგორც წესი, უნდა იყოს True სწორი სტატისტიკური ქცევისთვის.

Cross-Validation: მონაცემები დავყავი Store-Dept ჯგუფებად და თითოეული ჯგუფისთვის გამოვიყენე დროითი გაყოფა: 2012 წლის 1 იანვრამდე მონაცემები ტრენინგისთვის (y_train, X_train) და 2012 წლის 1 იანვრიდან 2012 წლის 1 ივლისამდე მონაცემები ვალიდაციისთვის (y_val, X_val).

მეტრიკა: შეფასებისთვის გამოვიყენე weighted_mae (WMAE) და RMSE.

MLflow ინტეგრაცია: ექსპერიმენტები დალოგილ იქნა MLflow-ზე, DagsHub-ის მეშვეობით. დაფიქსირდა SARIMAX-ის პარამეტრები (order, seasonal_order) და ეგზოგენური მახასიათებლები.

შედეგები და პრობლემები
ამ ექსპერიმენტის Overall WMAE არის 61,288,039,612,882,950. ეს წარმოუდგენლად მაღალი მაჩვენებელია და მიუთითებს, რომ მოდელი საერთოდ არ მუშაობს სწორად. ტიპიური WMAE ამ კონკურსისთვის არის 2000−3000 დიაპაზონში.

რატომ არის შედეგი ასეთი ცუდი?

ამხელა WMAE რამდენიმე სერიოზულ პრობლემაზე მიუთითებს:

მონაცემთა მასშტაბები და SARIMAXWrapper: SARIMAX მოდელი ძალიან მგრძნობიარეა მონაცემთა მასშტაბების მიმართ. მიუხედავად იმისა, რომ exog_cols ეგზოგენური ცვლადები გადავიყვანე float64-ზე, არ განმიხორციელებია მათი სკალირება (მაგალითად, StandardScaler-ით). ეს არის ყველაზე სავარაუდო მიზეზი ასეთი ექსტრემალურად ცუდი შედეგების. SARIMAX, განსაკუთრებით ეგზოგენურ ცვლადებთან ერთად, ხშირად მოითხოვს სკალირებულ მონაცემებს სტაბილური კონვერგენციისთვის.

enforce_stationarity და enforce_invertibility: ამ პარამეტრების False-ზე დაყენება ხელს უშლის statsmodels-ს, უზრუნველყოს მოდელის სტაბილურობა და სათანადო სტატისტიკური თვისებები. ეს შეიძლება იყოს კონვერგენციის პრობლემების და არასწორი პროგნოზების მიზეზი.

მონაცემთა სიმცირე ზოგიერთი ჯგუფისთვის: მართალია, გამოვტოვე ჯგუფები, რომლებსაც არ ჰქონდათ საკმარისი მონაცემები, SARIMAX-ს ზოგადად სჭირდება საკმაოდ დიდი დროითი სერია (მინიმუმ 2-3 სეზონური ციკლი), რათა სწორად შეაფასოს სეზონური კომპონენტები. 52 კვირიანი სეზონურობით, ზოგიერთ ჯგუფს შესაძლოა არ ჰქონდეს საკმარისი ისტორიული მონაცემები სტაბილური მოდელირებისთვის.

ჯგუფი-ჯგუფიზე ტრენინგი: მიუხედავად იმისა, რომ ეს არის SARIMA-ს სტანდარტული მიდგომა, 4000-ზე მეტი Store-Dept კომბინაციისთვის ცალკე მოდელის ტრენინგი ძალიან შრომატევადია და ადვილად შეიძლება გამოიწვიოს შეცდომები ან არასტაბილური შედეგები ზოგიერთი იშვიათი Time Series-ისთვის.

ეს ექსპერიმენტი ნათლად აჩვენებს, რომ SARIMAX-ის ეფექტური გამოყენებისთვის აუცილებელია მონაცემთა გაცილებით ფრთხილად მომზადება, განსაკუთრებით სკალირება, და მოდელის პარამეტრების უფრო სწორი კონფიგურაცია.

ახალი, გაუმჯობესებული SARIMAX Notebook-ის შექმნა
მოდით, შევქმნათ ახალი Notebook, რომელიც გამოასწორებს ზემოთ ხსენებულ პრობლემებს. მთავარი აქცენტი იქნება:

ეგზოგენური ცვლადების სკალირება.

enforce_stationarity=True და enforce_invertibility=True გამოყენება.

SARIMAXWrapper-ის გაუმჯობესება სკალირების Pipeline-ში ჩართვით.

უფრო მკაცრი შემოწმება მონაცემთა სიგრძეზე.

უფრო მოკლე სეზონური პერიოდის (მაგალითად, 4 კვირა) ან უფრო მარტივი order პარამეტრების ცდა საწყის ეტაპზე, რადგან 52 კვირა შეიძლება ძალიან გრძელი იყოს ყველა სერიისთვის. თუმცა, კონკურსის კონტექსტში, 52 კვირა ლოგიკურია.
SARIMAX მოდელის გაუმჯობესებული ცდა და შედეგები
წინა ექსპერიმენტების საფუძველზე, განვახორციელე შემდეგი მნიშვნელოვანი გაუმჯობესებები SARIMAX მოდელში:

ეგზოგენური ცვლადების სკალირება: SARIMAX-ის მგრძნობელობის გამო, ჩავრთე StandardScaler Pipeline-ში, რათა ყველა ეგზოგენური მახასიათებელი იყოს სკალირებული. ეს უზრუნველყოფს მოდელის სტაბილურ მორგებას და ოპტიმალურ კონვერგენციას.

enforce_stationarity და enforce_invertibility პარამეტრების დაყენება True-ზე: ამ პარამეტრების გააქტიურება აუცილებელია statsmodels SARIMAX მოდელის სტატისტიკური ვალიდურობისა და პროგნოზების სანდოობის უზრუნველსაყოფად.

დამატებითი ეგზოგენური მახასიათებლები: ჩავრთე Year, DayOfWeek და Store Size როგორც ეგზოგენური ცვლადები, გაყიდვების დინამიკის უკეთ აღსაწერად. IsHoliday სვეტი პირდაპირ გადაკეთდა რიცხვით ფორმატში.

მედიანური შევსება: NaN მნიშვნელობები შევავსე რიცხვითი სვეტების მედიანური მნიშვნელობით, რაც უფრო რობუსტურია ffill/bfill-თან შედარებით.

პროგნოზების 0-ზე კლიპინგი: უარყოფითი პროგნოზები 0-ზე დავაყენე, რადგან გაყიდვების მოცულობა არ შეიძლება იყოს ნეგატიური.

მონაცემთა მინიმალური სიგრძის ადაპტაცია: შევამცირე მინიმალური მოთხოვნები ტრენინგისა (min_train_len = 52) და ვალიდაციის (min_val_len = 12) ნაკრებებისთვის, რათა მოდელს შეძლებოდა უფრო მეტი Store-Dept კომბინაციის დამუშავება.

ამ ცვლილებების შემდეგ, მოდელმა დაიწყო ვალიდური WMAE და RMSE მნიშვნელობების გენერირება თითოეული Store-Dept კომბინაციისთვის, რაც წინა ექსპერიმენტებში შეუძლებელი იყო ექსტრემალურად მაღალი WMAE-ის გამო.

SARIMAX-ის შეზღუდვები და ალტერნატიული მიდგომის საჭიროება
მიუხედავად განხორციელებული გაუმჯობესებებისა და მოდელის გამართული მუშაობისა ცალკეული სერიებისთვის, SARIMAX-ის გამოყენება Walmart-ის მონაცემთა ნაკრებისთვის (რომელიც მოიცავს 3,331 უნიკალურ Store-Dept კომბინაციას) წარმოუდგენლად არაეფექტური აღმოჩნდა.

პრობლემა:
SARIMAX მოდელი თითოეული დროითი სერიისთვის ცალ-ცალკე იწვრთნება. თითოეული მოდელის დატრენინგებას, თუნდაც მცირე ცვლილებების შემდეგ, წუთები ან მეტი სჭირდებოდა. ამდენი Store-Dept კომბინაციის პირობებში, მთელი dataset-ის დამუშავების დრო საათებს, შესაძლოა დღეებსაც კი გაგრძელდა. მაგალითად, ჩემს ბოლო ცდაში, რამდენიმე წუთის შემდეგ მოდელი მხოლოდ პირველ ათეულ კომბინაციაზე იყო გადასული. ასეთი გრძელი სატრენინგო დრო ფიზიკურად შეუძლებელს ხდიდა მოდელის ოპტიმიზაციას, ჰიპერპარამეტრების დარეგულირებას ან ალტერნატიული SARIMA პარამეტრების ტესტირებას გონივრულ ვადებში.

გადაწყვეტილება:
SARIMAX-ის გამოთვლითი სირთულე და დროითი შეზღუდვები არ არის პრაქტიკული ამ მასშტაბის ამოცანისთვის. შესაბამისად, გადავწყვიტე, ყურადღება გადავიტანო უფრო მასშტაბურ და ეფექტურ დროითი სერიების პროგნოზირების მოდელებზე. ეს მოიცავს გლობალური მანქანური სწავლების მოდელებს (როგორიცაა LightGBM ან XGBoost) დროითი სერიების მახასიათებლებთან ერთად, რომლებიც შეძლებენ მთელი dataset-ის ერთიანად დამუშავებას გაცილებით სწრაფად და ხშირად უკეთესი სიზუსტით.

