ამ პროექტში ვაკეთებთ Walmart მაღაზიების გაყიდვების პროგნოზირებას. 
მონაცემების გასამზადებლად შევაერთეთ მოცემული train და test დატასეტები features და stores დატასეტებთან.
დავსპლიტეთ დამერჯილი დატასეტი. ვჰენდლავთ Nan ველიუებს და კატეგორიულები ცვლადები გადაგვყავს ნუმერიქალ ცვლადებში.

XGBoost:
საუკეთესო შედეგი:
n_estimators: 2000
verbosity: 1
wmae: 2815
ოპტიმიზაციები:
დავამატე Year, Month, Week, Day, DayOfWeek, IsMonthStart, IsMonthEnd, IsWeekend, და Quarter 
დავატრენინგე მოდელი y′ = log(Weekly_Sales + 1) და პროგნოზი შევაქციე ამ ფუნქციით exp(y′) - 1, ვარიაციის სტაბილიზაციისთვის.
პარამეტრების n_estimators, max_depth, learning_rate, subsample, colsample_bytree და min_child_weight სხვადასხვა კომბინაციებიც მოსვინჯე, თუმცა wmae მივიღე 3370. შედეგად ოპტიმიზაციებამდე არსებული მოდელი უკეთეს შედეგს დებდა. ამიტომ ვცადე randomizedSearchCV, თუმცა შედეგი მაინც არა დამაკმაყოფილებელი იყო და გადავწყვიტე დამემატებინა რეგულარიზაციები. gamma: 0.1, 'reg_alpha': 0.1, 'reg_lambda': 1.0 .(Gamma adds a minimum loss reduction threshold for making a split, discouraging the model from growing branches that don’t contribute substantive improvements, L1 (reg_alpha) and L2 (reg_lambda) penalties shrink coefficient magnitudes and help prevent individual leaves from drifting too far; and by subsampling at the tree‑level, the model sees fewer features per split which further guards against overfitting).

LightGBM:
n_estimators: 2000, learning_rate: 0.015, num_leaves: 70, max_depth: 14, Wmae: 2773.
n_estimators: 3000, learning_rate: 0.001, num_leaves: 80, max_depth: 14, Wmae: 5050.
n_estimators: 3000, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2741.
n_estimators: 3500, learning_rate: 0.01, num_leaves: 80, max_depth: 14, Wmae: 2682.
ციკლური ენკოდირება სეზონური პატერნებისთვის(სეზონები ხო ციკლურია და ამიტომ სინუსის და კოსინუსის ფუნქციები გამოვიყენე ტრანსფორმაციისთვის). დავამატე ფიჩერები quarter, dayofyear, weekofyear. შევქემენი ბინარული ინდიკატორები თითოეული markdown ტიპისთვის. დავუმატე შემდეგი ფუნქციები: Total_MarkDown, რომელიც ყველა markdown მნიშვნელობის ჯამს წარმოადგენს, MarkDown_Count, რომელიც აქტიური მარკდაუნების რაოდენობას აღნიშნავს, და Has_MarkDown, რომელიც ბინარული ინდიკატორია ნებისმიერი markdown-ის არსებობისთვის. ახალი ფუნქციები: CPI_Unemployment_Ratio, რომელიც ეკონომიკური ჯანმრთელობის ინდიკატორია, CPI_Normalized, რომელიც სტანდარტიზებული CPI მნიშვნელობებია, და Unemployment_Normalized სტანდარტიზებული უმუშევრობის მაჩვენებლებისთვის. ეს იჭერს ეკონომიკურ პირობებს, რომლებიც მოქმედებს მომხმარებლის ხარჯვაზე. Lag ფუნქციები შეიცავს წინა 1, 2, 4 და 8 კვირის გაყიდვებს. ყველა lag ფუნქცია იქმნება Store და Dept მიხედვით დაჯგუფებული Weekly_Sales-ის შესაბამისი პერიოდით გადატანით. შემდეგი ფუნქციები: Sales_Mean და Sales_Std 4, 8, 12 კვირის window-ებისთვის. ეს იჭერს გაყიდვების ტენდენციებს და ცვალებადობის ნიმუშებს. ფუნქციები: Is_Christmas_Period (15-31 დეკემბრისთვის), Is_Thanksgiving_Period (20-30 ნოემბრისთვის), Is_Back_To_School (აგვისტო და სექტემბრის დასაწყისისთვის). ეს იჭერს ძირითადი შოპინგის სეზონებს განსხვავებული ნიმუშებით. ფუნქცია Store_Dept_Interaction = Store × 1000 + Dept, რომელიც იჭერს უნიკალურ მაღაზია-დეპარტამენტის კომბინაციებს. პარამეტრები შეიცავს reg_alpha=0.1 L1 რეგულარიზაციისთვის, reg_lambda=0.1 L2 რეგულარიზაციისთვის, min_split_gain=0.1 split-ების მინიმალური gain-ისთვის, min_child_weight=0.001 მინიმალური child weight-ისთვის, subsample=0.8 რიგების sampling-ისთვის, colsample_bytree=0.8 სვეტების sampling-ისთვის. ეს ამცირებს overfitting-ს და აუმჯობესებს generalization-ს. წმაე: 1877.


N-BEATS: 
თავდაპირველი N-BEATS მოდელი early stopping პრობლემას აწყდებოდა 18-ე ეპოქზე, რაც იწვევდა არაოპტიმალურ შედეგებს. ეს იყო patience=10-ის გამოდა ასევე მაღალი learning_rate=0.001-ის გამო.








DLinear მოდელის მიმოხილვა
DLinear (Decomposition Linear) არის Deep Learning არქიტექტურა, რომელიც დაფუძნებულია დროითი სერიების დეკომპოზიციაზე. მისი მთავარი იდეაა დროითი სერიის დაყოფა ტრენდულ (Trend) და სეზონურ (Seasonal) კომპონენტებად, შემდეგ კი თითოეული კომპონენტის დამოუკიდებლად მოდელირება ხაზოვანი ფენების გამოყენებით. ეს მიდგომა მიზნად ისახავს რთული და არაწრფივი დამოკიდებულებების გამარტივებას და პროგნოზირების სიზუსტის გაუმჯობესებას.
მონაცემთა მომზადება (Data Preprocessing)
1. მონაცემთა ჩატვირთვა და გაერთიანება

თავდაპირველად, Kaggle API-ის გამოყენებით ჩამოვტვირთე და ჩავტვირთე საჭირო მონაცემთა ნაკრებები: train.csv, test.csv, stores.csv და features.csv.
შემდეგ, Date სვეტი გადავაკეთე datetime ტიპად და გავაერთიანე train_df, test_df, stores_df და features_df ერთიან DataFrame-ებში Store და Date სვეტების მიხედვით.

2. Feature Engineering

create_features ფუნქციის გამოყენებით შევქმენი დამატებითი დროზე დაფუძნებული (time-based) და სხვა მახასიათებლები:

Year, Month, Day, Week, DayOfWeek, Quarter - თარიღის კომპონენტები.

IsMonthStart, IsMonthEnd - თვის დასაწყისი/დასასრული.

IsHoliday - დღესასწაულის აღმნიშვნელი (ბულიანისგან რიცხვით ფორმატში გადაყვანილი).

Type (მაღაზიის ტიპი) - LabelEncoder-ის გამოყენებით გადავაკოდირე რიცხვით მნიშვნელობებად.

Numeric column-ებში არსებული NaN მნიშვნელობები შევავსე თითოეული სვეტის მედიანური მნიშვნელობით.

3. მონაცემთა გაწმენდა (Data Cleaning)

Weekly_Sales სვეტში არსებული NaN მნიშვნელობები წავშალე, რადგან ეს ჩვენი სამიზნე ცვლადია.

შერჩეულ feature_columns-ში დარჩენილი NaN მნიშვნელობები შევავსე 0-ით, რათა მოდელმა შეძლოს მათი დამუშავება.

4. მონაცემთა დაყოფა და სკალირება (Data Splitting and Scaling)

მონაცემები ქრონოლოგიურად დავყავი train, validation და test ნაკრებებად (60%/20%/20% თანაფარდობით Date სვეტის quantile-ების მიხედვით). ეს მიდგომა უზრუნველყოფს, რომ მოდელი მომზადდეს წარსულ მონაცემებზე და შეფასდეს მომავალი პერიოდის მონაცემებზე.

მოდელის მომზადებამდე, feature_columns სვეტები StandardScaler-ის გამოყენებით გავასკალერე. Scaler მოვამზადე მხოლოდ train მონაცემებზე, შემდეგ კი მისი გამოყენებით მოხდა validation და test ნაკრებების ტრანსფორმაცია, რათა თავიდან ავიცილოთ Data Leakage.

DLinear მოდელის არქიტექტურა და ტრენინგი
1. TimeSeriesDataset

შევქმენი TimeSeriesDataset კლასი, რომელიც PyTorch-ის Dataset-ს აიმპლემენტირებს. ეს კლასი პასუხისმგებელია დროითი სერიების მონაცემების "sequence"-ებად გადაყვანაზე, რაც Deep Learning მოდელებისთვის არის აუცილებელი. თითოეული "sequence" შედგება sequence_length (ამ შემთხვევაში 12 კვირა) მახასიათებლებისგან და პროგნოზირებადი სამიზნე მნიშვნელობისგან (შემდეგი კვირის Weekly_Sales).

2. DLinear მოდელი

DLinear მოდელის არქიტექტურა შევქმენი nn.Module-ის გამოყენებით. მოდელი მოიცავს decomposition ფენას (AvgPool1d), რომელიც გამოყოფს ტრენდულ კომპონენტს, და ორ ხაზოვან ფენას (Linear_Seasonal, Linear_Trend), რომლებიც ამუშავებენ სეზონურ და ტრენდულ კომპონენტებს.

3. ტრენინგის პარამეტრები და პროცესი

Device: გამოვიყენე cuda თუ GPU ხელმისაწვდომია, წინააღმდეგ შემთხვევაში cpu.

Hyperparameters:

sequence_length: 12 (წინა 12 კვირის მონაცემები პროგნოზისთვის)

batch_size: 64

learning_rate: 0.001

weight_decay: 1e-4

num_epochs: 50

Loss Function: nn.MSELoss() (Mean Squared Error) შეირჩა, როგორც Loss Function.

Optimizer: optim.Adam() გამოყენებულია მოდელის წონების ოპტიმიზაციისთვის.

Scheduler: optim.lr_scheduler.ReduceLROnPlateau გამოყენებულია Learning Rate-ის ავტომატურად შესამცირებლად, თუ Validation Loss არ უმჯობესდება გარკვეული პერიოდის განმავლობაში (patience=5).

Early Stopping: განხორციელდა Early Stopping მექანიზმი patience=10-ით, რათა თავიდან ავიცილოთ Overfitting და შევაჩეროთ ტრენინგი, თუ Validation Loss არ უმჯობესდება 10 ეპოქის განმავლობაში.

WandB Integration: ყველა ექსპერიმენტი დალოგილ იქნა Weights & Biases (WandB) პლატფორმაზე (project="Store-Sales-Forecasting", entity="agasi22-free-university-of-tbilisi-"). WandB-ზე ილოგებოდა Loss, MAE, RMSE და Learning Rate ყოველი ეპოქის შემდეგ, ასევე საბოლოო შედეგები და გრაფიკები.

შედეგები და შეფასება
DLinear მოდელის ტრენინგის შემდეგ, მისი ეფექტურობა შევაფასე Validation და Test ნაკრებებზე. შეფასების მეტრიკებად გამოვიყენე Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) და Mean Squared Error (MSE).

DLinear მოდელის შედეგები:

Final Validation Results:

MAE: 15478.33

RMSE: 27178.58

MSE: 738672181.62

Final Test Results:

MAE: 15534.75

RMSE: 26844.63

MSE: 719990429.43

მიღებული შედეგები, განსაკუთრებით MAE და RMSE, აჩვენებს, რომ DLinear მოდელის პროგნოზები საკმაოდ შორსაა რეალური გაყიდვებისგან. გაყიდვების საშუალო მაჩვენებლებთან შედარებით (რომელიც ხშირად ათიათასებს აღწევს), 15,000-ზე მეტი MAE მიუთითებს მნიშვნელოვან ცდომილებაზე. ეს იმაზე მეტყველებს, რომ მიუხედავად DLinear-ის დეკომპოზიციური მიდგომისა, ამ კონკრეტულ ამოცანაში მან ვერ შეძლო გაყიდვების კომპლექსური დინამიკის ადეკვატურად აღქმა.

დასკვნა და სამომავლო ნაბიჯები
DLinear მოდელის ექსპერიმენტის შედეგები არ იყო დამაკმაყოფილებელი ამ კონკრეტული Walmart-ის გაყიდვების პროგნოზირების ამოცანისთვის. მიუხედავად იმისა, რომ Deep Learning არქიტექტურებს აქვთ პოტენციალი რთული დროითი სერიების მოდელირებისთვის, DLinear-ის ამ იმპლემენტაციამ ვერ მიაღწია სასურველ სიზუსტეს. სავარაუდო მიზეზები შეიძლება იყოს:

მოდელის სირთულე: შესაძლოა DLinear-ის შედარებით მარტივი ხაზოვანი კომპონენტები არ არის საკმარისი Walmart-ის გაყიდვების მონაცემებში არსებული რთული, არაწრფივი დამოკიდებულებების აღსაქმელად.

Feature Engineering-ის გაუმჯობესება: შესაძლოა, დამატებითი, უფრო რთული ან დომენზე სპეციფიკური მახასიათებლების შექმნა საჭიროა Deep Learning მოდელებისთვის.

Hyperparameter Tuning: მიუხედავად იმისა, რომ ჩატარდა ტრენინგი და Scheduler-ი, შესაძლოა DLinear-ისთვის ოპტიმალური ჰიპერპარამეტრების მოძიება მეტ ძალისხმევას მოითხოვდეს.

მონაცემების მრავალფეროვნება: Walmart-ის მონაცემები შეიცავს უამრავ სხვადასხვა მაღაზიასა და დეპარტამენტს, თითოეულს თავისი უნიკალური დინამიკით. DLinear-ის მიდგომამ შესაძლოა ვერ შეძლო ამ მრავალფეროვნების ეფექტურად მოდელირება.

ამ ექსპერიმენტის საფუძველზე, გადავწყვიტეთ, რომ ამ ამოცანისთვის DLinear არ არის საუკეთესო არჩევანი. ჩვენი გუნდი ფოკუსირებას მოახდენს LightGBM, Prophet და ARIMA მოდელებზე, რადგან მათთან ადრეულმა ექსპერიმენტებმა გაცილებით უკეთესი შედეგები აჩვენა. ამ მოდელებზე დეტალურად ვიმუშავებთ, განვახორციელებთ feature selection-ს, cross-validation-ს, ჰიპერპარამეტრების ოპტიმიზაციას და დავაფიქსირებთ ყველა ნაბიჯს MLflow-სა და WandB-ზე, როგორც ინსტრუქციაშია მოთხოვნილი.






SARIMAX მოდელირება Walmart-ის გაყიდვების პროგნოზირებისთვის
ამ განყოფილებაში განვიხილავთ SARIMAX (Seasonal AutoRegressive Integrated Moving Average with Exogenous Regressors) მოდელების გამოყენების მცდელობას Walmart-ის მაღაზიის გაყიდვების პროგნოზირებისთვის. SARIMAX, როგორც ტრადიციული დროითი სერიების მოდელი, განსაკუთრებით ეფექტურია სეზონურობის, ტრენდების და ციკლური კომპონენტების აღსაქმელად. ის ასევე იძლევა ეგზოგენური ცვლადების (გარე ფაქტორების) ჩართვის საშუალებას, რაც მნიშვნელოვანია გაყიდვების მსგავსი მონაცემებისთვის.

მონაცემთა მომზადება

მონაცემთა მომზადების პროცესი სტანდარტული იყო Walmart-ის კონკურსის მონაცემთა ნაკრებისთვის:

მონაცემთა ჩატვირთვა და გაერთიანება: train.csv, features.csv და stores.csv ფაილები გაერთიანდა Store, Date და IsHoliday სვეტების მიხედვით. Date სვეტი გადაკეთდა datetime ტიპად.

NaN მნიშვნელობების შევსება: Temperature, Fuel_Price, CPI, Unemployment და MarkDown სვეტებში არსებული NaN მნიშვნელობები შევავსე მედიანური მნიშვნელობით. მედიანა შეირჩა, რადგან ის უფრო მდგრადია ექსტრემალური მნიშვნელობების მიმართ, ვიდრე საშუალო და უკეთესად შეესაბამება SARIMAX მოდელის მგრძნობელობას მონაცემთა მიმართ.

დამატებითი მახასიათებლები (Feature Engineering): შევქმენი დამატებითი დროითი მახასიათებლები, როგორიცაა week, sin_13, cos_13, sin_23, cos_23. ეს ციკლური მახასიათებლები დაგვეხმარება სეზონურობის უკეთ აღქმაში, განსაკუთრებით 13 და 23-კვირიანი ციკლებისთვის, რომლებიც შეიძლება გავლენას ახდენდნენ გაყიდვებზე.

SARIMAX მოდელირების პირველი მცდელობა (SARIMAX_Initial_Attempt.ipynb)

ჩემი პირველი მიდგომა SARIMAX-თან გულისხმობდა SARIMAXWrapper კლასის შექმნას, რათა მოდელი scikit-learn Pipeline-ში გამომეყენებინა.

ძირითადი პარამეტრები და მიდგომა:

SARIMAX პარამეტრები: order=(1,1,2) და seasonal_order=(1,1,1,52) განისაზღვრა მოდელისთვის.

enforce_stationarity=False და enforce_invertibility=False: ეს პარამეტრები საწყის ეტაპზე False-ზე დავაყენე, რათა მოდელს გაადვილებოდა კონვერგენცია. თუმცა, როგორც აღმოჩნდა, ეს იყო მნიშვნელოვანი შეცდომა, რადგან არ უზრუნველყოფდა მოდელის სტაბილურობასა და სწორ სტატისტიკურ ქცევას.

ჯგუფებად დაყოფა: მონაცემები დავყავი Store-Dept ჯგუფებად და თითოეული ჯგუფისთვის ცალკე SARIMAX მოდელი იწვრთნებოდა.

მეტრიკა: შეფასებისთვის გამოვიყენე Weighted Mean Absolute Error (WMAE) და Root Mean Square Error (RMSE).

MLflow ინტეგრაცია: ექსპერიმენტები დაილოგა MLflow-ზე DagsHub-ის მეშვეობით.

შედეგები და პრობლემები:

ამ პირველი ექსპერიმენტის Overall WMAE იყო წარმოუდგენლად მაღალი (დაახლოებით 61,288,039,612,882,950), რაც ნათლად მიუთითებდა, რომ მოდელი საერთოდ არ მუშაობდა სწორად. ტიპიური WMAE ამ კონკურსისთვის 2000-3000 დიაპაზონშია.

ძირითადი მიზეზები ასეთი ცუდი შედეგებისთვის:

მონაცემთა არასწორი სკალირება: SARIMAX მოდელი უკიდურესად მგრძნობიარეა მონაცემთა მასშტაბების მიმართ, განსაკუთრებით ეგზოგენური ცვლადების გამოყენებისას. ეგზოგენური ცვლადების სკალირების არარსებობა იყო ყველაზე სავარაუდო მიზეზი ექსტრემალურად ცუდი შედეგების.

enforce_stationarity და enforce_invertibility პარამეტრები False-ზე: ამ პარამეტრების არასწორად დაყენებამ ხელი შეუშალა მოდელის სტაბილურობასა და სწორ სტატისტიკურ თვისებებს, რამაც გამოიწვია კონვერგენციის პრობლემები და არასწორი პროგნოზები.

მონაცემთა სიმცირე ზოგიერთი ჯგუფისთვის: მიუხედავად იმისა, რომ ცარიელი ჯგუფები გამოირიცხა, SARIMAX-ს ზოგადად სჭირდება საკმაოდ დიდი დროითი სერია, განსაკუთრებით 52-კვირიანი სეზონურობის გათვალისწინებით.

SARIMAX მოდელირების მეორე მცდელობა (SARIMAX_Improved_Attempt.ipynb)

წინა ექსპერიმენტის პრობლემების გათვალისწინებით, შეიქმნა ახალი Notebook, რომელიც მიზნად ისახავდა ზემოთ ხსენებული შეცდომების გამოსწორებას. ამ მცდელობაში მთავარი აქცენტი გაკეთდა მონაცემთა წინასწარ დამუშავებასა და მოდელის პარამეტრების სწორ კონფიგურაციაზე.

ძირითადი გაუმჯობესებები:

ეგზოგენური ცვლადების სკალირება: Pipeline-ში ჩაირთო StandardScaler, რათა ყველა ეგზოგენური მახასიათებელი ყოფილიყო სკალირებული. ეს გადამწყვეტი ნაბიჯი იყო მოდელის სტაბილური მორგებისა და ოპტიმალური კონვერგენციისთვის.

enforce_stationarity=True და enforce_invertibility=True: ეს პარამეტრები დაყენდა True-ზე, რაც აუცილებელია statsmodels SARIMAX მოდელის სტატისტიკური ვალიდურობისა და პროგნოზების სანდოობის უზრუნველსაყოფად.

დამატებითი ეგზოგენური მახასიათებლები: Year, DayOfWeek და Store Size ჩართული იქნა ეგზოგენურ ცვლადებად, რათა უკეთ აღეწერათ გაყიდვების დინამიკა.

პროგნოზების 0-ზე კლიპინგი: უარყოფითი პროგნოზები დაყენდა 0-ზე, რადგან გაყიდვების მოცულობა არ შეიძლება იყოს ნეგატიური.

მონაცემთა მინიმალური სიგრძის ადაპტაცია: შემცირდა მინიმალური მოთხოვნები ტრენინგისა (min_train_len = 52) და ვალიდაციის (min_val_len = 12) ნაკრებებისთვის, რათა მოდელს შეძლებოდა უფრო მეტი Store-Dept კომბინაციის დამუშავება.

შედეგები და შეზღუდვები:

ამ ცვლილებების შემდეგ, მოდელმა დაიწყო ვალიდური WMAE და RMSE მნიშვნელობების გენერირება თითოეული Store-Dept კომბინაციისთვის, რაც წინა ექსპერიმენტებში შეუძლებელი იყო. თუმცა, მიუხედავად იმისა, რომ ინდივიდუალური მოდელები გამართულად მუშაობდნენ, მთელი dataset-ის დამუშავების დრო წარმოუდგენლად ხანგრძლივი აღმოჩნდა.

კერძოდ, პრობლემები იყო:

გამოთვლითი სირთულე: SARIMAX მოდელი თითოეული დროითი სერიისთვის ცალ-ცალკე იწვრთნება. 3,331 უნიკალური Store-Dept კომბინაციისთვის ცალკე მოდელის დატრენინგებას, თუნდაც მცირე ცვლილებების შემდეგ, წუთები ან მეტი სჭირდებოდა.

Colab-ის სესიების შეჩერება: ამდენი Store-Dept კომბინაციის პირობებში, მთელი dataset-ის დამუშავების დრო საათებს, შესაძლოა დღეებსაც კი გაგრძელდა. ჩემს ბოლო ცდაში, რამდენიმე წუთის შემდეგ მოდელი მხოლოდ პირველ ათეულ კომბინაციაზე იყო გადასული, სანამ Colab-ის სესია არ შეწყდებოდა (სავარაუდოდ, რესურსების, როგორიცაა GPU ან RAM, უკმარისობის გამო). ასეთი ხანგრძლივი სატრენინგო დრო ფიზიკურად შეუძლებელს ხდიდა მოდელის ოპტიმიზაციას, ჰიპერპარამეტრების დარეგულირებას ან ალტერნატიული SARIMA პარამეტრების ტესტირებას გონივრულ ვადებში.

დასკვნა:

SARIMAX-ის გამოთვლითი სირთულე და დროითი შეზღუდვები არ არის პრაქტიკული ამ მასშტაბის ამოცანისთვის. მიუხედავად იმისა, რომ SARIMAX-ის მიერ მოწოდებული სტატისტიკური სიზუსტე შეიძლება იყოს მაღალი ინდივიდუალური სერიებისთვის, მისი მასშტაბირება ათასობით დროითი სერიისთვის უბრალოდ არაეფექტურია. ამ მიზეზით, გადავწყვიტე, ეს კონკრეტული Notebook არ დამესრულებინა და მის ნაცვლად, ყურადღება გადავიტანო უფრო მასშტაბურ და ეფექტურ დროითი სერიების პროგნოზირების მოდელებზე, როგორიცაა გლობალური მანქანური სწავლების მოდელები (მაგალითად, LightGBM ან XGBoost) დროითი სერიების მახასიათებლებთან ერთად. ეს მოდელები შეძლებენ მთელი dataset-ის ერთიანად დამუშავებას გაცილებით სწრაფად და ხშირად უკეთესი სიზუსტით.











