{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eghib22/Store-Sales-Forecasting/blob/main/model_experiment_SARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQGKayghxe9G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mv \"kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls -l ~/.kaggle/\n",
        "\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip walmart-recruiting-store-sales-forecasting\n",
        "!unzip '*.csv.zip'\n",
        "!pip install -q dagshub mlflow scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iDaYH3jxoXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import mlflow\n",
        "import dagshub\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "dagshub.init(repo_owner='eghib22', repo_name='Store-Sales-Forecasting', mlflow=True)\n",
        "mlflow.set_experiment(\"SARIMAX_Modeling\")\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "features = pd.read_csv('features.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "df = train.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "df = df.merge(stores, on='Store', how='left')\n",
        "\n",
        "df['Temperature'] = df['Temperature'].fillna(method='ffill').fillna(method='bfill')\n",
        "df['Fuel_Price'] = df['Fuel_Price'].fillna(method='ffill').fillna(method='bfill')\n",
        "df['CPI'] = df['CPI'].fillna(method='ffill').fillna(method='bfill')\n",
        "df['Unemployment'] = df['Unemployment'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "df['HolidayWeek'] = df.groupby(['Store', 'Dept'])['IsHoliday'].transform('max')\n",
        "exog_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Month', 'WeekOfYear', 'HolidayWeek']\n",
        "\n",
        "print(f\"Using exogenous features: {exog_cols}\")\n",
        "\n",
        "def weighted_mae(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCr3y5AAxqhN"
      },
      "outputs": [],
      "source": [
        "class SARIMAXWrapper(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, order=(1,1,1), seasonal_order=(1,1,1,52)):\n",
        "        self.order = order\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.model_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model_ = SARIMAX(\n",
        "            y,\n",
        "            exog=X,\n",
        "            order=self.order,\n",
        "            seasonal_order=self.seasonal_order,\n",
        "            enforce_stationarity=False,\n",
        "            enforce_invertibility=False\n",
        "        ).fit(disp=False)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = self.model_.predict(start=X.index[0], end=X.index[-1], exog=X)\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKUjgql9xs4_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "all_preds = []\n",
        "\n",
        "store_dept_groups = df.groupby(['Store', 'Dept'])\n",
        "total_groups = len(store_dept_groups)\n",
        "\n",
        "print(f\"--- Starting SARIMAX for {total_groups} Store-Dept combos ---\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"SARIMAX_Improved_CV_Run\"):\n",
        "    mlflow.log_param(\"SARIMAX_order\", \"(1,1,2)\")\n",
        "    mlflow.log_param(\"SARIMAX_seasonal_order\", \"(1,1,1,52)\")\n",
        "    mlflow.log_param(\"exogenous_features\", exog_cols)\n",
        "\n",
        "    for idx, ((store_id, dept_id), group) in enumerate(store_dept_groups, start=1):\n",
        "        print(f\"\\n--- Processing Store: {store_id}, Dept: {dept_id} ({idx}/{total_groups}) ---\")\n",
        "\n",
        "        g = group.sort_values('Date').set_index('Date')\n",
        "        y = g['Weekly_Sales']\n",
        "        X = g[exog_cols].copy()\n",
        "        weights = g['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "        y_train = y[y.index < '2012-01-01']\n",
        "        y_val = y[(y.index >= '2012-01-01') & (y.index < '2012-07-01')]\n",
        "        X_train = X.loc[y_train.index].copy()\n",
        "        X_val = X.loc[y_val.index].copy()\n",
        "        weights_val = weights.loc[y_val.index]\n",
        "\n",
        "        if len(y_train) < 100 or len(y_val) < 20:\n",
        "            print(f\"   Skipped: Not enough data (Train: {len(y_train)}, Val: {len(y_val)})\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            X_train = X_train.astype('float64')\n",
        "            X_val = X_val.astype('float64')\n",
        "\n",
        "            pipe = Pipeline([\n",
        "                ('model', SARIMAXWrapper(order=(1,1,2), seasonal_order=(1,1,1,52)))\n",
        "            ])\n",
        "\n",
        "            pipe.fit(X_train, y_train)\n",
        "            pred = pipe.predict(X_val)\n",
        "\n",
        "            wmae = weighted_mae(y_val, pred, weights_val)\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
        "\n",
        "            print(f\"   WMAE: {wmae:.2f} | RMSE: {rmse:.2f}\")\n",
        "\n",
        "            results.append({\n",
        "                'Store': store_id,\n",
        "                'Dept': dept_id,\n",
        "                'RMSE': rmse,\n",
        "                'WMAE': wmae\n",
        "            })\n",
        "\n",
        "            all_preds.append(pd.DataFrame({\n",
        "                'y_true': y_val.values,\n",
        "                'y_pred': pred.values,\n",
        "                'weight': weights_val.values\n",
        "            }))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Failed: {e}\")\n",
        "            continue\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    if len(all_preds) > 0:\n",
        "        all_df = pd.concat(all_preds)\n",
        "        overall_wmae = np.sum(all_df['weight'] * np.abs(all_df['y_true'] - all_df['y_pred'])) / np.sum(all_df['weight'])\n",
        "        print(f\"\\n Overall WMAE: {overall_wmae:.2f}\")\n",
        "        mlflow.log_metric('Overall_WMAE', overall_wmae)\n",
        "    else:\n",
        "        print(\"No valid predictions generated.\")\n",
        "\n",
        "    mlflow.log_metric('total_groups_processed', len(results))\n",
        "\n",
        "    best_model = Pipeline([\n",
        "        ('model', SARIMAXWrapper(order=(1,1,2), seasonal_order=(1,1,1,52)))\n",
        "    ])\n",
        "    joblib.dump(best_model, \"sarimax_pipeline.pkl\")\n",
        "    mlflow.log_artifact(\"sarimax_pipeline.pkl\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('/content/drive/MyDrive/sarimax_results.csv', index=False)\n",
        "\n",
        "    print(results_df.head())\n",
        "\n",
        "print(\"Done. Model and results saved.\")\n",
        "mlflow.end_run()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgbJhtfuCAz1IBOkhH2Fme",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}