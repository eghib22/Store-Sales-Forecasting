{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eghib22/Store-Sales-Forecasting/blob/main/model_experiment_XGBoost_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1HPsAmGXy2E",
        "outputId": "14e3abea-972b-41bd-f485-46d55c04fecd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.5.10)\n",
            "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n",
            "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
            "Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.2)\n",
            "Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.8.0)\n",
            "Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.39.4)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n",
            "Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (4.14.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from treelib>=1.6.4->dagshub) (1.17.0)\n",
            "Requirement already satisfied: botocore<1.40.0,>=1.39.4 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.39.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n",
            "Requirement already satisfied: graphql-core<3.2.7,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (3.2.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.26 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.4->boto3->dagshub) (2.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.4.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! pip install dagshub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TkCVhuaYIXT",
        "outputId": "0e0b15dc-4a18-40aa-f216-20883bd2c185"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cJoCWUAqYMf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216751c4-5d58-4782-e825-97964f94b952"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "qybR_XwZbbiu",
        "outputId": "b742aeba-1a69-4211-ea18-ddf040dada95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac27ade2-202a-4ede-b9e1-3410a37d6bd5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ac27ade2-202a-4ede-b9e1-3410a37d6bd5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ekaterineghibradze\",\"key\":\"b1414052fbae86987efff2083c8dcbd1\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "D1kbLG2Pb9lE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ~/.kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtNbrtCycvO7",
        "outputId": "adc63fa0-20a5-4864-a5cb-39b2d9b3e9b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 74 Jul 15 19:02 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN4023PLZ-1I",
        "outputId": "c34fe8d2-5727-4131-aa68-ca06f01a5027"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNg-36PMbu7W",
        "outputId": "cd2930e2-bc9d-4378-8caf-db64bc369d89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv.zip        \n",
            "replace sampleSubmission.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "replace stores.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: stores.csv              \n",
            "replace test.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv.zip            \n",
            "replace train.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '*.csv.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGHVXffUeA_V",
        "outputId": "3b8e837c-e05d-4890-ebea-47a9e31a8a02"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: yy\n",
            "  inflating: test.csv                \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '*.csv.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpflLFtWeQpt",
        "outputId": "95b36d9c-a84c-42df-fdf2-034d9c739444"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: yy\n",
            "  inflating: test.csv                \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dagshub\n",
        "dagshub.init(repo_owner='eghib22', repo_name='Store-Sales-Forecasting', mlflow=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "HbyDRX6Gjs2N",
        "outputId": "773347ab-c927-4025-d810-6727ea6ed1a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as eghib22\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as eghib22\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"eghib22/Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"eghib22/Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository eghib22/Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository eghib22/Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "features = pd.read_csv('features.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "sample_submission = pd.read_csv('sampleSubmission.csv')\n",
        "\n",
        "# Explore the data\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(train.head())\n",
        "print(train.columns)\n",
        "\n",
        "print(\"\\nTest shape:\", test.shape)\n",
        "print(test.head())\n",
        "print(test.columns)\n",
        "\n",
        "print(\"\\nFeatures shape:\", features.shape)\n",
        "print(features.head())\n",
        "print(features.columns)\n",
        "\n",
        "print(\"\\nStores shape:\", stores.shape)\n",
        "print(stores.head())\n",
        "print(stores.columns)\n",
        "\n",
        "print(\"\\nSample Submission shape:\", sample_submission.shape)\n",
        "print(sample_submission.head())\n",
        "print(sample_submission.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpgC7NMjecK",
        "outputId": "e42c0e6c-e89d-41c7-889a-80886ccc33c8",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (421570, 5)\n",
            "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
            "0      1     1  2010-02-05      24924.50      False\n",
            "1      1     1  2010-02-12      46039.49       True\n",
            "2      1     1  2010-02-19      41595.55      False\n",
            "3      1     1  2010-02-26      19403.54      False\n",
            "4      1     1  2010-03-05      21827.90      False\n",
            "Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday'], dtype='object')\n",
            "\n",
            "Test shape: (115064, 4)\n",
            "   Store  Dept        Date  IsHoliday\n",
            "0      1     1  2012-11-02      False\n",
            "1      1     1  2012-11-09      False\n",
            "2      1     1  2012-11-16      False\n",
            "3      1     1  2012-11-23       True\n",
            "4      1     1  2012-11-30      False\n",
            "Index(['Store', 'Dept', 'Date', 'IsHoliday'], dtype='object')\n",
            "\n",
            "Features shape: (8190, 12)\n",
            "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
            "0      1  2010-02-05        42.31       2.572        NaN        NaN   \n",
            "1      1  2010-02-12        38.51       2.548        NaN        NaN   \n",
            "2      1  2010-02-19        39.93       2.514        NaN        NaN   \n",
            "3      1  2010-02-26        46.63       2.561        NaN        NaN   \n",
            "4      1  2010-03-05        46.50       2.625        NaN        NaN   \n",
            "\n",
            "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
            "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
            "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
            "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
            "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
            "4        NaN        NaN        NaN  211.350143         8.106      False  \n",
            "Index(['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
            "       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment',\n",
            "       'IsHoliday'],\n",
            "      dtype='object')\n",
            "\n",
            "Stores shape: (45, 3)\n",
            "   Store Type    Size\n",
            "0      1    A  151315\n",
            "1      2    A  202307\n",
            "2      3    B   37392\n",
            "3      4    A  205863\n",
            "4      5    B   34875\n",
            "Index(['Store', 'Type', 'Size'], dtype='object')\n",
            "\n",
            "Sample Submission shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02             0\n",
            "1  1_1_2012-11-09             0\n",
            "2  1_1_2012-11-16             0\n",
            "3  1_1_2012-11-23             0\n",
            "4  1_1_2012-11-30             0\n",
            "Index(['Id', 'Weekly_Sales'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge train and test with features and stores\n",
        "train_merged = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "train_merged = pd.merge(train_merged, stores, on='Store', how='left')\n",
        "\n",
        "test_merged = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_merged = pd.merge(test_merged, stores, on='Store', how='left')\n",
        "\n",
        "# Convert Date to datetime\n",
        "train_merged['Date'] = pd.to_datetime(train_merged['Date'])\n"
      ],
      "metadata": {
        "id": "GznCjGNPkKZ7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_data = train_merged[train_merged['Date'] < '2012-01-01']\n",
        "val_data = train_merged[(train_merged['Date'] >= '2012-01-01') & (train_merged['Date'] < '2012-07-01')]\n",
        "test_data = train_merged[train_merged['Date'] >= '2012-07-01']\n",
        "\n",
        "print(\"Train:\", train_data.shape)\n",
        "print(\"Validation:\", val_data.shape)\n",
        "print(\"Test (local):\", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQnvIhe4lnEZ",
        "outputId": "19616b48-1188-4035-9730-134b26930394"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (294132, 17)\n",
            "Validation: (77110, 17)\n",
            "Test (local): (50328, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.info())\n",
        "print(train_data.describe())\n",
        "print(train_data.isnull().sum())\n",
        "train_data['Weekly_Sales'].hist(bins=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RS0ucGnnngsS",
        "outputId": "b5a71f55-5498-434b-c352-016b0fa86985",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 294132 entries, 0 to 421526\n",
            "Data columns (total 17 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         294132 non-null  int64         \n",
            " 1   Dept          294132 non-null  int64         \n",
            " 2   Date          294132 non-null  datetime64[ns]\n",
            " 3   Weekly_Sales  294132 non-null  float64       \n",
            " 4   IsHoliday_x   294132 non-null  bool          \n",
            " 5   Temperature   294132 non-null  float64       \n",
            " 6   Fuel_Price    294132 non-null  float64       \n",
            " 7   MarkDown1     23700 non-null   float64       \n",
            " 8   MarkDown2     20154 non-null   float64       \n",
            " 9   MarkDown3     23116 non-null   float64       \n",
            " 10  MarkDown4     21042 non-null   float64       \n",
            " 11  MarkDown5     23994 non-null   float64       \n",
            " 12  CPI           294132 non-null  float64       \n",
            " 13  Unemployment  294132 non-null  float64       \n",
            " 14  IsHoliday_y   294132 non-null  bool          \n",
            " 15  Type          294132 non-null  object        \n",
            " 16  Size          294132 non-null  int64         \n",
            "dtypes: bool(2), datetime64[ns](1), float64(10), int64(3), object(1)\n",
            "memory usage: 36.5+ MB\n",
            "None\n",
            "               Store           Dept                           Date  \\\n",
            "count  294132.000000  294132.000000                         294132   \n",
            "mean       22.180178      44.184135  2011-01-18 08:32:17.546407680   \n",
            "min         1.000000       1.000000            2010-02-05 00:00:00   \n",
            "25%        11.000000      18.000000            2010-07-30 00:00:00   \n",
            "50%        22.000000      37.000000            2011-01-21 00:00:00   \n",
            "75%        33.000000      72.000000            2011-07-15 00:00:00   \n",
            "max        45.000000      99.000000            2011-12-30 00:00:00   \n",
            "std        12.780170      30.430023                            NaN   \n",
            "\n",
            "        Weekly_Sales    Temperature     Fuel_Price     MarkDown1  \\\n",
            "count  294132.000000  294132.000000  294132.000000  23700.000000   \n",
            "mean    16105.306895      58.909079       3.209664   4534.018155   \n",
            "min     -4988.940000      -2.060000       2.472000      0.500000   \n",
            "25%      2146.025000      45.260000       2.808000    759.800000   \n",
            "50%      7727.310000      60.420000       3.129000   3332.410000   \n",
            "75%     20356.665000      73.340000       3.595000   6123.190000   \n",
            "max    693099.360000     100.140000       4.211000  34348.140000   \n",
            "std     22961.301005      18.934062       0.439060   5246.595617   \n",
            "\n",
            "           MarkDown2      MarkDown3     MarkDown4     MarkDown5  \\\n",
            "count   20154.000000   23116.000000  21042.000000  23994.000000   \n",
            "mean     7808.916193    8259.218705   1531.493447   6393.284052   \n",
            "min         0.000000      -0.870000      2.000000    135.160000   \n",
            "25%        34.000000     123.200000    162.330000   1792.920000   \n",
            "50%       171.880000     379.570000    747.890000   4078.210000   \n",
            "75%      2027.770000    1330.530000   2084.640000   7509.900000   \n",
            "max    104519.540000  141630.610000  20834.370000  37581.270000   \n",
            "std     18351.717034   22208.167492   2141.139739   7193.397172   \n",
            "\n",
            "                 CPI   Unemployment           Size  \n",
            "count  294132.000000  294132.000000  294132.000000  \n",
            "mean      169.529527       8.232486  136889.595597  \n",
            "min       126.064000       4.420000   34875.000000  \n",
            "25%       131.901968       7.287000   93638.000000  \n",
            "50%       182.077986       8.058000  140167.000000  \n",
            "75%       211.096358       8.684000  202505.000000  \n",
            "max       223.249677      14.313000  219622.000000  \n",
            "std        38.464447       1.865189   60917.875957  \n",
            "Store                0\n",
            "Dept                 0\n",
            "Date                 0\n",
            "Weekly_Sales         0\n",
            "IsHoliday_x          0\n",
            "Temperature          0\n",
            "Fuel_Price           0\n",
            "MarkDown1       270432\n",
            "MarkDown2       273978\n",
            "MarkDown3       271016\n",
            "MarkDown4       273090\n",
            "MarkDown5       270138\n",
            "CPI                  0\n",
            "Unemployment         0\n",
            "IsHoliday_y          0\n",
            "Type                 0\n",
            "Size                 0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me2e3YHePd7r",
        "outputId": "00007129-75bc-4044-b8c8-389cd4f38f9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.1 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.4)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.58.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.116.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.35.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.1)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.56b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.7.9)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KtQep7gPd4P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import TimeSeriesSplit\n",
        "# from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "# from sklearn.preprocessing import FunctionTransformer\n",
        "# import joblib\n",
        "# import mlflow\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# # --- Enhanced Preprocessing with Time Series Features ---\n",
        "# def enhanced_preprocess(df, is_train=True):\n",
        "#     df = df.copy()\n",
        "\n",
        "#     # Ensure Date is datetime\n",
        "#     if 'Date' not in df.columns:\n",
        "#         raise ValueError(\"Date column is required\")\n",
        "\n",
        "#     if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
        "#         df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "#     # Sort by Store and Date for proper time series ordering\n",
        "#     df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "#     # Basic preprocessing\n",
        "#     type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "#     df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "#     if 'IsHoliday_x' in df.columns:\n",
        "#         df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "#         df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "#     elif 'IsHoliday' in df.columns:\n",
        "#         df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "#     markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "#     for col in markdown_cols:\n",
        "#         if col in df.columns:\n",
        "#             df[col] = df[col].fillna(0)\n",
        "\n",
        "#     # Enhanced time features\n",
        "#     df['Year'] = df['Date'].dt.year\n",
        "#     df['Month'] = df['Date'].dt.month\n",
        "#     df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "#     df['Day'] = df['Date'].dt.day\n",
        "#     df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "#     df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "#     df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "#     df['IsWeekend'] = df['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "#     df['Quarter'] = df['Date'].dt.quarter\n",
        "#     df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "#     df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "#     # Cyclical encoding for better time representation\n",
        "#     df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "#     df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "#     df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "#     df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "#     df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "#     df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "#     # Time-based lag features (only for training to avoid data leakage)\n",
        "#     if is_train and 'Weekly_Sales' in df.columns:\n",
        "#         df = create_lag_features(df)\n",
        "\n",
        "#     return df\n",
        "\n",
        "# def create_lag_features(df):\n",
        "#     \"\"\"Create lag features for time series without data leakage\"\"\"\n",
        "#     df_with_lags = df.copy()\n",
        "\n",
        "#     # Sort by Store and Date\n",
        "#     df_with_lags = df_with_lags.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "#     # Create lag features for each store\n",
        "#     for store in df_with_lags['Store'].unique():\n",
        "#         store_mask = df_with_lags['Store'] == store\n",
        "#         store_data = df_with_lags[store_mask].copy()\n",
        "\n",
        "#         # Lag features (1, 2, 4, 8, 12 weeks back)\n",
        "#         lag_periods = [1, 2, 4, 8, 12]\n",
        "#         for lag in lag_periods:\n",
        "#             df_with_lags.loc[store_mask, f'Sales_lag_{lag}'] = store_data['Weekly_Sales'].shift(lag)\n",
        "\n",
        "#         # Rolling statistics (avoid data leakage by using only past data)\n",
        "#         for window in [4, 8, 12]:\n",
        "#             df_with_lags.loc[store_mask, f'Sales_rolling_mean_{window}'] = (\n",
        "#                 store_data['Weekly_Sales'].shift(1).rolling(window=window, min_periods=1).mean()\n",
        "#             )\n",
        "#             df_with_lags.loc[store_mask, f'Sales_rolling_std_{window}'] = (\n",
        "#                 store_data['Weekly_Sales'].shift(1).rolling(window=window, min_periods=1).std()\n",
        "#             )\n",
        "\n",
        "#         # Trend features\n",
        "#         df_with_lags.loc[store_mask, 'Sales_trend_4w'] = (\n",
        "#             store_data['Weekly_Sales'].shift(1) - store_data['Weekly_Sales'].shift(5)\n",
        "#         )\n",
        "#         df_with_lags.loc[store_mask, 'Sales_trend_8w'] = (\n",
        "#             store_data['Weekly_Sales'].shift(1) - store_data['Weekly_Sales'].shift(9)\n",
        "#         )\n",
        "\n",
        "#     # Fill NaN values for lag features\n",
        "#     lag_cols = [col for col in df_with_lags.columns if 'lag_' in col or 'rolling_' in col or 'trend_' in col]\n",
        "#     for col in lag_cols:\n",
        "#         df_with_lags[col] = df_with_lags[col].fillna(df_with_lags[col].median())\n",
        "\n",
        "#     return df_with_lags\n",
        "\n",
        "# # Custom time series cross-validation\n",
        "# def time_series_cv_score(model, X, y, weights=None, n_splits=3):\n",
        "#     \"\"\"Custom time series cross-validation with proper temporal ordering\"\"\"\n",
        "#     tscv = TimeSeriesSplit(n_splits=n_splits, test_size=None)\n",
        "#     scores = []\n",
        "\n",
        "#     for train_idx, val_idx in tscv.split(X):\n",
        "#         X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n",
        "#         y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "#         # Fit model\n",
        "#         model.fit(X_train_cv, y_train_cv)\n",
        "#         y_pred_cv = model.predict(X_val_cv)\n",
        "\n",
        "#         # Calculate WMAE\n",
        "#         if weights is not None:\n",
        "#             w_val_cv = weights.iloc[val_idx]\n",
        "#             wmae = np.sum(w_val_cv * np.abs(y_val_cv - y_pred_cv)) / np.sum(w_val_cv)\n",
        "#         else:\n",
        "#             wmae = mean_absolute_error(y_val_cv, y_pred_cv)\n",
        "\n",
        "#         scores.append(wmae)\n",
        "\n",
        "#     return np.mean(scores)\n",
        "\n",
        "# # --- Main Training Pipeline ---\n",
        "# def train_xgboost_timeseries(train_data, val_data):\n",
        "#     # Preprocess data\n",
        "#     print(\"Preprocessing training data...\")\n",
        "#     train_processed = enhanced_preprocess(train_data, is_train=True)\n",
        "#     X_train = train_processed.drop(columns=['Weekly_Sales', 'Date'])\n",
        "#     y_train = train_processed['Weekly_Sales']\n",
        "\n",
        "#     print(\"Preprocessing validation data...\")\n",
        "#     val_processed = enhanced_preprocess(val_data, is_train=False)\n",
        "#     X_val = val_processed.drop(columns=['Weekly_Sales', 'Date'])\n",
        "#     y_val = val_processed['Weekly_Sales']\n",
        "#     weights_val = val_processed['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "#     # Updated parameter grid with time series considerations\n",
        "#     param_dist = {\n",
        "#         \"n_estimators\": [800, 1200, 1600],\n",
        "#         \"max_depth\": [4, 6, 8],  # Slightly deeper for complex time patterns\n",
        "#         \"learning_rate\": [0.01, 0.03, 0.05],  # Lower learning rates for stability\n",
        "#         \"subsample\": [0.7, 0.8, 0.9],\n",
        "#         \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
        "#         \"min_child_weight\": [3, 5, 7],\n",
        "#         \"reg_alpha\": [0, 0.1, 0.5],  # L1 regularization\n",
        "#         \"reg_lambda\": [1, 2, 3]      # L2 regularization\n",
        "#     }\n",
        "\n",
        "#     print(\"Starting time series aware parameter search...\")\n",
        "\n",
        "#     # Best parameters tracking\n",
        "#     best_wmae = float('inf')\n",
        "#     best_params = None\n",
        "#     best_model = None\n",
        "\n",
        "#     # Manual parameter search with time series validation\n",
        "#     np.random.seed(42)\n",
        "#     n_trials = 20\n",
        "\n",
        "#     for trial in range(n_trials):\n",
        "#         # Sample random parameters\n",
        "#         params = {\n",
        "#             key: np.random.choice(values) if isinstance(values, list) else values\n",
        "#             for key, values in param_dist.items()\n",
        "#         }\n",
        "\n",
        "#         # Create model with sampled parameters\n",
        "#         model = xgb.XGBRegressor(\n",
        "#             objective='reg:squarederror',\n",
        "#             random_state=42,\n",
        "#             n_jobs=-1,\n",
        "#             **params\n",
        "#         )\n",
        "\n",
        "#         # Evaluate using time series cross-validation\n",
        "#         cv_score = time_series_cv_score(model, X_train, y_train, n_splits=3)\n",
        "\n",
        "#         print(f\"Trial {trial + 1}/{n_trials}: CV WMAE = {cv_score:.4f}\")\n",
        "\n",
        "#         if cv_score < best_wmae:\n",
        "#             best_wmae = cv_score\n",
        "#             best_params = params.copy()\n",
        "#             best_model = model\n",
        "\n",
        "#     print(f\"\\nBest CV WMAE: {best_wmae:.4f}\")\n",
        "#     print(\"Best Parameters:\", best_params)\n",
        "\n",
        "#     # Train final model on full training data\n",
        "#     print(\"\\nTraining final model...\")\n",
        "#     final_model = xgb.XGBRegressor(\n",
        "#         objective='reg:squarederror',\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1,\n",
        "#         **best_params\n",
        "#     )\n",
        "#     final_model.fit(X_train, y_train)\n",
        "\n",
        "#     # Evaluate on validation set\n",
        "#     y_pred = final_model.predict(X_val)\n",
        "#     wmae = np.sum(weights_val * np.abs(y_val - y_pred)) / np.sum(weights_val)\n",
        "#     print(f\"Validation WMAE: {wmae:.4f}\")\n",
        "\n",
        "#     # Log to MLflow\n",
        "#     with mlflow.start_run(run_name=\"XGB-TimeSeries-Tuned\"):\n",
        "#         mlflow.log_param(\"tuned\", True)\n",
        "#         mlflow.log_param(\"time_series_aware\", True)\n",
        "#         for param, value in best_params.items():\n",
        "#             mlflow.log_param(param, value)\n",
        "#         mlflow.log_metric(\"CV_WMAE\", best_wmae)\n",
        "#         mlflow.log_metric(\"Val_WMAE\", wmae)\n",
        "\n",
        "#         # Feature importance\n",
        "#         feature_importance = pd.DataFrame({\n",
        "#             'feature': X_train.columns,\n",
        "#             'importance': final_model.feature_importances_\n",
        "#         }).sort_values('importance', ascending=False)\n",
        "\n",
        "#         print(\"\\nTop 10 Most Important Features:\")\n",
        "#         print(feature_importance.head(10))\n",
        "\n",
        "#         # Save model\n",
        "#         joblib.dump(final_model, \"xgb_timeseries_model.pkl\")\n",
        "#         mlflow.log_artifact(\"xgb_timeseries_model.pkl\")\n",
        "\n",
        "#         print(\"\\nModel, parameters, and metrics logged to MLflow.\")\n",
        "\n",
        "#     return final_model, wmae, feature_importance\n",
        "\n",
        "# # --- Usage ---\n",
        "# # Assuming you have train_data and val_data loaded\n",
        "# # model, wmae, feature_importance = train_xgboost_timeseries(train_data, val_data)\n",
        "\n",
        "# # Additional tips for further improvement:\n",
        "# \"\"\"\n",
        "# 1. Ensemble Methods: Consider combining XGBoost with other models (LightGBM, CatBoost)\n",
        "# 2. Store-specific models: Train separate models for different store types\n",
        "# 3. External features: Add economic indicators, weather data, competitor data\n",
        "# 4. Advanced time series features: Fourier transforms, seasonal decomposition\n",
        "# 5. Post-processing: Apply business rules and constraints to predictions\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "vNAz7VuitFFv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess(df):\n",
        "#     # Encode categorical\n",
        "#     type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "#     df = df.copy()  # Avoid SettingWithCopyWarning\n",
        "#     df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "#     # Convert 'Store' and 'Dept' to integers\n",
        "#     df['Store'] = df['Store'].astype(int)\n",
        "#     df['Dept'] = df['Dept'].astype(int)\n",
        "\n",
        "#     # Rename IsHoliday_x to IsHoliday and drop IsHoliday_y\n",
        "#     if 'IsHoliday_x' in df.columns:\n",
        "#         df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "#         df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "#     elif 'IsHoliday' in df.columns: # Handle the case where the input is the test_merged\n",
        "#         df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "\n",
        "#     # Date features\n",
        "#     df['Year'] = df['Date'].dt.year\n",
        "#     df['Month'] = df['Date'].dt.month\n",
        "#     df['Week'] = df['Date'].dt.isocalendar().week\n",
        "#     df['Day'] = df['Date'].dt.day\n",
        "#     # Fill MarkDown NaNs\n",
        "#     markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "#     for col in markdown_cols:\n",
        "#         if col in df.columns:\n",
        "#             df[col] = df[col].fillna(0)\n",
        "#     # Drop Date column\n",
        "#     df = df.drop(columns=['Date'])\n",
        "#     return df"
      ],
      "metadata": {
        "id": "SW1wA3v2mVVd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import TimeSeriesSplit\n",
        "# from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "# import joblib\n",
        "# import mlflow\n",
        "\n",
        "# # --- Simplified but effective preprocessing ---\n",
        "# def enhanced_preprocess(df, target_col='Weekly_Sales'):\n",
        "#     df = df.copy()\n",
        "\n",
        "#     # Ensure Date is datetime and sort properly\n",
        "#     if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
        "#         df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "#     # Sort by Store and Date - CRITICAL for time series\n",
        "#     df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "#     # Basic preprocessing\n",
        "#     type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "#     df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "#     # Handle holiday columns\n",
        "#     if 'IsHoliday_x' in df.columns:\n",
        "#         df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "#         df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "#     elif 'IsHoliday' in df.columns:\n",
        "#         df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "#     # Fill markdown columns\n",
        "#     markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "#     for col in markdown_cols:\n",
        "#         if col in df.columns:\n",
        "#             df[col] = df[col].fillna(0)\n",
        "\n",
        "#     # Keep only essential time features\n",
        "#     df['Year'] = df['Date'].dt.year\n",
        "#     df['Month'] = df['Date'].dt.month\n",
        "#     df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "#     df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "#     df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "#     df['IsWeekend'] = df['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "#     df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "#     # Simple cyclical encoding for month (most important)\n",
        "#     df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "#     df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "\n",
        "#     # Add simple lag features ONLY if we have the target\n",
        "#     if target_col in df.columns:\n",
        "#         df = add_simple_lags(df, target_col)\n",
        "\n",
        "#     return df\n",
        "\n",
        "# def add_simple_lags(df, target_col):\n",
        "#     \"\"\"Add only the most important lag features\"\"\"\n",
        "#     df_with_lags = df.copy()\n",
        "\n",
        "#     # Create lag features for each store\n",
        "#     for store in df_with_lags['Store'].unique():\n",
        "#         store_mask = df_with_lags['Store'] == store\n",
        "#         store_data = df_with_lags[store_mask].copy()\n",
        "\n",
        "#         # Only add the most predictive lags\n",
        "#         # 1-week lag (previous week)\n",
        "#         df_with_lags.loc[store_mask, 'Sales_lag_1'] = store_data[target_col].shift(1)\n",
        "\n",
        "#         # 4-week lag (same week last month)\n",
        "#         df_with_lags.loc[store_mask, 'Sales_lag_4'] = store_data[target_col].shift(4)\n",
        "\n",
        "#         # 52-week lag (same week last year) - if we have enough data\n",
        "#         if len(store_data) > 52:\n",
        "#             df_with_lags.loc[store_mask, 'Sales_lag_52'] = store_data[target_col].shift(52)\n",
        "\n",
        "#         # Simple 4-week rolling mean (exclude current week)\n",
        "#         df_with_lags.loc[store_mask, 'Sales_rolling_mean_4'] = (\n",
        "#             store_data[target_col].shift(1).rolling(window=4, min_periods=1).mean()\n",
        "#         )\n",
        "\n",
        "#     # Fill NaN values with median\n",
        "#     lag_cols = [col for col in df_with_lags.columns if 'Sales_lag_' in col or 'Sales_rolling_' in col]\n",
        "#     for col in lag_cols:\n",
        "#         df_with_lags[col] = df_with_lags[col].fillna(df_with_lags[col].median())\n",
        "\n",
        "#     return df_with_lags\n",
        "\n",
        "# # --- Better parameter grid for time series ---\n",
        "# def get_optimized_params():\n",
        "#     \"\"\"Parameters specifically tuned for time series with XGBoost\"\"\"\n",
        "#     return {\n",
        "#         'n_estimators': 1000,\n",
        "#         'max_depth': 5,\n",
        "#         'learning_rate': 0.05,\n",
        "#         'subsample': 0.8,\n",
        "#         'colsample_bytree': 0.8,\n",
        "#         'min_child_weight': 3,\n",
        "#         'reg_alpha': 0.1,\n",
        "#         'reg_lambda': 1.0,\n",
        "#         'objective': 'reg:squarederror',\n",
        "#         'random_state': 42,\n",
        "#         'n_jobs': -1\n",
        "#     }\n",
        "\n",
        "# # --- Main function to fix your model ---\n",
        "# def fix_xgboost_timeseries(train_data, val_data):\n",
        "#     \"\"\"\n",
        "#     Simplified approach focusing on what actually works for time series\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"Processing training data...\")\n",
        "#     train_processed = enhanced_preprocess(train_data)\n",
        "\n",
        "#     # Create training features\n",
        "#     feature_cols = [col for col in train_processed.columns\n",
        "#                    if col not in ['Weekly_Sales', 'Date']]\n",
        "#     X_train = train_processed[feature_cols]\n",
        "#     y_train = train_processed['Weekly_Sales']\n",
        "\n",
        "#     print(\"Processing validation data...\")\n",
        "#     val_processed = enhanced_preprocess(val_data)\n",
        "#     X_val = val_processed[feature_cols]\n",
        "#     y_val = val_processed['Weekly_Sales']\n",
        "#     weights_val = val_processed['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "#     # Start with good baseline parameters\n",
        "#     base_params = get_optimized_params()\n",
        "\n",
        "#     print(\"Training baseline model...\")\n",
        "#     model = xgb.XGBRegressor(**base_params)\n",
        "#     model.fit(X_train, y_train)\n",
        "\n",
        "#     # Evaluate baseline\n",
        "#     y_pred = model.predict(X_val)\n",
        "#     baseline_wmae = np.sum(weights_val * np.abs(y_val - y_pred)) / np.sum(weights_val)\n",
        "#     print(f\"Baseline WMAE: {baseline_wmae:.4f}\")\n",
        "\n",
        "#     # Simple parameter tuning - only tune the most important ones\n",
        "#     best_wmae = baseline_wmae\n",
        "#     best_params = base_params.copy()\n",
        "#     best_model = model\n",
        "\n",
        "#     # Tune learning rate\n",
        "#     for lr in [0.01, 0.03, 0.05, 0.1]:\n",
        "#         params = base_params.copy()\n",
        "#         params['learning_rate'] = lr\n",
        "#         params['n_estimators'] = int(1000 * 0.05 / lr)  # Adjust n_estimators\n",
        "\n",
        "#         model = xgb.XGBRegressor(**params)\n",
        "#         model.fit(X_train, y_train)\n",
        "\n",
        "#         y_pred = model.predict(X_val)\n",
        "#         wmae = np.sum(weights_val * np.abs(y_val - y_pred)) / np.sum(weights_val)\n",
        "\n",
        "#         print(f\"Learning rate {lr}: WMAE = {wmae:.4f}\")\n",
        "\n",
        "#         if wmae < best_wmae:\n",
        "#             best_wmae = wmae\n",
        "#             best_params = params.copy()\n",
        "#             best_model = model\n",
        "\n",
        "#     # Tune max_depth\n",
        "#     for depth in [3, 4, 5, 6, 7]:\n",
        "#         params = best_params.copy()\n",
        "#         params['max_depth'] = depth\n",
        "\n",
        "#         model = xgb.XGBRegressor(**params)\n",
        "#         model.fit(X_train, y_train)\n",
        "\n",
        "#         y_pred = model.predict(X_val)\n",
        "#         wmae = np.sum(weights_val * np.abs(y_val - y_pred)) / np.sum(weights_val)\n",
        "\n",
        "#         print(f\"Max depth {depth}: WMAE = {wmae:.4f}\")\n",
        "\n",
        "#         if wmae < best_wmae:\n",
        "#             best_wmae = wmae\n",
        "#             best_params = params.copy()\n",
        "#             best_model = model\n",
        "\n",
        "#     # Tune regularization\n",
        "#     for reg_alpha in [0, 0.1, 0.5, 1.0]:\n",
        "#         for reg_lambda in [1.0, 2.0, 3.0]:\n",
        "#             params = best_params.copy()\n",
        "#             params['reg_alpha'] = reg_alpha\n",
        "#             params['reg_lambda'] = reg_lambda\n",
        "\n",
        "#             model = xgb.XGBRegressor(**params)\n",
        "#             model.fit(X_train, y_train)\n",
        "\n",
        "#             y_pred = model.predict(X_val)\n",
        "#             wmae = np.sum(weights_val * np.abs(y_val - y_pred)) / np.sum(weights_val)\n",
        "\n",
        "#             print(f\"Reg alpha {reg_alpha}, lambda {reg_lambda}: WMAE = {wmae:.4f}\")\n",
        "\n",
        "#             if wmae < best_wmae:\n",
        "#                 best_wmae = wmae\n",
        "#                 best_params = params.copy()\n",
        "#                 best_model = model\n",
        "\n",
        "#     print(f\"\\nBest WMAE: {best_wmae:.4f}\")\n",
        "#     print(\"Best Parameters:\", best_params)\n",
        "\n",
        "#     # Feature importance\n",
        "#     feature_importance = pd.DataFrame({\n",
        "#         'feature': X_train.columns,\n",
        "#         'importance': best_model.feature_importances_\n",
        "#     }).sort_values('importance', ascending=False)\n",
        "\n",
        "#     print(\"\\nTop 10 Most Important Features:\")\n",
        "#     print(feature_importance.head(10))\n",
        "\n",
        "#     # Log to MLflow\n",
        "#     with mlflow.start_run(run_name=\"XGB-Fixed-TimeSeries\"):\n",
        "#         for param, value in best_params.items():\n",
        "#             mlflow.log_param(param, value)\n",
        "#         mlflow.log_metric(\"WMAE\", best_wmae)\n",
        "#         mlflow.log_metric(\"baseline_WMAE\", baseline_wmae)\n",
        "#         mlflow.log_metric(\"improvement\", baseline_wmae - best_wmae)\n",
        "\n",
        "#         joblib.dump(best_model, \"xgb_fixed_model.pkl\")\n",
        "#         mlflow.log_artifact(\"xgb_fixed_model.pkl\")\n",
        "\n",
        "#     return best_model, best_wmae, feature_importance\n",
        "\n",
        "# # --- Quick diagnostic function ---\n",
        "# def diagnose_features(train_data, val_data):\n",
        "#     \"\"\"Quick check of feature quality\"\"\"\n",
        "#     train_processed = enhanced_preprocess(train_data)\n",
        "#     val_processed = enhanced_preprocess(val_data)\n",
        "\n",
        "#     print(\"Training data shape:\", train_processed.shape)\n",
        "#     print(\"Validation data shape:\", val_processed.shape)\n",
        "\n",
        "#     # Check for missing values\n",
        "#     print(\"\\nMissing values in training:\")\n",
        "#     print(train_processed.isnull().sum().sum())\n",
        "\n",
        "#     print(\"\\nMissing values in validation:\")\n",
        "#     print(val_processed.isnull().sum().sum())\n",
        "\n",
        "#     # Check target distribution\n",
        "#     print(f\"\\nTarget stats - Train: mean={train_processed['Weekly_Sales'].mean():.2f}, std={train_processed['Weekly_Sales'].std():.2f}\")\n",
        "#     print(f\"Target stats - Val: mean={val_processed['Weekly_Sales'].mean():.2f}, std={val_processed['Weekly_Sales'].std():.2f}\")\n",
        "\n",
        "#     return train_processed, val_processed\n",
        "\n",
        "# # --- Usage ---\n",
        "# # First, run diagnostics\n",
        "# train_processed, val_processed = diagnose_features(train_data, val_data)\n",
        "\n",
        "# # Then run the fixed model\n",
        "# model, wmae, feature_importance = fix_xgboost_timeseries(train_data, val_data)"
      ],
      "metadata": {
        "id": "9u9L8A4Ha9Vo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import joblib\n",
        "import mlflow\n",
        "\n",
        "# --- Keep your original preprocessing (it was working!) ---\n",
        "def enhanced_preprocess(df):\n",
        "    df = df.copy()\n",
        "    type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "    df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "    if 'IsHoliday_x' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "        df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "    elif 'IsHoliday' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "    df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "    df['IsWeekend'] = df['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "    return df.drop(columns=['Date'])\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, holidays=None):\n",
        "    \"\"\"Calculate WMAE and regular MAE\"\"\"\n",
        "    if holidays is not None:\n",
        "        weights = holidays.apply(lambda x: 5 if x else 1)\n",
        "        wmae = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "    else:\n",
        "        wmae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'WMAE': wmae,\n",
        "        'MAE': mae\n",
        "    }\n",
        "\n",
        "def train_xgboost_model(X_train, y_train, X_val, y_val, train_holidays=None, val_holidays=None):\n",
        "    \"\"\"Train XGBoost model with comprehensive feature logging - Based on your successful code\"\"\"\n",
        "    print(\"🚀 Training XGBoost model...\")\n",
        "\n",
        "    # Log feature information\n",
        "    feature_list = list(X_train.columns)\n",
        "    print(f\"   📋 Total Features: {len(feature_list)}\")\n",
        "    print(f\"   📋 Feature List: {feature_list}\")\n",
        "\n",
        "    # Categorize features for better understanding\n",
        "    feature_categories = {\n",
        "        'ID_Features': [f for f in feature_list if f in ['Store', 'Dept']],\n",
        "        'Store_Info': [f for f in feature_list if f in ['Size']],\n",
        "        'Economic': [f for f in feature_list if f in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']],\n",
        "        'Date_Features': [f for f in feature_list if f in ['Month', 'DayOfWeek', 'Week', 'Day', 'Year', 'Quarter']],\n",
        "        'Holiday_Features': [f for f in feature_list if 'Holiday' in f or 'BackToSchool' in f],\n",
        "        'Type_OneHot': [f for f in feature_list if f.startswith('Type_') and f != 'Type'],\n",
        "        'Type_Label': [f for f in feature_list if f == 'Type'],\n",
        "        'Boolean_Features': [f for f in feature_list if f in ['IsWeekend', 'IsMonthStart', 'IsMonthEnd']],\n",
        "        'Markdown_Features': [f for f in feature_list if f.startswith('MarkDown')],\n",
        "        'Other': [f for f in feature_list if f not in\n",
        "                 ['Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                  'Month', 'DayOfWeek', 'Week', 'Day', 'Year', 'Quarter', 'IsWeekend', 'IsMonthStart', 'IsMonthEnd',\n",
        "                  'Type'] and\n",
        "                  'Holiday' not in f and 'BackToSchool' not in f and not f.startswith('MarkDown')]\n",
        "    }\n",
        "\n",
        "    print(f\"   📊 Feature Categories:\")\n",
        "    for category, features in feature_categories.items():\n",
        "        if features:\n",
        "            print(f\"      {category}: {features}\")\n",
        "\n",
        "    # XGBoost parameters matching your successful configuration\n",
        "    params = {\n",
        "        'n_estimators': 200,          # Same as your successful experiment\n",
        "        'max_depth': 8,               # Same as your successful experiment\n",
        "        'learning_rate': 0.05,        # Same as your successful experiment\n",
        "        'subsample': 0.8,             # Same as your successful experiment\n",
        "        'colsample_bytree': 0.8,      # Same as your successful experiment\n",
        "        'colsample_bylevel': 0.8,     # Same as your successful experiment\n",
        "        'min_child_weight': 3,        # Same as your successful experiment\n",
        "        'gamma': 0.1,                 # Same as your successful experiment\n",
        "        'reg_alpha': 0.1,             # Same as your successful experiment\n",
        "        'reg_lambda': 1.0,            # Same as your successful experiment\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    print(f\"   📋 Parameters: {params}\")\n",
        "    print(f\"   🔄 Training XGBoost with regularization...\")\n",
        "\n",
        "    # Train model\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"   📊 Making predictions...\")\n",
        "\n",
        "    # Make predictions\n",
        "    train_pred = model.predict(X_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_metrics = calculate_metrics(y_train, train_pred, train_holidays)\n",
        "    val_metrics = calculate_metrics(y_val, val_pred, val_holidays)\n",
        "\n",
        "    # Feature importance analysis\n",
        "    feature_importance = model.feature_importances_\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_list,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "        'feature_importance': importance_df,\n",
        "        'params': params,\n",
        "        'feature_list': feature_list,\n",
        "        'feature_categories': feature_categories\n",
        "    }\n",
        "\n",
        "def fine_tune_xgboost(X_train, y_train, X_val, y_val, train_holidays=None, val_holidays=None):\n",
        "    \"\"\"Fine-tune around your successful parameters\"\"\"\n",
        "    print(\"🔧 Fine-tuning XGBoost parameters...\")\n",
        "\n",
        "    # Base parameters from your successful model\n",
        "    base_params = {\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'colsample_bylevel': 0.8,\n",
        "        'min_child_weight': 3,\n",
        "        'gamma': 0.1,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 1.0,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    # Fine-tune only the most critical parameters\n",
        "    param_dist = {\n",
        "        \"n_estimators\": [150, 200, 250, 300],\n",
        "        \"max_depth\": [6, 7, 8, 9],\n",
        "        \"learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
        "    }\n",
        "\n",
        "    # Custom scorer for WMAE\n",
        "    def wmae_scorer(y_true, y_pred):\n",
        "        if val_holidays is not None:\n",
        "            weights = val_holidays.apply(lambda x: 5 if x else 1)\n",
        "            wmae = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "        else:\n",
        "            wmae = mean_absolute_error(y_true, y_pred)\n",
        "        return -wmae  # Negative because sklearn wants higher scores to be better\n",
        "\n",
        "    scorer = make_scorer(wmae_scorer, greater_is_better=True)\n",
        "\n",
        "    # Create model with base parameters\n",
        "    xgb_model = xgb.XGBRegressor(**base_params)\n",
        "\n",
        "    # Randomized search with fewer iterations\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=xgb_model,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=15,  # Reduced iterations\n",
        "        cv=3,       # Reduced CV folds\n",
        "        scoring=scorer,\n",
        "        verbose=1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"   🔍 Starting parameter search...\")\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    # Get best model\n",
        "    best_model = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"   ✅ Best Parameters: {best_params}\")\n",
        "\n",
        "    # Evaluate best model\n",
        "    train_pred = best_model.predict(X_train)\n",
        "    val_pred = best_model.predict(X_val)\n",
        "\n",
        "    train_metrics = calculate_metrics(y_train, train_pred, train_holidays)\n",
        "    val_metrics = calculate_metrics(y_val, val_pred, val_holidays)\n",
        "\n",
        "    print(f\"   📊 Training WMAE: {train_metrics['WMAE']:.4f}\")\n",
        "    print(f\"   📊 Validation WMAE: {val_metrics['WMAE']:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return {\n",
        "        'model': best_model,\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "        'feature_importance': feature_importance,\n",
        "        'params': best_params,\n",
        "        'feature_list': list(X_train.columns)\n",
        "    }\n",
        "\n",
        "# --- Main training pipeline ---\n",
        "def train_improved_xgboost(train_data, val_data):\n",
        "    \"\"\"Main function using your successful approach\"\"\"\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"🎯 XGBOOST TRAINING PIPELINE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # --- Data preprocessing (keep your original) ---\n",
        "    print(\"📊 Preprocessing data...\")\n",
        "    X_train = enhanced_preprocess(train_data.drop(columns=['Weekly_Sales']))\n",
        "    y_train = train_data['Weekly_Sales']\n",
        "\n",
        "    val_data_processed = enhanced_preprocess(val_data)\n",
        "    X_val = val_data_processed.drop(columns=['Weekly_Sales'])\n",
        "    y_val = val_data_processed['Weekly_Sales']\n",
        "\n",
        "    # Extract holiday information for WMAE calculation\n",
        "    train_holidays = train_data['IsHoliday'] if 'IsHoliday' in train_data.columns else None\n",
        "    val_holidays = val_data['IsHoliday'] if 'IsHoliday' in val_data.columns else None\n",
        "\n",
        "    print(f\"   ✅ Training shape: {X_train.shape}\")\n",
        "    print(f\"   ✅ Validation shape: {X_val.shape}\")\n",
        "\n",
        "    # --- Train with your successful parameters first ---\n",
        "    print(\"\\n🚀 Training with your successful parameters...\")\n",
        "    baseline_results = train_xgboost_model(X_train, y_train, X_val, y_val, train_holidays, val_holidays)\n",
        "\n",
        "    baseline_wmae = baseline_results['val_metrics']['WMAE']\n",
        "    print(f\"   📊 Baseline WMAE: {baseline_wmae:.4f}\")\n",
        "\n",
        "    # --- Fine-tune if needed ---\n",
        "    print(\"\\n🔧 Fine-tuning parameters...\")\n",
        "    tuned_results = fine_tune_xgboost(X_train, y_train, X_val, y_val, train_holidays, val_holidays)\n",
        "\n",
        "    tuned_wmae = tuned_results['val_metrics']['WMAE']\n",
        "    print(f\"   📊 Tuned WMAE: {tuned_wmae:.4f}\")\n",
        "\n",
        "    # Choose best model\n",
        "    if tuned_wmae < baseline_wmae:\n",
        "        print(f\"   ✅ Tuned model is better! Improvement: {baseline_wmae - tuned_wmae:.4f}\")\n",
        "        final_results = tuned_results\n",
        "    else:\n",
        "        print(f\"   ✅ Baseline model is better! Using original parameters.\")\n",
        "        final_results = baseline_results\n",
        "\n",
        "    # --- Feature importance analysis ---\n",
        "    print(\"\\n📈 TOP 10 MOST IMPORTANT FEATURES:\")\n",
        "    print(final_results['feature_importance'].head(10))\n",
        "\n",
        "    # --- MLflow logging ---\n",
        "    with mlflow.start_run(run_name=\"XGB-Successful-Config\"):\n",
        "        mlflow.log_param(\"approach\", \"based_on_successful_config\")\n",
        "        for param, value in final_results['params'].items():\n",
        "            mlflow.log_param(param, value)\n",
        "\n",
        "        mlflow.log_metric(\"train_WMAE\", final_results['train_metrics']['WMAE'])\n",
        "        mlflow.log_metric(\"val_WMAE\", final_results['val_metrics']['WMAE'])\n",
        "        mlflow.log_metric(\"train_MAE\", final_results['train_metrics']['MAE'])\n",
        "        mlflow.log_metric(\"val_MAE\", final_results['val_metrics']['MAE'])\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(final_results['model'], \"xgb_successful_model.pkl\")\n",
        "        mlflow.log_artifact(\"xgb_successful_model.pkl\")\n",
        "\n",
        "        print(\"\\n✅ Model and metrics logged to MLflow!\")\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# --- Usage ---\n",
        "# Simply call this function with your data:\n",
        "results = train_improved_xgboost(train_data, val_data)\n",
        "#\n",
        "# The model will be in: results['model']\n",
        "print(results['val_metrics']['WMAE'])\n",
        "# Feature importance: results['feature_importance']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meo6Tj9OPw8T",
        "outputId": "dee23e4e-342c-477c-9ddc-fda6cfb8b181"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "🎯 XGBOOST TRAINING PIPELINE\n",
            "==================================================\n",
            "📊 Preprocessing data...\n",
            "   ✅ Training shape: (294132, 23)\n",
            "   ✅ Validation shape: (77110, 23)\n",
            "\n",
            "🚀 Training with your successful parameters...\n",
            "🚀 Training XGBoost model...\n",
            "   📋 Total Features: 23\n",
            "   📋 Feature List: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'IsMonthStart', 'IsMonthEnd', 'IsWeekend', 'Quarter']\n",
            "   📊 Feature Categories:\n",
            "      ID_Features: ['Store', 'Dept']\n",
            "      Store_Info: ['Size']\n",
            "      Economic: ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
            "      Date_Features: ['Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'Quarter']\n",
            "      Holiday_Features: ['IsHoliday']\n",
            "      Type_Label: ['Type']\n",
            "      Boolean_Features: ['IsMonthStart', 'IsMonthEnd', 'IsWeekend']\n",
            "      Markdown_Features: ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
            "   📋 Parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.8, 'min_child_weight': 3, 'gamma': 0.1, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1}\n",
            "   🔄 Training XGBoost with regularization...\n",
            "   📊 Making predictions...\n",
            "   📊 Baseline WMAE: 3300.6077\n",
            "\n",
            "🔧 Fine-tuning parameters...\n",
            "🔧 Fine-tuning XGBoost parameters...\n",
            "   🔍 Starting parameter search...\n",
            "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
            "   ✅ Best Parameters: {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.1}\n",
            "   📊 Training WMAE: 2208.0641\n",
            "   📊 Validation WMAE: 2831.8596\n",
            "   📊 Tuned WMAE: 2831.8596\n",
            "   ✅ Tuned model is better! Improvement: 468.7480\n",
            "\n",
            "📈 TOP 10 MOST IMPORTANT FEATURES:\n",
            "      feature  importance\n",
            "1        Dept    0.349015\n",
            "12       Size    0.153859\n",
            "11       Type    0.119358\n",
            "6   MarkDown3    0.061002\n",
            "0       Store    0.058419\n",
            "15      Month    0.039399\n",
            "16       Week    0.031670\n",
            "13  IsHoliday    0.028662\n",
            "9         CPI    0.026811\n",
            "22    Quarter    0.025228\n",
            "\n",
            "✅ Model and metrics logged to MLflow!\n",
            "🏃 View run XGB-Successful-Config at: https://dagshub.com/eghib22/Store-Sales-Forecasting.mlflow/#/experiments/0/runs/1811392ff2a04a6bb303303db5e1c1fe\n",
            "🧪 View experiment at: https://dagshub.com/eghib22/Store-Sales-Forecasting.mlflow/#/experiments/0\n",
            "2831.859642393986\n"
          ]
        }
      ]
    }
  ]
}