{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eghib22/Store-Sales-Forecasting/blob/main/model_experiment_Temporal_Fusion_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_1HPsAmGXy2E",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafe181a-6362-4840-a858-166622a9ed66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2TkCVhuaYIXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d48b97c-f25a-4647-e288-968a98523e6b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cJoCWUAqYMf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d782731e-989b-45a5-b87a-187b784f2fe3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "qybR_XwZbbiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "d0eaec19-bfad-4e77-af4e-87ef430475f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b8161160-2cd4-4a82-a597-9ad97d97679b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b8161160-2cd4-4a82-a597-9ad97d97679b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ekaterineghibradze\",\"key\":\"b1414052fbae86987efff2083c8dcbd1\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "D1kbLG2Pb9lE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ~/.kaggle/"
      ],
      "metadata": {
        "id": "vtNbrtCycvO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c4ddf1-4f56-4f39-87a6-b3a2027e96be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 74 Jul  6 12:27 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "yN4023PLZ-1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d1e97f-2cd9-4c34-ef72-20dfda577d35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "DNg-36PMbu7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22243c2d-6164-4a2b-e5fa-f71504811455"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv.zip        \n",
            "replace sampleSubmission.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "replace stores.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: stores.csv              \n",
            "replace test.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv.zip            \n",
            "replace train.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '*.csv.zip'"
      ],
      "metadata": {
        "id": "BGHVXffUeA_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf32cf3-73fa-40ce-d402-f3f735479437"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: test.csv                \n",
            "\n",
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '*.csv.zip'"
      ],
      "metadata": {
        "id": "QpflLFtWeQpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00adbe73-5cea-4a23-f6b8-2f46b5a1b47a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv                \n",
            "\n",
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "4 archives were successfully processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "features = pd.read_csv('features.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "sample_submission = pd.read_csv('sampleSubmission.csv')\n",
        "\n",
        "# Explore the data\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(train.head())\n",
        "print(train.columns)\n",
        "\n",
        "print(\"\\nTest shape:\", test.shape)\n",
        "print(test.head())\n",
        "print(test.columns)\n",
        "\n",
        "print(\"\\nFeatures shape:\", features.shape)\n",
        "print(features.head())\n",
        "print(features.columns)\n",
        "\n",
        "print(\"\\nStores shape:\", stores.shape)\n",
        "print(stores.head())\n",
        "print(stores.columns)\n",
        "\n",
        "print(\"\\nSample Submission shape:\", sample_submission.shape)\n",
        "print(sample_submission.head())\n",
        "print(sample_submission.columns)"
      ],
      "metadata": {
        "id": "lZpgC7NMjecK",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88467166-ff08-44b9-c99e-a143f55eef3a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (421570, 5)\n",
            "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
            "0      1     1  2010-02-05      24924.50      False\n",
            "1      1     1  2010-02-12      46039.49       True\n",
            "2      1     1  2010-02-19      41595.55      False\n",
            "3      1     1  2010-02-26      19403.54      False\n",
            "4      1     1  2010-03-05      21827.90      False\n",
            "Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday'], dtype='object')\n",
            "\n",
            "Test shape: (115064, 4)\n",
            "   Store  Dept        Date  IsHoliday\n",
            "0      1     1  2012-11-02      False\n",
            "1      1     1  2012-11-09      False\n",
            "2      1     1  2012-11-16      False\n",
            "3      1     1  2012-11-23       True\n",
            "4      1     1  2012-11-30      False\n",
            "Index(['Store', 'Dept', 'Date', 'IsHoliday'], dtype='object')\n",
            "\n",
            "Features shape: (8190, 12)\n",
            "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
            "0      1  2010-02-05        42.31       2.572        NaN        NaN   \n",
            "1      1  2010-02-12        38.51       2.548        NaN        NaN   \n",
            "2      1  2010-02-19        39.93       2.514        NaN        NaN   \n",
            "3      1  2010-02-26        46.63       2.561        NaN        NaN   \n",
            "4      1  2010-03-05        46.50       2.625        NaN        NaN   \n",
            "\n",
            "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
            "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
            "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
            "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
            "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
            "4        NaN        NaN        NaN  211.350143         8.106      False  \n",
            "Index(['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
            "       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment',\n",
            "       'IsHoliday'],\n",
            "      dtype='object')\n",
            "\n",
            "Stores shape: (45, 3)\n",
            "   Store Type    Size\n",
            "0      1    A  151315\n",
            "1      2    A  202307\n",
            "2      3    B   37392\n",
            "3      4    A  205863\n",
            "4      5    B   34875\n",
            "Index(['Store', 'Type', 'Size'], dtype='object')\n",
            "\n",
            "Sample Submission shape: (115064, 2)\n",
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02             0\n",
            "1  1_1_2012-11-09             0\n",
            "2  1_1_2012-11-16             0\n",
            "3  1_1_2012-11-23             0\n",
            "4  1_1_2012-11-30             0\n",
            "Index(['Id', 'Weekly_Sales'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge train and test with features and stores\n",
        "train_merged = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "train_merged = pd.merge(train_merged, stores, on='Store', how='left')\n",
        "\n",
        "test_merged = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_merged = pd.merge(test_merged, stores, on='Store', how='left')\n",
        "\n",
        "# Convert Date to datetime\n",
        "train_merged['Date'] = pd.to_datetime(train_merged['Date'])\n"
      ],
      "metadata": {
        "id": "GznCjGNPkKZ7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_data = train_merged[train_merged['Date'] < '2012-01-01']\n",
        "val_data = train_merged[(train_merged['Date'] >= '2012-01-01') & (train_merged['Date'] < '2012-07-01')]\n",
        "test_data = train_merged[train_merged['Date'] >= '2012-07-01']\n",
        "\n",
        "print(\"Train:\", train_data.shape)\n",
        "print(\"Validation:\", val_data.shape)\n",
        "print(\"Test (local):\", test_data.shape)"
      ],
      "metadata": {
        "id": "nQnvIhe4lnEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697702cd-8469-4105-822f-9b52380f71fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (294132, 17)\n",
            "Validation: (77110, 17)\n",
            "Test (local): (50328, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(project=\"Store-Sales-Forecasting\", name=\"temporal-fusion-transformer-training-run\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "g1TQT-utqmX1",
        "outputId": "d219f1b6-0292-446d-cfe0-5a2829df522a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meghib22\u001b[0m (\u001b[33meghib22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_122815-ilhel84h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/ilhel84h' target=\"_blank\">temporal-fusion-transformer-training-run</a></strong> to <a href='https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting' target=\"_blank\">https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/ilhel84h' target=\"_blank\">https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/ilhel84h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/eghib22-free-university-of-tbilisi-/Store-Sales-Forecasting/runs/ilhel84h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ce9f6f95f10>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Map Type A/B/C to 0/1/2\n",
        "    type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "    df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "    # Ensure IsHoliday is int (True/False → 1/0)\n",
        "    if 'IsHoliday_x' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "        df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "    elif 'IsHoliday' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "    # Fill NaNs in MarkDown columns with 0\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    # Extract date features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "vb3wF8lEL135"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# WMAE calculation function\n",
        "def wmae(y_true, y_pred, weights=None):\n",
        "    \"\"\"Weighted Mean Absolute Error\"\"\"\n",
        "    if weights is None:\n",
        "        weights = np.ones_like(y_true)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "class WalmartDataset(Dataset):\n",
        "    \"\"\"Custom dataset for Walmart sales data\"\"\"\n",
        "\n",
        "    def __init__(self, data, sequence_length=8, prediction_length=1, target_col='Weekly_Sales'):\n",
        "        self.data = data.copy()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.target_col = target_col\n",
        "\n",
        "        # Sort by Store, Dept, and time\n",
        "        self.data = self.data.sort_values(['Store', 'Dept', 'Year', 'Month', 'Week'])\n",
        "\n",
        "        # Create time index\n",
        "        self.data['time_idx'] = self.data.groupby(['Store', 'Dept']).cumcount()\n",
        "\n",
        "        # Determine feature columns once during initialization\n",
        "        self._determine_feature_columns()\n",
        "\n",
        "        # Get unique store-dept combinations\n",
        "        self.store_dept_combinations = self.data[['Store', 'Dept']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        # Create sequences\n",
        "        self.sequences = []\n",
        "        self.create_sequences()\n",
        "\n",
        "    def _determine_feature_columns(self):\n",
        "        \"\"\"Determine which columns to use as features (done once during initialization)\"\"\"\n",
        "        all_cols = list(self.data.columns)\n",
        "        excluded_cols = [self.target_col, 'time_idx']\n",
        "\n",
        "        # Remove excluded columns and any remaining non-numeric columns\n",
        "        self.feature_cols = []\n",
        "        skipped_cols = []\n",
        "\n",
        "        for col in all_cols:\n",
        "            if col not in excluded_cols:\n",
        "                # Check if column contains numeric data\n",
        "                try:\n",
        "                    pd.to_numeric(self.data[col].iloc[0])\n",
        "                    self.feature_cols.append(col)\n",
        "                except (ValueError, TypeError):\n",
        "                    skipped_cols.append(col)\n",
        "                    continue\n",
        "\n",
        "        # Print information only once\n",
        "        if skipped_cols:\n",
        "            print(f\"Skipped non-numeric columns: {skipped_cols}\")\n",
        "        print(f\"Using feature columns: {self.feature_cols}\")\n",
        "        print(f\"Total features: {len(self.feature_cols)}\")\n",
        "\n",
        "    def create_sequences(self):\n",
        "        \"\"\"Create sequences for training\"\"\"\n",
        "        for _, row in self.store_dept_combinations.iterrows():\n",
        "            store, dept = row['Store'], row['Dept']\n",
        "\n",
        "            # Get data for this store-dept combination\n",
        "            store_dept_data = self.data[\n",
        "                (self.data['Store'] == store) &\n",
        "                (self.data['Dept'] == dept)\n",
        "            ].reset_index(drop=True)\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(len(store_dept_data) - self.sequence_length - self.prediction_length + 1):\n",
        "                seq_data = store_dept_data.iloc[i:i + self.sequence_length + self.prediction_length]\n",
        "\n",
        "                if len(seq_data) == self.sequence_length + self.prediction_length:\n",
        "                    self.sequences.append({\n",
        "                        'store': store,\n",
        "                        'dept': dept,\n",
        "                        'sequence': seq_data\n",
        "                    })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_data = self.sequences[idx]['sequence']\n",
        "\n",
        "        # Split into input and target\n",
        "        input_seq = sequence_data.iloc[:self.sequence_length]\n",
        "        target_seq = sequence_data.iloc[self.sequence_length:self.sequence_length + self.prediction_length]\n",
        "\n",
        "        # Use pre-determined feature columns\n",
        "        input_data = input_seq[self.feature_cols].values.astype(np.float32)\n",
        "        target_data = target_seq[self.target_col].values.astype(np.float32)\n",
        "\n",
        "        # Convert to tensors\n",
        "        X = torch.FloatTensor(input_data)\n",
        "        y = torch.FloatTensor(target_data)\n",
        "\n",
        "        # Static features (same for all time steps) - ensure they're numeric\n",
        "        static_data = np.array([\n",
        "            float(input_seq['Store'].iloc[0]),\n",
        "            float(input_seq['Dept'].iloc[0]),\n",
        "            float(input_seq['Type'].iloc[0]),\n",
        "            float(input_seq['Size'].iloc[0])\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        static_features = torch.FloatTensor(static_data)\n",
        "\n",
        "        return X, y, static_features\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        output = self.layer_norm(output + query)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "class VariableSelectionNetwork(nn.Module):\n",
        "    \"\"\"Variable selection network for feature importance\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        weights = torch.relu(self.linear1(x))\n",
        "        weights = self.dropout(weights)\n",
        "        weights = self.linear2(weights)\n",
        "        weights = self.softmax(weights)\n",
        "\n",
        "        # Apply weights to input\n",
        "        selected_features = x * weights\n",
        "\n",
        "        return selected_features, weights\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    \"\"\"Gated Residual Network\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.gate = nn.Linear(hidden_dim, output_dim)\n",
        "        self.layer_norm = nn.LayerNorm(output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Skip connection\n",
        "        if input_dim != output_dim:\n",
        "            self.skip_connection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.skip_connection = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Main path\n",
        "        h = torch.relu(self.linear1(x))\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        # Gate\n",
        "        gate = torch.sigmoid(self.gate(h))\n",
        "\n",
        "        # Output\n",
        "        output = self.linear2(h) * gate\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_connection is not None:\n",
        "            residual = self.skip_connection(x)\n",
        "        else:\n",
        "            residual = x\n",
        "\n",
        "        # Add residual and normalize\n",
        "        output = self.layer_norm(output + residual)\n",
        "\n",
        "        return output\n",
        "\n",
        "class TemporalFusionTransformer(nn.Module):\n",
        "    \"\"\"Temporal Fusion Transformer for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 static_dim,\n",
        "                 hidden_dim=128,\n",
        "                 n_heads=8,\n",
        "                 n_layers=3,\n",
        "                 dropout=0.1,\n",
        "                 prediction_length=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.static_dim = static_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.prediction_length = prediction_length\n",
        "\n",
        "        # Variable selection networks\n",
        "        self.variable_selection = VariableSelectionNetwork(\n",
        "            input_dim, hidden_dim, input_dim, dropout\n",
        "        )\n",
        "\n",
        "        # Static feature processing\n",
        "        self.static_encoder = GatedResidualNetwork(\n",
        "            static_dim, hidden_dim, hidden_dim, dropout\n",
        "        )\n",
        "\n",
        "        # Temporal processing\n",
        "        self.temporal_encoder = nn.LSTM(\n",
        "            input_dim, hidden_dim, batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Multi-head attention layers\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            MultiHeadAttention(hidden_dim, n_heads, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Position encoding\n",
        "        self.position_encoding = nn.Parameter(torch.randn(1000, hidden_dim))\n",
        "\n",
        "        # Output layers\n",
        "        self.output_projection = nn.Sequential(\n",
        "            GatedResidualNetwork(hidden_dim, hidden_dim, hidden_dim, dropout),\n",
        "            nn.Linear(hidden_dim, prediction_length)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, static_features):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Variable selection\n",
        "        selected_features, selection_weights = self.variable_selection(x)\n",
        "\n",
        "        # Static context\n",
        "        static_context = self.static_encoder(static_features)\n",
        "\n",
        "        # Temporal encoding\n",
        "        temporal_features, (hidden, cell) = self.temporal_encoder(selected_features)\n",
        "\n",
        "        # Add position encoding\n",
        "        pos_encoding = self.position_encoding[:seq_len, :].unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "        temporal_features = temporal_features + pos_encoding\n",
        "\n",
        "        # Multi-head attention\n",
        "        attention_output = temporal_features\n",
        "        for attention_layer in self.attention_layers:\n",
        "            attention_output, _ = attention_layer(attention_output, attention_output, attention_output)\n",
        "\n",
        "        # Combine with static context\n",
        "        # Use the last time step for prediction\n",
        "        final_features = attention_output[:, -1, :]  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Add static context\n",
        "        static_context_expanded = static_context.unsqueeze(1).expand(-1, 1, -1)\n",
        "        combined_features = final_features + static_context_expanded.squeeze(1)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.output_projection(combined_features)\n",
        "\n",
        "        return output, selection_weights\n",
        "\n",
        "def train_tft_model(train_data, val_data, test_data, epochs=50, batch_size=32, learning_rate=0.001):\n",
        "    \"\"\"Train the TFT model\"\"\"\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    # Preprocess data\n",
        "    train_processed = preprocess(train_data)\n",
        "    val_processed = preprocess(val_data)\n",
        "    test_processed = preprocess(test_data)\n",
        "\n",
        "    print(\"Data shapes after preprocessing:\")\n",
        "    print(f\"Train: {train_processed.shape}\")\n",
        "    print(f\"Validation: {val_processed.shape}\")\n",
        "    print(f\"Test: {test_processed.shape}\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\nCreating datasets...\")\n",
        "    train_dataset = WalmartDataset(train_processed, sequence_length=8, prediction_length=1)\n",
        "    val_dataset = WalmartDataset(val_processed, sequence_length=8, prediction_length=1)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Get feature dimensions\n",
        "    sample_x, sample_y, sample_static = train_dataset[0]\n",
        "    input_dim = sample_x.shape[1]\n",
        "    static_dim = sample_static.shape[0]\n",
        "\n",
        "    print(f\"\\nModel configuration:\")\n",
        "    print(f\"Input dimension: {input_dim}\")\n",
        "    print(f\"Static dimension: {static_dim}\")\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = TemporalFusionTransformer(\n",
        "        input_dim=input_dim,\n",
        "        static_dim=static_dim,\n",
        "        hidden_dim=128,\n",
        "        n_heads=8,\n",
        "        n_layers=3,\n",
        "        dropout=0.1,\n",
        "        prediction_length=1\n",
        "    )\n",
        "\n",
        "    wandb.config={\n",
        "          \"learning_rate\": 0.001,\n",
        "          \"epochs\": 50,\n",
        "          \"batch_size\": 32,\n",
        "          \"sequence_length\": 8,\n",
        "          \"prediction_length\": 1,\n",
        "          \"input_dim\": input_dim,\n",
        "          \"static_dim\": static_dim,\n",
        "          \"hidden_dim\": 128,\n",
        "          \"n_heads\": 8,\n",
        "          \"n_layers\": 3,\n",
        "          \"dropout\" :0.1,\n",
        "      }\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (x, y, static) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions, selection_weights = model(x, static)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(predictions.squeeze(), y.squeeze())\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, static in val_loader:\n",
        "                predictions, _ = model(x, static)\n",
        "                loss = criterion(predictions.squeeze(), y.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_predictions.extend(predictions.squeeze().cpu().numpy())\n",
        "                val_targets.extend(y.squeeze().cpu().numpy())\n",
        "\n",
        "        # Calculate average losses\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        wandb.log({\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": avg_val_loss,\n",
        "            \"epoch\": epoch + 1\n",
        "        })\n",
        "\n",
        "        # Calculate WMAE\n",
        "        val_wmae = wmae(np.array(val_targets), np.array(val_predictions))\n",
        "        wandb.log({\"val_wmae\": val_wmae})\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
        "            print(f'Epoch [{epoch+1:3d}/{epochs}] | '\n",
        "                  f'Train Loss: {avg_train_loss:.4f} | '\n",
        "                  f'Val Loss: {avg_val_loss:.4f} | '\n",
        "                  f'WMAE: {val_wmae:.4f} | '\n",
        "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    # After training, save the model\n",
        "    wandb.save(\"best_model.pth\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.plot(train_losses, label='Train Loss')\n",
        "    ax.plot(val_losses, label='Validation Loss')\n",
        "    ax.set_title('Training History')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    wandb.log({\"loss_curve\": wandb.Image(fig)})\n",
        "\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(val_targets, val_predictions, alpha=0.5)\n",
        "    plt.plot([min(val_targets), max(val_targets)], [min(val_targets), max(val_targets)], 'r--')\n",
        "    plt.title('Validation: Actual vs Predicted')\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def generate_test_predictions(model, test_merged, sample_submission):\n",
        "    \"\"\"Generate predictions for test set\"\"\"\n",
        "\n",
        "    # Preprocess test data\n",
        "    test_processed = preprocess(test_merged)\n",
        "\n",
        "    # Create dataset for prediction\n",
        "    test_dataset = WalmartDataset(test_processed, sequence_length=8, prediction_length=1)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Generate predictions\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _, static in test_loader:\n",
        "            predictions, _ = model(x, static)\n",
        "            all_predictions.extend(predictions.squeeze().cpu().numpy())\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission = sample_submission.copy()\n",
        "\n",
        "    # Map predictions to submission format\n",
        "    # This is a simplified mapping - you might need to adjust based on your data structure\n",
        "    if len(all_predictions) <= len(submission):\n",
        "        submission['Weekly_Sales'][:len(all_predictions)] = all_predictions\n",
        "    else:\n",
        "        submission['Weekly_Sales'] = all_predictions[:len(submission)]\n",
        "\n",
        "    return submission\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data (assuming you have the preprocessing function)\n",
        "    # train_data, val_data, test_data should be your split data\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training Temporal Fusion Transformer...\")\n",
        "    model, train_losses, val_losses = train_tft_model(\n",
        "        train_data, val_data, test_data,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        learning_rate=0.001\n",
        "    )\n",
        "\n",
        "    # Generate test predictions\n",
        "    print(\"Generating test predictions...\")\n",
        "    submission = generate_test_predictions(model, test_merged, sample_submission)\n",
        "\n",
        "    # Save submission\n",
        "    submission.to_csv('tft_submission.csv', index=False)\n",
        "    print(\"Submission saved to 'tft_submission.csv'\")\n",
        "\n",
        "    # Calculate final WMAE on validation set\n",
        "    val_dataset = WalmartDataset(preprocess(val_data), sequence_length=8, prediction_length=1)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, static in val_loader:\n",
        "            predictions, _ = model(x, static)\n",
        "            val_predictions.extend(predictions.squeeze().cpu().numpy())\n",
        "            val_targets.extend(y.squeeze().cpu().numpy())\n",
        "\n",
        "    final_wmae = wmae(np.array(val_targets), np.array(val_predictions))\n",
        "    print(f\"Final Validation WMAE: {final_wmae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNAz7VuitFFv",
        "outputId": "6adf4dd7-5d56-48cd-f058-88fe010c69ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Temporal Fusion Transformer...\n",
            "Preprocessing data...\n",
            "Data shapes after preprocessing:\n",
            "Train: (294132, 20)\n",
            "Validation: (77110, 20)\n",
            "Test: (50328, 20)\n",
            "\n",
            "Creating datasets...\n",
            "Skipped non-numeric columns: ['Date']\n",
            "Using feature columns: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day']\n",
            "Total features: 18\n",
            "Skipped non-numeric columns: ['Date']\n",
            "Using feature columns: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'Year', 'Month', 'Week', 'Day']\n",
            "Total features: 18\n",
            "\n",
            "Model configuration:\n",
            "Input dimension: 18\n",
            "Static dimension: 4\n",
            "Training samples: 268488\n",
            "Validation samples: 52469\n",
            "\n",
            "Starting training for 50 epochs...\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9V7x8-mUu-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}