{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eghib22/Store-Sales-Forecasting/blob/main/model_experiment_LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N9OwPKteDhVm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfd4f75c-6cf3-4c36-9e58-013e32fea176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory â€˜/root/.kaggleâ€™: File exists\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a11537c1-10de-4eaa-88f2-e0a3d1bc4f27\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a11537c1-10de-4eaa-88f2-e0a3d1bc4f27\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "total 4\n",
            "-rw------- 1 root root 74 Jul 13 16:10 kaggle.json\n",
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv.zip        \n",
            "replace sampleSubmission.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: yyy\n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "replace stores.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "y\n",
            "  inflating: stores.csv              \n",
            "replace test.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: test.csv.zip            \n",
            "replace train.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv.zip           \n",
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: yy\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: yy\n",
            "  inflating: test.csv                \n",
            "\n",
            "4 archives were successfully processed.\n",
            "y\n",
            "Archive:  sampleSubmission.csv.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: sampleSubmission.csv    \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "Archive:  features.csv.zip\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: features.csv            \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv                \n",
            "\n",
            "4 archives were successfully processed.\n",
            "y\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.5.10)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.1 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.4)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.58.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.116.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.35.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.1)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
            "Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n",
            "Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.8.0)\n",
            "Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.39.4)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n",
            "Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.10)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: botocore<1.40.0,>=1.39.4 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.39.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.56b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mv \"kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls -l ~/.kaggle/\n",
        "\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip walmart-recruiting-store-sales-forecasting\n",
        "!unzip '*.csv.zip'\n",
        "!unzip '*.csv.zip'\n",
        "!pip install mlflow dagshub lightgbm scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dagshub\n",
        "dagshub.init(repo_owner='eghib22', repo_name='Store-Sales-Forecasting', mlflow=True)\n",
        "\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "SMWjnn5dFU_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b82ac5c9-bdde-499c-b701-9d37242fa8db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"eghib22/Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"eghib22/Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository eghib22/Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository eghib22/Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "features = pd.read_csv('features.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "sample_submission = pd.read_csv('sampleSubmission.csv')\n",
        "\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "train_merged = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "train_merged = pd.merge(train_merged, stores, on='Store', how='left')\n",
        "\n",
        "test_merged = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "test_merged = pd.merge(test_merged, stores, on='Store', how='left')\n",
        "\n",
        "train_merged['Date'] = pd.to_datetime(train_merged['Date'])\n",
        "train_data = train_merged[train_merged['Date'] < '2012-01-01']\n",
        "val_data = train_merged[(train_merged['Date'] >= '2012-01-01') & (train_merged['Date'] < '2012-07-01')]\n"
      ],
      "metadata": {
        "id": "yaqsBchsFa0J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "    df = df.copy()\n",
        "    df['Type'] = df['Type'].map(type_map)\n",
        "    if 'IsHoliday_x' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "        df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "    elif 'IsHoliday' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "    df = df.drop(columns=['Date'])\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "SDjXta3oFdfV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def enhanced_preprocess_simple(df):\n",
        "    \"\"\"Simple enhanced preprocessing that works with your existing data\"\"\"\n",
        "    type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "    df = df.copy()\n",
        "\n",
        "    # Handle Type column\n",
        "    if 'Type' in df.columns:\n",
        "        df['Type'] = df['Type'].map(type_map)\n",
        "\n",
        "    # Handle holiday columns - check what exists\n",
        "    holiday_col = None\n",
        "    if 'IsHoliday_x' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday_x'].astype(int)\n",
        "        df = df.drop(columns=['IsHoliday_x'], errors='ignore')\n",
        "        if 'IsHoliday_y' in df.columns:\n",
        "            df = df.drop(columns=['IsHoliday_y'], errors='ignore')\n",
        "        holiday_col = 'IsHoliday'\n",
        "    elif 'IsHoliday' in df.columns:\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "        holiday_col = 'IsHoliday'\n",
        "    else:\n",
        "        df['IsHoliday'] = 0  # Default to no holiday\n",
        "        holiday_col = 'IsHoliday'\n",
        "\n",
        "    # Enhanced time features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "    # Cyclical encoding for better seasonal patterns\n",
        "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "    df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "    df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "    # Handle markdowns with better features\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "    for col in markdown_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "            # Create binary indicators\n",
        "            df[f'{col}_Present'] = (df[col] > 0).astype(int)\n",
        "\n",
        "    # Create markdown summary features\n",
        "    if any(col in df.columns for col in markdown_cols):\n",
        "        existing_markdown_cols = [col for col in markdown_cols if col in df.columns]\n",
        "        df['Total_MarkDown'] = df[existing_markdown_cols].sum(axis=1)\n",
        "        df['MarkDown_Count'] = (df[existing_markdown_cols] > 0).sum(axis=1)\n",
        "        df['Has_MarkDown'] = (df['Total_MarkDown'] > 0).astype(int)\n",
        "\n",
        "    # Economic indicators interactions\n",
        "    if 'CPI' in df.columns and 'Unemployment' in df.columns:\n",
        "        df['CPI_Unemployment_Ratio'] = df['CPI'] / (df['Unemployment'] + 0.01)\n",
        "        df['CPI_Normalized'] = (df['CPI'] - df['CPI'].mean()) / df['CPI'].std()\n",
        "        df['Unemployment_Normalized'] = (df['Unemployment'] - df['Unemployment'].mean()) / df['Unemployment'].std()\n",
        "\n",
        "    # Store size features\n",
        "    if 'Size' in df.columns:\n",
        "        df['Size_log'] = np.log1p(df['Size'])\n",
        "        df['Size_Normalized'] = (df['Size'] - df['Size'].mean()) / df['Size'].std()\n",
        "\n",
        "    # Temperature features if available\n",
        "    if 'Temperature' in df.columns:\n",
        "        df['Temperature_squared'] = df['Temperature'] ** 2\n",
        "        df['Temperature_Normalized'] = (df['Temperature'] - df['Temperature'].mean()) / df['Temperature'].std()\n",
        "\n",
        "    # Fuel price features\n",
        "    if 'Fuel_Price' in df.columns:\n",
        "        df['Fuel_Price_log'] = np.log1p(df['Fuel_Price'])\n",
        "        df['Fuel_Price_Normalized'] = (df['Fuel_Price'] - df['Fuel_Price'].mean()) / df['Fuel_Price'].std()\n",
        "\n",
        "    # Store-Department interaction\n",
        "    if 'Store' in df.columns and 'Dept' in df.columns:\n",
        "        df['Store_Dept_Interaction'] = df['Store'] * 1000 + df['Dept']\n",
        "\n",
        "    # Special dates (Christmas, Thanksgiving periods)\n",
        "    df['Is_Christmas_Period'] = ((df['Month'] == 12) & (df['Day'] >= 15)).astype(int)\n",
        "    df['Is_Thanksgiving_Period'] = ((df['Month'] == 11) & (df['Day'] >= 20)).astype(int)\n",
        "    df['Is_Back_To_School'] = ((df['Month'] == 8) | ((df['Month'] == 9) & (df['Day'] <= 15))).astype(int)\n",
        "\n",
        "    # Remove date column\n",
        "    df = df.drop(columns=['Date'])\n",
        "\n",
        "    # Fill any remaining NaN values\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_lag_features(df, target_col='Weekly_Sales'):\n",
        "    \"\"\"Create lag features for time series\"\"\"\n",
        "    if target_col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "    # Create lag features\n",
        "    for lag in [1, 2, 4, 8]:\n",
        "        df[f'Sales_Lag_{lag}'] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "    # Rolling statistics\n",
        "    for window in [4, 8, 12]:\n",
        "        df[f'Sales_Mean_{window}'] = df.groupby(['Store', 'Dept'])[target_col].transform(\n",
        "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
        "        )\n",
        "        df[f'Sales_Std_{window}'] = df.groupby(['Store', 'Dept'])[target_col].transform(\n",
        "            lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\n",
        "        )\n",
        "\n",
        "    # Fill NaN values created by lag features\n",
        "    lag_cols = [col for col in df.columns if 'Lag_' in col or 'Mean_' in col or 'Std_' in col]\n",
        "    for col in lag_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    return df\n",
        "\n",
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    \"\"\"Calculate WMAE\"\"\"\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "def train_improved_model(X_train, y_train, X_val, y_val, weights_val):\n",
        "    \"\"\"Train improved model with better regularization\"\"\"\n",
        "\n",
        "    # Model with improved regularization\n",
        "    model = lgb.LGBMRegressor(\n",
        "        random_state=42,\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.015,\n",
        "        num_leaves=70,\n",
        "        max_depth=14,\n",
        "        min_child_samples=20,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        min_split_gain=0.1,\n",
        "        min_child_weight=0.001,\n",
        "        subsample_freq=1,\n",
        "        objective='regression',\n",
        "        metric='rmse',\n",
        "        boosting_type='gbdt',\n",
        "        verbose=-1\n",
        "    )\n",
        "\n",
        "    # Fit with early stopping - fallback version\n",
        "    try:\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Early stopping failed: {e}\")\n",
        "        print(\"Training without early stopping...\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    wmae = weighted_mean_absolute_error(y_val, y_pred, weights_val)\n",
        "\n",
        "    print(f\"Validation RMSE: {rmse:.2f}\")\n",
        "    print(f\"Validation WMAE: {wmae:.2f}\")\n",
        "\n",
        "    return model, y_pred, wmae\n",
        "\n",
        "# Main execution - Simple version\n",
        "def run_simple_optimization():\n",
        "    \"\"\"Run the simple optimization\"\"\"\n",
        "\n",
        "    print(\"=== Simple Walmart Sales Optimization ===\")\n",
        "\n",
        "    # Use your existing data splits\n",
        "    # Assuming train_data and val_data are already defined from your original code\n",
        "\n",
        "    # Create lag features (this might take a moment)\n",
        "    print(\"Creating lag features...\")\n",
        "    train_with_lags = create_lag_features(train_data)\n",
        "    val_with_lags = create_lag_features(val_data)\n",
        "\n",
        "    # Enhanced preprocessing\n",
        "    print(\"Enhanced preprocessing...\")\n",
        "    X_train = enhanced_preprocess_simple(train_with_lags.drop(columns=['Weekly_Sales']))\n",
        "    y_train = train_with_lags['Weekly_Sales']\n",
        "\n",
        "    val_processed = enhanced_preprocess_simple(val_with_lags)\n",
        "    X_val = val_processed.drop(columns=['Weekly_Sales'])\n",
        "    y_val = val_processed['Weekly_Sales']\n",
        "\n",
        "    # Create weights\n",
        "    if 'IsHoliday' in val_processed.columns:\n",
        "        weights_val = val_processed['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "    else:\n",
        "        weights_val = pd.Series([1] * len(val_processed))\n",
        "\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Validation set shape: {X_val.shape}\")\n",
        "    print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training improved model...\")\n",
        "    model, y_pred, wmae = train_improved_model(X_train, y_train, X_val, y_val, weights_val)\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    # MLflow logging\n",
        "    mlflow.set_experiment(\"LightGBM_Training\")\n",
        "\n",
        "    with mlflow.start_run(run_name=\"optimized_lightGBM\"):\n",
        "        mlflow.log_param(\"approach\", \"enhanced_features_simple\")\n",
        "        mlflow.log_param(\"n_estimators\", 2000)\n",
        "        mlflow.log_param(\"learning_rate\", 0.015)\n",
        "        mlflow.log_param(\"num_leaves\", 70)\n",
        "        mlflow.log_param(\"max_depth\", 14)\n",
        "        mlflow.log_param(\"regularization\", \"alpha_0.1_lambda_0.1\")\n",
        "        mlflow.log_param(\"features_count\", X_train.shape[1])\n",
        "        mlflow.log_metric(\"Validation_WMAE\", wmae)\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(model, \"improved_lgbm_model.pkl\")\n",
        "        mlflow.log_artifact(\"improved_lgbm_model.pkl\")\n",
        "\n",
        "    print(f\"\\nFinal WMAE: {wmae:.2f}\")\n",
        "\n",
        "    if wmae < 2682:\n",
        "        print(\"ðŸŽ‰ Improvement achieved!\")\n",
        "    else:\n",
        "        print(\"Still need more optimization. Try ensemble methods or different model architectures.\")\n",
        "\n",
        "    return model, wmae, feature_importance\n",
        "\n",
        "# Run the optimization\n",
        "if __name__ == \"__main__\":\n",
        "    # This assumes your original data loading and splitting code has been run\n",
        "    model, wmae, feature_importance = run_simple_optimization()"
      ],
      "metadata": {
        "id": "hxeIYdNnFfqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe5b0d9-cd5b-4144-dec0-5a074db5226e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simple Walmart Sales Optimization ===\n",
            "Creating lag features...\n",
            "Enhanced preprocessing...\n",
            "Training set shape: (294132, 58)\n",
            "Validation set shape: (77110, 58)\n",
            "Number of features: 58\n",
            "Training improved model...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1793]\tvalid_0's rmse: 4989.84\n",
            "Validation RMSE: 4989.84\n",
            "Validation WMAE: 1877.43\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "         feature  importance\n",
            "1           Dept       15252\n",
            "13   Sales_Lag_1        7460\n",
            "27           Day        6706\n",
            "18   Sales_Std_4        5381\n",
            "22  Sales_Std_12        5111\n",
            "16   Sales_Lag_8        5071\n",
            "2    Temperature        4780\n",
            "14   Sales_Lag_2        4768\n",
            "26          Week        4715\n",
            "20   Sales_Std_8        4649\n",
            "ðŸƒ View run optimized_lightGBM at: https://dagshub.com/eghib22/Store-Sales-Forecasting.mlflow/#/experiments/3/runs/3703e8c4faad40a38962a866af316ff6\n",
            "ðŸ§ª View experiment at: https://dagshub.com/eghib22/Store-Sales-Forecasting.mlflow/#/experiments/3\n",
            "\n",
            "Final WMAE: 1877.43\n",
            "ðŸŽ‰ Improvement achieved!\n"
          ]
        }
      ]
    }
  ]
}